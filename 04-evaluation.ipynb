{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Prerequisites\n",
    "\n",
    "Before executing this notebook, make sure you have properly set up your Azure Services, created your Conda environment, and configured your environment variables as per the instructions provided in the [README.md](README.md) file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Quantitative Measures of Relevance\n",
    "\n",
    "In order to evaluate the effectiveness of our search system, we use several metrics that measure the relevance of the results it returns. These metrics help us understand how well our system is performing and guide us in tuning it for better performance.\n",
    "\n",
    "- **NDCG@10**: Normalized Discounted Cumulative Gain at 10 (NDCG@10) is a metric that assesses the effectiveness of a retrieval system at finding and correctly ordering the top 10 documents. The score ranges from 0 to 100, with higher scores indicating that the system's ordered list of documents closely matches the ideal order. This metric is widely used because it balances the need for precision (returning relevant results) with the need for proper sequencing (ordering the results correctly).\n",
    "\n",
    "- **NDCG@3**: NDCG@3 is similar to NDCG@10, but it focuses on the top 3 documents. This metric is particularly relevant in contexts where it's crucial to have the highest accuracy in the topmost results.\n",
    "\n",
    "### Deep Dive into Discounted Cumulative Gain (DCG)\n",
    "\n",
    "Discounted Cumulative Gain (DCG) is a metric used to measure the effectiveness of ranking algorithms, especially in information retrieval tasks like search engine result ranking. It assesses the quality of the ranking by considering both the relevance of the documents and their positions in the ranking list.\n",
    "\n",
    "#### Calculation\n",
    "\n",
    "In the context of Azure AI search, suppose we have a system that returns five chunks or documents (C1, C2, C3, C4, C5) in a specific order based on a user query. Each chunk or document is assigned a relevance score on a scale from 0 to 3, where:\n",
    "\n",
    "- 0 indicates the chunk/document is not relevant.\n",
    "- 1-2 indicates the chunk/document is somewhat relevant.\n",
    "- 3 indicates the chunk/document is completely relevant.\n",
    "\n",
    "For example, let's say we have the following relevance scores:\n",
    "\n",
    "- C1: 3\n",
    "- C2: 2\n",
    "- C3: 0\n",
    "- C4: 0\n",
    "- C5: 1\n",
    "\n",
    "The Cumulative Gain (CG) is the sum of these relevance scores:\n",
    "\n",
    "```\n",
    "CG = Î£(rel)i = 3 + 2 + 0 + 0 + 1 = 6\n",
    "```\n",
    "\n",
    "The Discounted Cumulative Gain (DCG) is a measure that discounts or reduces the relevance scores based on their position in the result set. It's calculated using the formula:\n",
    "\n",
    "```\n",
    "DCG = Î£(reli / log2(i + 1))\n",
    "```\n",
    "\n",
    "For the given example:\n",
    "\n",
    "```\n",
    "DCG5 = (3 / log2(2)) + (2 / log2(3)) + (0 / log2(4)) + (0 / log2(5)) + (1 / log2(6))\n",
    "DCG5 â‰ˆ 4.67\n",
    "```\n",
    "\n",
    "This DCG score indicates the overall relevance of the search results, taking into account both the relevance of each individual chunk/document and its position in the result set. The higher the DCG score, the better the search results are in terms of relevance.\n",
    "\n",
    "#### Ideal Discounted Cumulative Gain (IDCG)\n",
    "\n",
    "To calculate the IDCG, we reorder the chunks/documents in descending order of relevance and compute the DCG. This gives us the best possible DCG for a given set of chunks/documents.\n",
    "\n",
    "For the example given:\n",
    "```\n",
    "IDCG5 = (3 / log2(2)) + (2 / log2(3)) + (1 / log2(4)) + (0 / log2(5)) + (0 / log2(6))\n",
    "IDCG5 â‰ˆ 4.76\n",
    "```\n",
    "\n",
    "#### Normalized Discounted Cumulative Gain (nDCG)\n",
    "\n",
    "Normalized DCG (nDCG) is obtained by dividing the DCG by the IDCG. This normalization helps in comparing the performance of different ranking algorithms across different datasets.\n",
    "\n",
    "For the example given:\n",
    "```\n",
    "nDCG = DCG5 / IDCG5\n",
    "nDCG â‰ˆ 0.98\n",
    "```\n",
    "\n",
    "DCG, IDCG, and nDCG provide insights into the quality of ranking algorithms by considering both relevance and ranking position. They are valuable metrics in evaluating and improving Azure AI search performance in the context of the RAg pattern. By understanding these metrics, we can better tune our Azure AI search systems to deliver the most relevant chunks and documents to our users based on their queries.\n",
    "\n",
    "## Python Implementation Using sklearn.metrics.ndcg_score\n",
    "\n",
    "The `sklearn.metrics.ndcg_score` function is a powerful tool for measuring the quality of rankings, particularly in scenarios like information retrieval tasks. It calculates the Normalized Discounted Cumulative Gain (NDCG), which evaluates the relevance of items in a ranked list.\n",
    "\n",
    "Here's how you can leverage `ndcg_score` for measuring ranking and relevance:\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "Ensure you have two arrays:\n",
    "\n",
    "- `y_true`: This array contains the true relevance scores of items (ground truth).\n",
    "- `y_score`: This array contains the predicted scores of items.\n",
    "\n",
    "### Calculate NDCG\n",
    "\n",
    "Call the `ndcg_score` function with appropriate parameters:\n",
    "\n",
    "- `y_true`: Array-like of shape (n_samples, n_labels) containing true relevance scores.\n",
    "- `y_score`: Array-like of shape (n_samples, n_labels) containing predicted scores.\n",
    "\n",
    "Optionally, you can specify:\n",
    "\n",
    "- `k`: Consider only the highest k scores in the ranking.\n",
    "- `sample_weight`: Apply sample weights if necessary.\n",
    "- `ignore_ties`: Ignore ties in `y_score` for efficiency gains.\n",
    "\n",
    "### Interpret Results\n",
    "\n",
    "The function returns a value between 0 and 1, representing the averaged NDCG scores for all samples. A higher NDCG score indicates better ranking performance, where items with higher true relevance scores are ranked higher in `y_score`.\n",
    "\n",
    "Here's a simplified example:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Example data\n",
    "true_relevance = np.asarray([[10, 0, 0, 1, 5]])  # True relevance scores\n",
    "predicted_scores = np.asarray([[.1, .2, .3, 4, 70]])  # Predicted scores\n",
    "\n",
    "# Calculate NDCG\n",
    "ndcg = ndcg_score(true_relevance, predicted_scores)\n",
    "\n",
    "print(\"NDCG Score:\", ndcg)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG Score: 0.887075631680536\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Example data\n",
    "true_relevance = np.asarray([[10, 5, 5, 1, 5]])  # True relevance scores\n",
    "predicted_scores = np.asarray([[8, 10, 0, 0, 0]])  # Predicted scores\n",
    "\n",
    "# Calculate NDCG\n",
    "ndcg = ndcg_score(true_relevance, predicted_scores, k=5)\n",
    "\n",
    "print(\"NDCG Score:\", ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM driven evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluators.pf_tester import PromptFlowManagerEvaluator\n",
    "\n",
    "eval_flow = \"src\\\\evaluators\\\\promptflow_util\\\\relevance\\\\flow.dag.yaml\"\n",
    "evaluator = PromptFlowManagerEvaluator(eval_flow=eval_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "question=\"How does the 'Input Range Hi and Lo' setting correlate with valve travel range and Zero Power Condition in the FIELDVUE DVC6200 HW2, and what are the implications for valve calibration?\",\n",
    "source=\"Figure 3-1 and related text detailing calibration related to Zero Power Condition.\"\n",
    "answer=\"The band The Beatles began their journey in London, England, and they changed the history of music.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-13 18:36:30 -0600   12808 execution.flow     INFO     Start executing nodes in thread pool mode.\n",
      "2024-02-13 18:36:30 -0600   12808 execution.flow     INFO     Start to run 2 nodes with concurrency level 16.\n",
      "2024-02-13 18:36:30 -0600   12808 execution.flow     INFO     Executing node relevance_score. node run id: 7a1813e2-5bff-465f-b82b-c355f5e8d9d4_relevance_score_0\n",
      "2024-02-13 18:36:31 -0600   12808 execution.flow     INFO     Node relevance_score completes.\n",
      "2024-02-13 18:36:31 -0600   12808 execution.flow     INFO     Executing node concat_scores. node run id: 7a1813e2-5bff-465f-b82b-c355f5e8d9d4_concat_scores_0\n",
      "2024-02-13 18:36:31 -0600   12808 execution.flow     INFO     Node concat_scores completes.\n",
      "2024-02-13 18:36:31 -0600   12808 execution.flow     INFO     Start to run 1 nodes with concurrency level 16.\n",
      "2024-02-13 18:36:31 -0600   12808 execution.flow     INFO     Executing node aggregate_variants_results. node run id: 7a1813e2-5bff-465f-b82b-c355f5e8d9d4_aggregate_variants_results_reduce\n",
      "2024-02-13 18:36:31 -0600   12808 execution.flow     INFO     Node aggregate_variants_results completes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 18:36:31,988 - micro - MainProcess - INFO     Test result: {'gpt_relevance': 1.0} (pf_tester.py:run_promptflow_evaluations:100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpt_relevance': 1.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.run_promptflow_evaluations(chat_history, question=question, context=source, answer=answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vector-indexing-azureaisearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
