{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Prerequisites\n",
    "\n",
    "Before executing this notebook, make sure you have properly set up your Azure Services, created your Conda environment, and configured your environment variables as per the instructions provided in the [README.md](README.md) file.\n",
    "\n",
    "## üìã Table of Contents\n",
    "\n",
    "This notebook guides you through the following sections:\n",
    "\n",
    "> **üí° Note:** Please refer to the notebook `01-creation-indexes.ipynb` for detailed information and steps on how to create Azure AI Search Indexes.\n",
    "\n",
    "1. [**Indexing Vectorized Content from Documents**](#index-documents)\n",
    "    - Chunk, vectorize, and index local PDF files and website addresses.\n",
    "    - Download, chunk, vectorize, and index all `.docx` files from a SharePoint site.\n",
    "    - Download PDF files stored in Blob Storage, apply complex OCR processing through GPT-4 Vision, chunk and vectorize the content, and finally index the processed data in Azure AI Search.\n",
    "    \n",
    "2. [**Indexing Vectorized Content from Domanin knowledge document containing complex Documents Images or more**](#index-images)\n",
    "    - Leverage complex OCR, image recognition, and summarization capabilities using GPT-4 Vision. Chunk, vectorize, and index extracted metadata from images stored in Blob Storage.\n",
    "\n",
    "3. [**Indexing Vectorized Content from Audio**](#index-audio)\n",
    "    - Process WAV audio data using Azure AI Speech Translator capabilities, chunk, vectorize, and index audio files stored in Blob Storage and indexed in Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = r\"C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\"  # change your directory here\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure AI Search Indexes \n",
    "\n",
    "Please refer to the notebook [01-creation-indexes.ipynb](01-creation-indexes.ipynb) for detailed information and steps on how to create Azure AI Search Indexes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing Vectorized Content from Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 17:36:01,861 - micro - MainProcess - INFO     Loading OpenAIEmbeddings object with model, deployment foundational-ada, and chunk size 1000 (ai_search_indexing.py:load_embedding_model:150)\n",
      "c:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\vector-indexing-azureaisearch\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189: LangChainDeprecationWarning: The class `AzureOpenAIEmbeddings` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.AzureOpenAIEmbeddings instead.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\vector-indexing-azureaisearch\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.OpenAIEmbeddings instead.\n",
      "  warn_deprecated(\n",
      "2024-01-12 17:36:03,371 - micro - MainProcess - INFO     AzureOpenAIEmbeddings object has been created successfully. You can now access the embeddings\n",
      "                using the '.embeddings' attribute. (ai_search_indexing.py:load_embedding_model:161)\n",
      "vector_search_configuration is not a known attribute of class <class 'azure.search.documents.indexes.models._index.SearchField'> and will be ignored\n",
      "2024-01-12 17:36:04,799 - micro - MainProcess - INFO     The Azure AI search index 'test-diferences' has been loaded correctly. (ai_search_indexing.py:load_azureai_index:212)\n"
     ]
    }
   ],
   "source": [
    "# Import the TextChunkingIndexing class from the langchain_integration module\n",
    "from src.indexers.ai_search_indexing import AzureAIndexer\n",
    "\n",
    "DEPLOYMENT_NAME = \"foundational-ada\"\n",
    "\n",
    "# Create an instance of the TextChunkingIndexing class\n",
    "azure_search_indexer_client = AzureAIndexer(\n",
    "    index_name=\"test-diferences\", embedding_azure_deployment_name=DEPLOYMENT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"utils\\\\data\\\\autogen.pdf\"\n",
    "url_pdf = \"https://arxiv.org/pdf/2308.08155.pdf\"\n",
    "blob_path = \"https://testeastusdev001.blob.core.windows.net/testretrieval/autogen.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 17:36:08,627 - micro - MainProcess - INFO     Reading .pdf file from C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmp3edo4zo3. (loading.py:load_file:96)\n",
      "2024-01-12 17:36:08,629 - micro - MainProcess - INFO     Loading file with Loader PyPDFLoader (loading.py:load_file:106)\n",
      "2024-01-12 17:36:10,756 - micro - MainProcess - INFO     Deleted temporary file: C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmp3edo4zo3 (loading.py:load_file_from_blob:138)\n",
      "2024-01-12 17:36:10,758 - micro - MainProcess - INFO     Creating a splitter of type: recursive (chunking.py:get_splitter:56)\n",
      "2024-01-12 17:36:10,759 - micro - MainProcess - INFO     Using tiktoken encoder: cl100k_base (chunking.py:get_splitter:64)\n",
      "2024-01-12 17:36:10,760 - micro - MainProcess - INFO     Obtained splitter of type: RecursiveCharacterTextSplitter (chunking.py:split_documents_in_chunks_from_documents:161)\n",
      "2024-01-12 17:36:10,892 - micro - MainProcess - INFO     Number of chunks obtained: 110 (chunking.py:split_documents_in_chunks_from_documents:164)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "Chunk Number: 1, Character Count: 2114, Token Count: 494\n",
      "Chunk Number: 2, Character Count: 671, Token Count: 142\n",
      "Chunk Number: 3, Character Count: 2387, Token Count: 494\n",
      "Chunk Number: 4, Character Count: 2554, Token Count: 493\n",
      "Chunk Number: 5, Character Count: 1485, Token Count: 289\n",
      "Chunk Number: 6, Character Count: 2508, Token Count: 494\n",
      "Chunk Number: 7, Character Count: 2392, Token Count: 499\n",
      "Chunk Number: 8, Character Count: 620, Token Count: 132\n",
      "Chunk Number: 9, Character Count: 2302, Token Count: 492\n",
      "Chunk Number: 10, Character Count: 1936, Token Count: 362\n",
      "Chunk Number: 11, Character Count: 2542, Token Count: 492\n",
      "Chunk Number: 12, Character Count: 2557, Token Count: 502\n",
      "Chunk Number: 13, Character Count: 1063, Token Count: 228\n",
      "Chunk Number: 14, Character Count: 2139, Token Count: 489\n",
      "Chunk Number: 15, Character Count: 1962, Token Count: 414\n",
      "Chunk Number: 16, Character Count: 1589, Token Count: 487\n",
      "Chunk Number: 17, Character Count: 1958, Token Count: 451\n",
      "Chunk Number: 18, Character Count: 2353, Token Count: 501\n",
      "Chunk Number: 19, Character Count: 2239, Token Count: 493\n",
      "Chunk Number: 20, Character Count: 1126, Token Count: 273\n",
      "Chunk Number: 21, Character Count: 2428, Token Count: 492\n",
      "Chunk Number: 22, Character Count: 2498, Token Count: 486\n",
      "Chunk Number: 23, Character Count: 871, Token Count: 170\n",
      "Chunk Number: 24, Character Count: 2689, Token Count: 494\n",
      "Chunk Number: 25, Character Count: 2007, Token Count: 493\n",
      "Chunk Number: 26, Character Count: 457, Token Count: 137\n",
      "Chunk Number: 27, Character Count: 1686, Token Count: 483\n",
      "Chunk Number: 28, Character Count: 1768, Token Count: 494\n",
      "Chunk Number: 29, Character Count: 764, Token Count: 201\n",
      "Chunk Number: 30, Character Count: 1589, Token Count: 483\n",
      "Chunk Number: 31, Character Count: 1570, Token Count: 466\n",
      "Chunk Number: 32, Character Count: 993, Token Count: 287\n",
      "Chunk Number: 33, Character Count: 1495, Token Count: 482\n",
      "Chunk Number: 34, Character Count: 918, Token Count: 305\n",
      "Chunk Number: 35, Character Count: 2164, Token Count: 500\n",
      "Chunk Number: 36, Character Count: 2352, Token Count: 492\n",
      "Chunk Number: 37, Character Count: 1305, Token Count: 263\n",
      "Chunk Number: 38, Character Count: 1737, Token Count: 365\n",
      "Chunk Number: 39, Character Count: 2169, Token Count: 479\n",
      "Chunk Number: 40, Character Count: 2351, Token Count: 490\n",
      "Chunk Number: 41, Character Count: 1321, Token Count: 284\n",
      "Chunk Number: 42, Character Count: 2172, Token Count: 501\n",
      "Chunk Number: 43, Character Count: 2477, Token Count: 504\n",
      "Chunk Number: 44, Character Count: 1121, Token Count: 237\n",
      "Chunk Number: 45, Character Count: 2340, Token Count: 505\n",
      "Chunk Number: 46, Character Count: 1161, Token Count: 235\n",
      "Chunk Number: 47, Character Count: 2089, Token Count: 478\n",
      "Chunk Number: 48, Character Count: 1690, Token Count: 399\n",
      "Chunk Number: 49, Character Count: 2401, Token Count: 492\n",
      "Chunk Number: 50, Character Count: 1624, Token Count: 345\n",
      "Chunk Number: 51, Character Count: 1792, Token Count: 483\n",
      "Chunk Number: 52, Character Count: 2347, Token Count: 497\n",
      "Chunk Number: 53, Character Count: 1138, Token Count: 228\n",
      "Chunk Number: 54, Character Count: 2313, Token Count: 502\n",
      "Chunk Number: 55, Character Count: 2268, Token Count: 486\n",
      "Chunk Number: 56, Character Count: 824, Token Count: 186\n",
      "Chunk Number: 57, Character Count: 2256, Token Count: 505\n",
      "Chunk Number: 58, Character Count: 2201, Token Count: 497\n",
      "Chunk Number: 59, Character Count: 648, Token Count: 153\n",
      "Chunk Number: 60, Character Count: 2334, Token Count: 490\n",
      "Chunk Number: 61, Character Count: 2155, Token Count: 421\n",
      "Chunk Number: 62, Character Count: 1519, Token Count: 487\n",
      "Chunk Number: 63, Character Count: 1754, Token Count: 477\n",
      "Chunk Number: 64, Character Count: 1883, Token Count: 488\n",
      "Chunk Number: 65, Character Count: 1181, Token Count: 235\n",
      "Chunk Number: 66, Character Count: 2191, Token Count: 497\n",
      "Chunk Number: 67, Character Count: 1701, Token Count: 373\n",
      "Chunk Number: 68, Character Count: 2344, Token Count: 500\n",
      "Chunk Number: 69, Character Count: 2130, Token Count: 467\n",
      "Chunk Number: 70, Character Count: 2266, Token Count: 505\n",
      "Chunk Number: 71, Character Count: 628, Token Count: 194\n",
      "Chunk Number: 72, Character Count: 245, Token Count: 56\n",
      "Chunk Number: 73, Character Count: 2349, Token Count: 505\n",
      "Chunk Number: 74, Character Count: 1525, Token Count: 299\n",
      "Chunk Number: 75, Character Count: 1442, Token Count: 342\n",
      "Chunk Number: 76, Character Count: 1433, Token Count: 510\n",
      "Chunk Number: 77, Character Count: 558, Token Count: 130\n",
      "Chunk Number: 78, Character Count: 2028, Token Count: 490\n",
      "Chunk Number: 79, Character Count: 2447, Token Count: 502\n",
      "Chunk Number: 80, Character Count: 862, Token Count: 196\n",
      "Chunk Number: 81, Character Count: 2249, Token Count: 492\n",
      "Chunk Number: 82, Character Count: 1349, Token Count: 343\n",
      "Chunk Number: 83, Character Count: 1607, Token Count: 480\n",
      "Chunk Number: 84, Character Count: 434, Token Count: 145\n",
      "Chunk Number: 85, Character Count: 1948, Token Count: 496\n",
      "Chunk Number: 86, Character Count: 602, Token Count: 170\n",
      "Chunk Number: 87, Character Count: 2160, Token Count: 486\n",
      "Chunk Number: 88, Character Count: 1897, Token Count: 473\n",
      "Chunk Number: 89, Character Count: 1740, Token Count: 476\n",
      "Chunk Number: 90, Character Count: 1476, Token Count: 381\n",
      "Chunk Number: 91, Character Count: 1263, Token Count: 504\n",
      "Chunk Number: 92, Character Count: 1590, Token Count: 493\n",
      "Chunk Number: 93, Character Count: 1365, Token Count: 502\n",
      "Chunk Number: 94, Character Count: 906, Token Count: 305\n",
      "Chunk Number: 95, Character Count: 2166, Token Count: 489\n",
      "Chunk Number: 96, Character Count: 2083, Token Count: 482\n",
      "Chunk Number: 97, Character Count: 724, Token Count: 172\n",
      "Chunk Number: 98, Character Count: 1659, Token Count: 481\n",
      "Chunk Number: 99, Character Count: 773, Token Count: 186\n",
      "Chunk Number: 100, Character Count: 1133, Token Count: 502\n",
      "Chunk Number: 101, Character Count: 1036, Token Count: 495\n",
      "Chunk Number: 102, Character Count: 1653, Token Count: 498\n",
      "Chunk Number: 103, Character Count: 464, Token Count: 118\n",
      "Chunk Number: 104, Character Count: 2129, Token Count: 505\n",
      "Chunk Number: 105, Character Count: 2268, Token Count: 494\n",
      "Chunk Number: 106, Character Count: 1177, Token Count: 302\n",
      "Chunk Number: 107, Character Count: 1750, Token Count: 424\n",
      "Chunk Number: 108, Character Count: 1920, Token Count: 490\n",
      "Chunk Number: 109, Character Count: 1994, Token Count: 479\n",
      "Chunk Number: 110, Character Count: 2063, Token Count: 455\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "chunks = azure_search_indexer_client.load_and_chunck_files(\n",
    "    file_paths=blob_path,\n",
    "    splitter_type=\"recursive\",\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=128,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Docxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_path = \"utils\\\\data\\\\test.docx\"\n",
    "word_url = \"https://testeastusdev001.blob.core.windows.net/testretrieval/test.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 17:36:46,032 - micro - MainProcess - INFO     Reading .docx file from C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpvx76sq_j. (loading.py:load_file:96)\n",
      "2024-01-12 17:36:46,034 - micro - MainProcess - INFO     Loading file with Loader Docx2txtLoader (loading.py:load_file:106)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 17:36:46,256 - micro - MainProcess - INFO     Deleted temporary file: C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpvx76sq_j (loading.py:load_file_from_blob:138)\n",
      "2024-01-12 17:36:46,258 - micro - MainProcess - INFO     Creating a splitter of type: recursive (chunking.py:get_splitter:56)\n",
      "2024-01-12 17:36:46,259 - micro - MainProcess - INFO     Using tiktoken encoder: cl100k_base (chunking.py:get_splitter:64)\n",
      "2024-01-12 17:36:46,260 - micro - MainProcess - INFO     Obtained splitter of type: RecursiveCharacterTextSplitter (chunking.py:split_documents_in_chunks_from_documents:161)\n",
      "2024-01-12 17:36:46,292 - micro - MainProcess - INFO     Number of chunks obtained: 15 (chunking.py:split_documents_in_chunks_from_documents:164)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Chunk Number: 1, Character Count: 2105, Token Count: 465\n",
      "Chunk Number: 2, Character Count: 1713, Token Count: 404\n",
      "Chunk Number: 3, Character Count: 2155, Token Count: 496\n",
      "Chunk Number: 4, Character Count: 2016, Token Count: 439\n",
      "Chunk Number: 5, Character Count: 2337, Token Count: 482\n",
      "Chunk Number: 6, Character Count: 1680, Token Count: 359\n",
      "Chunk Number: 7, Character Count: 2199, Token Count: 500\n",
      "Chunk Number: 8, Character Count: 1414, Token Count: 355\n",
      "Chunk Number: 9, Character Count: 1984, Token Count: 467\n",
      "Chunk Number: 10, Character Count: 2181, Token Count: 469\n",
      "Chunk Number: 11, Character Count: 1466, Token Count: 323\n",
      "Chunk Number: 12, Character Count: 2084, Token Count: 458\n",
      "Chunk Number: 13, Character Count: 2364, Token Count: 491\n",
      "Chunk Number: 14, Character Count: 2178, Token Count: 420\n",
      "Chunk Number: 15, Character Count: 2537, Token Count: 504\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "chunks = azure_search_indexer_client.load_and_chunck_files(\n",
    "    file_paths=word_url,\n",
    "    splitter_type=\"recursive\",\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=128,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 09:10:57,577 - micro - MainProcess - INFO     Obtained splitter of type: RecursiveCharacterTextSplitter (chunking.py:split_documents_in_chunks_from_documents:95)\n",
      "2024-01-09 09:10:57,593 - micro - MainProcess - INFO     Number of chunks obtained: 418 (chunking.py:split_documents_in_chunks_from_documents:98)\n"
     ]
    }
   ],
   "source": [
    "docs_chunked = split_documents_in_chunks_from_documents(\n",
    "    docs, chunk_size=512, chunk_overlap=128, use_recursive_splitter=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter_1 = CharacterTextSplitter(chunk_size=512, chunk_overlap=128)\n",
    "text_splitter_2 = CharacterTextSplitter(chunk_size=512, chunk_overlap=128, separator=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "412\n"
     ]
    }
   ],
   "source": [
    "a = text_splitter_1.split_documents(docs)\n",
    "print(len(a))\n",
    "b = text_splitter_2.split_documents(docs)\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='AutoGen : Enabling Next-Gen LLM\\nApplications via Multi-Agent Conversation\\nQingyun Wu‚Ä†, Gagan Bansal‚àó, Jieyu Zhang¬±, Yiran Wu‚Ä†, Beibin Li‚àó\\nErkang Zhu‚àó, Li Jiang‚àó, Xiaoyun Zhang‚àó, Shaokun Zhang‚Ä†, Jiale Liu‚àì\\nAhmed Awadallah‚àó, Ryen W. White‚àó, Doug Burger‚àó, Chi Wang‚àó1\\n‚àóMicrosoft Research,‚Ä†Pennsylvania State University\\n¬±University of Washington,‚àìXidian University\\nAgent CustomizationConversable agent\\nFlexible Conversation Patterns\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='¬±University of Washington,‚àìXidian University\\nAgent CustomizationConversable agent\\nFlexible Conversation Patterns\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\nHierarchical chatJoint chatMulti-Agent Conversations‚Ä¶Execute the following code‚Ä¶\\nGot it! Here is the revised code ‚Ä¶No, please plot % change!Plot a chart of META and TESLA stock price change YTD.\\nOutput:$Month\\nOutput:%MonthError package yfinanceis not installed\\nSorry! Please first pip install yfinanceand then execute the code\\nInstalling‚Ä¶\\nExample Agent Chat', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='Sorry! Please first pip install yfinanceand then execute the code\\nInstalling‚Ä¶\\nExample Agent Chat\\nFigure 1: AutoGen enables diverse LLM-based applications using multi-agent conversations. (Left)\\nAutoGen agents are conversable, customizable, and can be based on LLMs, tools, humans, or even\\na combination of them. (Top-middle) Agents can converse to solve tasks. (Right) They can form\\na chat, potentially with humans in the loop. (Bottom-middle) The framework supports flexible\\nconversation patterns.\\nAbstract', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='a chat, potentially with humans in the loop. (Bottom-middle) The framework supports flexible\\nconversation patterns.\\nAbstract\\nAutoGen2is an open-source framework that allows developers to build LLM ap-\\nplications via multiple agents that can converse with each other to accomplish\\ntasks. AutoGen agents are customizable, conversable , and can operate in vari-\\nous modes that employ combinations of LLMs, human inputs, and tools. Using', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='ous modes that employ combinations of LLMs, human inputs, and tools. Using\\nAutoGen , developers can also flexibly define agent interaction behaviors. Both\\nnatural language and computer code can be used to program flexible conversation\\npatterns for different applications. AutoGen serves as a generic framework for\\nbuilding diverse applications of various complexities and LLM capacities. Em-\\npirical studies demonstrate the effectiveness of the framework in many example', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='pirical studies demonstrate the effectiveness of the framework in many example\\napplications, with domains ranging from mathematics, coding, question answer-\\ning, operations research, online decision-making, entertainment, etc.\\n1Corresponding author. Email: auto-gen@outlook.com\\n2https://github.com/microsoft/autogenarXiv:2308.08155v2  [cs.AI]  3 Oct 2023', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='1 Introduction\\nLarge language models (LLMs) are becoming a crucial building block in developing powerful agents\\nthat utilize LLMs for reasoning, tool usage, and adapting to new observations (Yao et al., 2022; Xi\\net al., 2023; Wang et al., 2023b) in many real-world tasks. Given the expanding tasks that could\\nbenefit from LLMs and the growing task complexity, an intuitive approach to scale up the power of\\nagents is to use multiple agents that cooperate. Prior work suggests that multiple agents can help', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='agents is to use multiple agents that cooperate. Prior work suggests that multiple agents can help\\nencourage divergent thinking (Liang et al., 2023), improve factuality and reasoning (Du et al., 2023),\\nand provide validation (Wu et al., 2023). In light of the intuition and early evidence of promise, it is\\nintriguing to ask the following question: how can we facilitate the development of LLM applications\\nthat could span a broad spectrum of domains and complexities based on the multi-agent approach?', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='that could span a broad spectrum of domains and complexities based on the multi-agent approach?\\nOur insight is to use multi-agent conversations to achieve it. There are at least three reasons con-\\nfirming its general feasibility and utility thanks to recent advances in LLMs: First, because chat-\\noptimized LLMs (e.g., GPT-4) show the ability to incorporate feedback, LLM agents can cooperate\\nthrough conversations with each other or human(s), e.g., a dialog where agents provide and seek rea-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='through conversations with each other or human(s), e.g., a dialog where agents provide and seek rea-\\nsoning, observations, critiques, and validation. Second, because a single LLM can exhibit a broad\\nrange of capabilities (especially when configured with the correct prompt and inference settings),\\nconversations between differently configured agents can help combine these broad LLM capabilities\\nin a modular and complementary manner. Third, LLMs have demonstrated ability to solve complex', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='in a modular and complementary manner. Third, LLMs have demonstrated ability to solve complex\\ntasks when the tasks are broken into simpler subtasks. Multi-agent conversations can enable this\\npartitioning and integration in an intuitive manner. How can we leverage the above insights and\\nsupport different applications with the common requirement of coordinating multiple agents, poten-\\ntially backed by LLMs, humans, or tools exhibiting different capacities? We desire a multi-agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='tially backed by LLMs, humans, or tools exhibiting different capacities? We desire a multi-agent\\nconversation framework with generic abstraction and effective implementation that has the flexibil-\\nity to satisfy different application needs. Achieving this requires addressing two critical questions:\\n(1) How can we design individual agents that are capable, reusable, customizable, and effective in\\nmulti-agent collaboration? (2) How can we develop a straightforward, unified interface that can', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='multi-agent collaboration? (2) How can we develop a straightforward, unified interface that can\\naccommodate a wide range of agent conversation patterns? In practice, applications of varying\\ncomplexities may need distinct sets of agents with specific capabilities, and may require different\\nconversation patterns, such as single- or multi-turn dialogs, different human involvement modes, and\\nstatic vs. dynamic conversation. Moreover, developers may prefer the flexibility to program agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='static vs. dynamic conversation. Moreover, developers may prefer the flexibility to program agent\\ninteractions in natural language or code. Failing to adequately address these two questions would\\nlimit the framework‚Äôs scope of applicability and generality.\\nWhile there is contemporaneous exploration of multi-agent approaches,3we present AutoGen , a\\ngeneralized multi-agent conversation framework (Figure 1), based on the following new concepts.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='generalized multi-agent conversation framework (Figure 1), based on the following new concepts.\\n1Customizable and conversable agents. AutoGen uses a generic design of agents that can lever-\\nage LLMs, human inputs, tools, or a combination of them. The result is that developers can\\neasily and quickly create agents with different roles (e.g., agents to write code, execute code,\\nwire in human feedback, validate outputs, etc.) by selecting and configuring a subset of built-in', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='wire in human feedback, validate outputs, etc.) by selecting and configuring a subset of built-in\\ncapabilities. The agent‚Äôs backend can also be readily extended to allow more custom behaviors.\\nTo make these agents suitable for multi-agent conversation, every agent is made conversable ‚Äì\\nthey can receive, react, and respond to messages. When configured properly, an agent can hold\\nmultiple turns of conversations with other agents autonomously or solicit human inputs at cer-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='multiple turns of conversations with other agents autonomously or solicit human inputs at cer-\\ntain rounds, enabling human agency and automation. The conversable agent design leverages the\\nstrong capability of the most advanced LLMs in taking feedback and making progress via chat\\nand also allows combining capabilities of LLMs in a modular fashion. (Section 2.1)\\n2Conversation programming. A fundamental insight of AutoGen is to simplify and unify com-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='2Conversation programming. A fundamental insight of AutoGen is to simplify and unify com-\\nplex LLM application workflows as multi-agent conversations. So AutoGen adopts a program-\\nming paradigm centered around these inter-agent conversations. We refer to this paradigm as\\nconversation programming , which streamlines the development of intricate applications via two\\nprimary steps: (1) defining a set of conversable agents with specific capabilities and roles (as', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='primary steps: (1) defining a set of conversable agents with specific capabilities and roles (as\\ndescribed above); (2) programming the interaction behavior between agents via conversation-\\ncentric computation andcontrol . Both steps can be achieved via a fusion of natural and pro-\\ngramming languages to build applications with a wide range of conversation patterns and agent\\nbehaviors. AutoGen provides ready-to-use implementations and also allows easy extension and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='behaviors. AutoGen provides ready-to-use implementations and also allows easy extension and\\nexperimentation for both steps. (Section 2.2)\\n3We refer to Appendix A for a detailed discussion.\\n2', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='AutoGen also provides a collection of multi-agent applications created using conversable agents\\nand conversation programming. These applications demonstrate how AutoGen can easily support\\napplications of various complexities and LLMs of various capabilities. Moreover, we perform both\\nevaluation on benchmarks and a pilot study of new applications. The results show that AutoGen can\\nhelp achieve outstanding performance on many tasks, and enable innovative ways of using LLMs,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='help achieve outstanding performance on many tasks, and enable innovative ways of using LLMs,\\nwhile reducing development effort. (Section 3 and Appendix D)\\n2 The AutoGen Framework\\nTo reduce the effort required for developers to create complex LLM applications across various do-\\nmains, a core design principle of AutoGen is to streamline and consolidate multi-agent workflows\\nusing multi-agent conversations. This approach also aims to maximize the reusability of imple-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='using multi-agent conversations. This approach also aims to maximize the reusability of imple-\\nmented agents. This section introduces the two key concepts of AutoGen : conversable agents and\\nconversation programming.\\n2.1 Conversable Agents\\nInAutoGen , aconversable agent is an entity with a specific role that can pass messages to send and\\nreceive information to and from other conversable agents, e.g., to start or continue a conversation. It', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='receive information to and from other conversable agents, e.g., to start or continue a conversation. It\\nmaintains its internal context based on sent and received messages and can be configured to possess\\na set of capabilities, e.g., enabled by LLMs, tools, or human input, etc. The agents can act according\\nto programmed behavior patterns described next.\\nAgent capabilities powered by LLMs, humans, and tools. Since an agent‚Äôs capabilities directly', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='Agent capabilities powered by LLMs, humans, and tools. Since an agent‚Äôs capabilities directly\\ninfluence how it processes and responds to messages, AutoGen allows flexibility to endow its agents\\nwith various capabilities. AutoGen supports many common composable capabilities for agents,\\nincluding 1) LLMs. LLM-backed agents exploit many capabilities of advanced LLMs such as role\\nplaying, implicit state inference and progress making conditioned on conversation history, providing', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='playing, implicit state inference and progress making conditioned on conversation history, providing\\nfeedback, adapting from feedback, and coding. These capabilities can be combined in different ways\\nvia novel prompting techniques4to increase an agent‚Äôs skill and autonomy. AutoGen also offers\\nenhanced LLM inference features such as result caching, error handling, message templating, etc.,\\nvia an enhanced LLM inference layer. 2) Humans. Human involvement is desired or even essential', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='via an enhanced LLM inference layer. 2) Humans. Human involvement is desired or even essential\\nin many LLM applications. AutoGen lets a human participate in agent conversation via human-\\nbacked agents, which could solicit human inputs at certain rounds of a conversation depending on\\nthe agent configuration. The default user proxy agent allows configurable human involvement levels\\nand patterns, e.g., frequency and conditions for requesting human input including the option for', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='and patterns, e.g., frequency and conditions for requesting human input including the option for\\nhumans to skip providing input. 3) Tools. Tool-backed agents have the capability to execute tools\\nvia code execution or function execution. For example, the default user proxy agent in AutoGen is\\nable to execute code suggested by LLMs, or make LLM-suggested function calls.\\nAgent customization and cooperation. Based on application-specific needs, each agent can be', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='Agent customization and cooperation. Based on application-specific needs, each agent can be\\nconfigured to have a mix of basic back-end types to display complex behavior in multi-agent con-\\nversations. AutoGen allows easy creation of agents with specialized capabilities and roles by reusing\\nor extending the built-in agents. The yellow-shaded area of Figure 2 provides a sketch of the built-in\\nagents in AutoGen . The ConversableAgent class is the highest-level agent abstraction and, by', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='agents in AutoGen . The ConversableAgent class is the highest-level agent abstraction and, by\\ndefault, can use LLMs, humans, and tools. The AssistantAgent andUserProxyAgent are two\\npre-configured ConversableAgent subclasses, each representing a common usage mode, i.e., act-\\ning as an AI assistant (backed by LLMs) and acting as a human proxy to solicit human input or\\nexecute code/function calls (backed by humans and/or tools).', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='execute code/function calls (backed by humans and/or tools).\\nIn the example on the right-hand side of Figure 1, an LLM-backed assistant agent and a tool- and\\nhuman-backed user proxy agent are deployed together to tackle a task. Here, the assistant agent\\ngenerates a solution with the help of LLMs and passes the solution to the user proxy agent. Then,\\nthe user proxy agent solicits human inputs or executes the assistant‚Äôs code and passes the results as\\nfeedback back to the assistant.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='feedback back to the assistant.\\n4Appendix C presents an example of such novel prompting techniques which empowers the default LLM-\\nbacked assistant agent in AutoGen to converse with other agents in multi-step problem solving.\\n3', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='2 Initiate Conversations:A.initiate_chat(‚ÄúPlot a chart of META and TESLA stock price change YTD.‚Äù, B)\\nAssistant BUser Proxy AAutoGenAgents\\nDeveloper Code# This funcwill be invoked in generate_replyA.register_reply(B,  reply_func_A2B)def reply_func_A2B(msg):ouput= input_from_human()‚Ä¶if not ouput:if msg includes code:output = execute(msg)return outputConversableAgent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='AssistantAgentUserProxyAgenthuman_input_mode= ‚ÄúNEVER‚Äùcode_execution_config= FalseDEFAULT_SYSTEM_MESSAGE = ‚ÄúYou are a helpful AI assistant‚Ä¶In the following cases, suggest python code‚Ä¶‚Äùhuman_input_mode=‚ÄúALWAYS‚Äù\\nGroupChatManagerhuman_input_mode= ‚ÄúNEVER‚Äùgroup_chat= [              ] \\n# Note: when no reply funcis registered, a list of default reply functions will be used. Agent Customization:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='Program ExecutionPlot a chart of META and TESLA stock price change YTD.Execute the following code‚Ä¶sendreceivereceiveConversation-Centric Computationgenerate_replyError: package yfinanceis not installedsendgenerate_replySorry! Please first pip install yfinanceand then executeConversation-Driven Control Flowgenerate_replyThe Resulting Automated Agent Chat:‚Ä¶1.2 Register a Custom Reply Func:1.1 Define Agents:Unified Conversation Interfaces:‚Ä¢send‚Ä¢receive ‚Ä¢generate_reply', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='Figure 2: Illustration of how to use AutoGen to program a multi-agent conversation. The top sub-\\nfigure illustrates the built-in agents provided by AutoGen , which have unified conversation interfaces\\nand can be customized. The middle sub-figure shows an example of using AutoGen to develop\\na two-agent system with a custom reply function. The bottom sub-figure illustrates the resulting\\nautomated agent chat from the two-agent system during program execution.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='automated agent chat from the two-agent system during program execution.\\nBy allowing custom agents that can converse with each other, conversable agents in AutoGen serve\\nas a useful building block. However, to develop applications where agents make meaningful progress\\non tasks, developers also need to be able to specify and mold these multi-agent conversations.\\n2.2 Conversation Programming\\nAs a solution to the above problem, AutoGen utilizes conversation programming , a paradigm that', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='2.2 Conversation Programming\\nAs a solution to the above problem, AutoGen utilizes conversation programming , a paradigm that\\nconsiders two concepts: the first is computation ‚Äì the actions agents take to compute their response\\nin a multi-agent conversation. And the second is control flow ‚Äì the sequence (or conditions) un-\\nder which these computations happen. As we will show in the applications section, the ability to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='der which these computations happen. As we will show in the applications section, the ability to\\nprogram these helps implement many flexible multi-agent conversation patterns. In AutoGen , these\\ncomputations are conversation-centric. An agent takes actions relevant to the conversations it is\\ninvolved in and its actions result in message passing for consequent conversations (unless a termina-\\ntion condition is satisfied). Similarly, control flow is conversation-driven ‚Äì the participating agents‚Äô', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='tion condition is satisfied). Similarly, control flow is conversation-driven ‚Äì the participating agents‚Äô\\ndecisions on which agents to send messages to and the procedure of computation are functions of the\\ninter-agent conversation. This paradigm helps one to reason intuitively about a complex workflow\\nas agent action taking and conversation message-passing between agents.\\nFigure 2 provides a simple illustration. The bottom sub-figure shows how individual agents perform', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='Figure 2 provides a simple illustration. The bottom sub-figure shows how individual agents perform\\ntheir role-specific, conversation-centric computations to generate responses (e.g., via LLM inference\\ncalls and code execution). The task progresses through conversations displayed in the dialog box.\\nThe middle sub-figure demonstrates a conversation-based control flow. When the assistant receives\\na message, the user proxy agent typically sends the human input as a reply. If there is no input, it', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='a message, the user proxy agent typically sends the human input as a reply. If there is no input, it\\nexecutes any code in the assistant‚Äôs message instead.\\n4', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='AutoGen features the following design patterns to facilitate conversation programming:\\n1.Unified interfaces and auto-reply mechanisms for automated agent chat. Agents in\\nAutoGen have unified conversation interfaces for performing the corresponding conversation-\\ncentric computation, including a send/receive function for sending/receiving messages and a\\ngenerate reply function for taking actions and generating a response based on the received', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='generate reply function for taking actions and generating a response based on the received\\nmessage. AutoGen also introduces and by default adopts an agent auto-reply mechanism to\\nrealize conversation-driven control: Once an agent receives a message from another agent, it au-\\ntomatically invokes generate reply and sends the reply back to the sender unless a termination\\ncondition is satisfied. AutoGen provides built-in reply functions based on LLM inference, code', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='condition is satisfied. AutoGen provides built-in reply functions based on LLM inference, code\\nor function execution, or human input. One can also register custom reply functions to customize\\nthe behavior pattern of an agent, e.g., chatting with another agent before replying to the sender\\nagent. Under this mechanism, once the reply functions are registered, and the conversation is\\ninitialized, the conversation flow is naturally induced, and thus the agent conversation proceeds', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='initialized, the conversation flow is naturally induced, and thus the agent conversation proceeds\\nnaturally without any extra control plane, i.e., a special module that controls the conversation\\nflow. For example, with the developer code in the blue-shaded area (marked ‚ÄúDeveloper Code‚Äù)\\nof Figure 2, one can readily trigger the conversation among the agents, and the conversation\\nwould proceed automatically, as shown in the dialog box in the grey shaded area (marked ‚ÄúPro-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='would proceed automatically, as shown in the dialog box in the grey shaded area (marked ‚ÄúPro-\\ngram Execution‚Äù) of Figure 2. The auto-reply mechanism provides a decentralized, modular, and\\nunified way to define the workflow.\\n2.Control by fusion of programming and natural language. AutoGen allows the usage of\\nprogramming and natural language in various control flow management patterns: 1) Natural-\\nlanguage control via LLMs. InAutoGen , one can control the conversation flow by prompting', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='language control via LLMs. InAutoGen , one can control the conversation flow by prompting\\nthe LLM-backed agents with natural language. For instance, the default system message of the\\nbuilt-in AssistantAgent inAutoGen uses natural language to instruct the agent to fix errors\\nand generate code again if the previous result indicates there are errors. It also guides the agent\\nto confine the LLM output to certain structures, making it easier for other tool-backed agents to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='to confine the LLM output to certain structures, making it easier for other tool-backed agents to\\nconsume. For example, instructing the agent to reply with ‚ÄúTERMINATE‚Äù when all tasks are\\ncompleted to terminate the program. More concrete examples of natural language controls can\\nbe found in Appendix C. 2) Programming-language control. InAutoGen , Python code can be\\nused to specify the termination condition, human input mode, and tool execution logic, e.g., the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='used to specify the termination condition, human input mode, and tool execution logic, e.g., the\\nmax number of auto replies. One can also register programmed auto-reply functions to control\\nthe conversation flow with Python code, as shown in the code block identified as ‚ÄúConversation-\\nDriven Control Flow‚Äù in Figure 2. 3) Control transition between natural and programming\\nlanguage. AutoGen also supports flexible control transition between natural and programming', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='language. AutoGen also supports flexible control transition between natural and programming\\nlanguage. One can achieve transition from code to natural-language control by invoking an LLM\\ninference containing certain control logic in a customized reply function; or transition from nat-\\nural language to code control via LLM-proposed function calls (Eleti et al., 2023).\\nIn the conversation programming paradigm, one can realize multi-agent conversations of diverse', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='In the conversation programming paradigm, one can realize multi-agent conversations of diverse\\npatterns. In addition to static conversation with predefined flow, AutoGen also supports dynamic\\nconversation flows with multiple agents. AutoGen provides two general ways to achieve this: 1)\\nCustomized generate reply function: within the customized generate reply function, one\\nagent can hold the current conversation while invoking conversations with other agents depending', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='agent can hold the current conversation while invoking conversations with other agents depending\\non the content of the current message and context. 2) Function calls: In this approach, LLM decides\\nwhether or not to call a particular function depending on the conversation status. By messaging\\nadditional agents in the called functions, the LLM can drive dynamic multi-agent conversation. In\\naddition, AutoGen supports more complex dynamic group chat via built-in GroupChatManager ,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='addition, AutoGen supports more complex dynamic group chat via built-in GroupChatManager ,\\nwhich can dynamically select the next speaker and then broadcast its response to other agents. We\\nelaborate on this feature and its application in Section 3. We provide implemented working systems\\nto showcase all these different patterns, with some of them visualized in Figure 3.\\n3 Applications of AutoGen\\nWe demonstrate six applications using AutoGen (see Figure 3) to illustrate its potential in simplify-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='3 Applications of AutoGen\\nWe demonstrate six applications using AutoGen (see Figure 3) to illustrate its potential in simplify-\\ning the development of high-performance multi-agent applications. These applications are selected\\nbased on their real-world relevance (A1, A2, A4, A5, A6), problem difficulty and solving capabil-\\nities enabled by AutoGen (A1, A2, A3, A4), and innovative potential (A5, A6). Together, these\\ncriteria showcase AutoGen ‚Äôs role in advancing the LLM-application landscape.\\n5', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='A1. Math Problem Solving\\nA4. Multi-agent CodingCommander\\nSafeguard\\nWriter\\nA6. Conversational ChessA2. Retrieval-augmented ChatRetrieval-augmentedAssistantRetrieval-augmentedUser Proxy\\nChess Board\\nHuman/AI Chess Player A\\nHuman/AI Chess Player B\\nStudent\\nAssistant\\nAssistantExpert\\nAsk  expert\\nBroadcast\\nManager\\nSpeak\\nA5. Dynamic Group Chat\\nALFWorldExecutorAssistant\\nGrounding Agent\\nA3. ALF ChatFigure 3: Six examples of diverse applications built using AutoGen . Their conversation patterns', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='Grounding Agent\\nA3. ALF ChatFigure 3: Six examples of diverse applications built using AutoGen . Their conversation patterns\\nshow AutoGen ‚Äôs flexibility and power.\\nA1: Math Problem Solving\\nMathematics is a foundational discipline and the promise of leveraging LLMs to assist with math\\nproblem solving opens up a new plethora of applications and avenues for exploration, including per-\\nsonalized AI tutoring, AI research assistance, etc. This section demonstrates how AutoGen can help', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='sonalized AI tutoring, AI research assistance, etc. This section demonstrates how AutoGen can help\\ndevelop LLM applications for math problem solving, showcasing strong performance and flexibility\\nin supporting various problem-solving paradigms.\\n(Scenario 1 ) We are able to build a system for autonomous math problem solving by directly reusing\\ntwo built-in agents from AutoGen . We evaluate our system and several alternative approaches,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='two built-in agents from AutoGen . We evaluate our system and several alternative approaches,\\nincluding open-source methods such as Multi-Agent Debate (Liang et al., 2023), LangChain Re-\\nAct (LangChain, 2023), vanilla GPT-4, and commercial products ChatGPT + Code Interpreter, and\\nChatGPT + Plugin (Wolfram Alpha), on the MATH (Hendrycks et al., 2021) dataset and summarize\\nthe results in Figure 4a. We perform evaluations over 120 randomly selected level-5 problems and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='the results in Figure 4a. We perform evaluations over 120 randomly selected level-5 problems and\\non the entire5test dataset from MATH. The results show that the built-in agents from AutoGen al-\\nready yield better performance out of the box compared to the alternative approaches, even including\\nthe commercial ones. ( Scenario 2 ) We also showcase a human-in-the-loop problem-solving process\\nwith the help of AutoGen . To incorporate human feedback with AutoGen , one only needs to set', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='with the help of AutoGen . To incorporate human feedback with AutoGen , one only needs to set\\nhuman input mode=‚ÄòALWAYS‚Äô in the UserProxyAgent of the system in scenario 1. We demon-\\nstrate that this system can effectively incorporate human inputs to solve challenging problems that\\ncannot be solved without humans. ( Scenario 3 ) We further demonstrate a novel scenario where\\nmultiple human users can participate in the conversations during the problem-solving process. Our', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='multiple human users can participate in the conversations during the problem-solving process. Our\\nexperiments and case studies for these scenarios show that AutoGen enables better performance or\\nnew experience compared to other solutions we experimented with. Due to the page limit, details of\\nthe evaluation, including case studies in three scenarios are in Appendix D.\\nA2: Retrieval-Augmented Code Generation and Question Answering', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='A2: Retrieval-Augmented Code Generation and Question Answering\\nRetrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic\\nlimitations of LLMs by incorporating external documents. In this section, we employ AutoGen to\\nbuild a Retrieval-Augmented Generation (RAG) system (Lewis et al., 2020; Parvez et al., 2021)\\nnamed Retrieval-augmented Chat. The system consists of two agents: a Retrieval-augmented User', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='named Retrieval-augmented Chat. The system consists of two agents: a Retrieval-augmented User\\nProxy agent and a Retrieval-augmented Assistant agent, both of which are extended from built-in\\nagents from AutoGen . The Retrieval-augmented User Proxy includes a vector database (Chroma,\\n5We did not evaluate ChatGPT on the whole dataset since it requires substantial manual effort and is re-\\nstricted by its hourly message-number limitation. Multi-agent debate and LangChain ReAct were also not', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='stricted by its hourly message-number limitation. Multi-agent debate and LangChain ReAct were also not\\nevaluated since they underperformed vanilla GPT-4 on the smaller test set.\\n6', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='AutoGen ChatGPT\\n+CodeChatGPT\\n+PluginGPT-4 Multi-Agent\\nDebateLangChain\\nReAct\\nMethods01020304050607080Success Ratio (%)52.5%\\n48.33%\\n45.0%\\n30.0%\\n26.67%\\n23.33%69.48%\\n55.18%120 Level-5 problems\\nWhole Dataset(a) A1: Performance on MATH (w/ GPT-4).\\nF1 Recall\\nMetrics01020304050607080Percentage (%)25.88%66.65%\\n15.12%58.56%\\n22.79%62.59%AutoGen\\nAuotGen W/O interactive retrieval\\nDPR (b) A2: Q&A tasks (w/ GPT-3.5).\\nAutoGen (3 agent) AutoGen (2 agent) ReAct\\nMethods020406080100Success Ratio (%)69%\\n54% 54%77%\\n63%66%Average', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='AutoGen (3 agent) AutoGen (2 agent) ReAct\\nMethods020406080100Success Ratio (%)69%\\n54% 54%77%\\n63%66%Average\\nBest of 3\\n(c) A3: Performance on ALFWorld.\\nF1 Recall\\nMetrics020406080100Percentage (%)96.00%98.00%\\n88.00%\\n78.00%83.00%\\n72.00%\\n48.00%\\n32.00%Multi-GPT4\\nSingle-GPT4\\nMulti-GPT3.5\\nSingle-GPT3.5 (d) A4: Performance on OptiGuide.\\nFigure 4: Performance on four applications A1-A4. (a) shows that AutoGen agents can be used', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='Figure 4: Performance on four applications A1-A4. (a) shows that AutoGen agents can be used\\nout of the box to achieve the most competitive performance on math problem solving tasks; (b)\\nshows that AutoGen can be used to realize effective retrieval augmentation and realize a novel\\ninteractive retrieval feature to boost performance on Q&A tasks; (c) shows that AutoGen can be used\\nto introduce a three-agent system with a grounding agent to improve performance on ALFWorld;', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='to introduce a three-agent system with a grounding agent to improve performance on ALFWorld;\\n(d) shows that a multi-agent design is helpful in boosting performance in coding tasks that need\\nsafeguards.\\n2023) with SentenceTransformers (Reimers & Gurevych, 2019) as the context retriever. A detailed\\nworkflow description of the Retrieval-augmented Chat is provided in Appendix D.\\nWe evaluate Retrieval-augmented Chat in both question-answering and code-generation scenarios.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='We evaluate Retrieval-augmented Chat in both question-answering and code-generation scenarios.\\n(Scenario 1 ) We first perform an evaluation regarding natural question answering on the Natural\\nQuestions dataset (Kwiatkowski et al., 2019) and report results in Figure 4b. In this evaluation, we\\ncompare our system with DPR (Dense Passage Retrieval) following an existing evaluation6prac-\\ntice (Adlakha et al., 2023). Leveraging the conversational design and natural-language control,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='tice (Adlakha et al., 2023). Leveraging the conversational design and natural-language control,\\nAutoGen introduces a novel interactive retrieval feature in this application: whenever the retrieved\\ncontext does not contain the information, instead of terminating, the LLM-based assistant would\\nreply ‚Äú Sorry, I cannot find any information about... UPDATE CONTEXT. ‚Äù which will invoke more\\nretrieval attempts. We conduct an ablation study in which we prompt the assistant agent to say ‚ÄúI', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='retrieval attempts. We conduct an ablation study in which we prompt the assistant agent to say ‚ÄúI\\ndon‚Äôt know‚Äù instead of ‚ÄúUPDATE CONTEXT. ‚Äù in cases where relevant information is not found,\\nand report results in Figure 4b. The results show that the interactive retrieval mechanism indeed\\nplays a non-trivial role in the process. We give a concrete example and results using this appealing\\nfeature in Appendix D. ( Scenario 2 ) We further demonstrate how Retrieval-augmented Chat aids in', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='feature in Appendix D. ( Scenario 2 ) We further demonstrate how Retrieval-augmented Chat aids in\\ngenerating code based on a given codebase that contains code not included in GPT-4‚Äôs training data.\\nEvaluation and demonstration details for both scenarios are included in Appendix D.\\n6The results of DPR with GPT-3.5 shown in Figure 4b are from (Adlakha et al., 2023). We use GPT-3.5 as\\na shorthand for GPT-3.5-turbo.\\n7', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='A3: Decision Making in Text World Environments\\nIn this subsection, we demonstrate how AutoGen can be used to develop effective applications that\\ninvolve interactive or online decision making. We perform the study using the ALFWorld (Shridhar\\net al., 2021) benchmark, which includes a diverse collection of synthetic language-based interactive\\ndecision-making tasks in household environments.\\nWith AutoGen , we implemented a two-agent system to solve tasks from ALFWorld. It consists of', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='With AutoGen , we implemented a two-agent system to solve tasks from ALFWorld. It consists of\\nan LLM-backed assistant agent responsible for suggesting plans to conduct a task and an executor\\nagent responsible for executing actions in the ALFWorld environments. This system integrates Re-\\nAct prompting (Yao et al., 2022), and is able to achieve similar performance. A common challenge\\nencountered in both ReAct and the AutoGen -based two-agent system is their occasional inability to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='encountered in both ReAct and the AutoGen -based two-agent system is their occasional inability to\\nleverage basic commonsense knowledge about the physical world. This deficiency can lead to the\\nsystem getting stuck in a loop due to repetitive errors. Fortunately, the modular design of AutoGen\\nallows us to address this issue effectively: With AutoGen , we are able to introduce a grounding\\nagent, which supplies crucial commonsense knowledge‚Äìsuch as ‚ÄúYou must find and take the object', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='agent, which supplies crucial commonsense knowledge‚Äìsuch as ‚ÄúYou must find and take the object\\nbefore you can examine it. You must go to where the target object is before you can use it. ‚Äù ‚Äìwhenever\\nthe system exhibits early signs of recurring errors. It significantly enhances the system‚Äôs ability to\\navoid getting entangled in error loops. We compare the task-solving performance of the two variants\\nof our system with GPT-3.5-turbo and ReAct7on the 134 unseen tasks from ALFWorld and report', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='of our system with GPT-3.5-turbo and ReAct7on the 134 unseen tasks from ALFWorld and report\\nresults in Figure 4c. The results show that introducing a grounding agent could bring in a 15%\\nperformance gain on average. Upon examining the systems‚Äô outputs, we observe that the grounding\\nagent, by delivering background commonsense knowledge at the right junctures, significantly miti-\\ngated the tendency of the system to persist with a flawed plan, thereby avoiding the creation of error', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='gated the tendency of the system to persist with a flawed plan, thereby avoiding the creation of error\\nloops. For an example trajectory comparing the systems see Appendix D, Figure 10.\\nA4: Multi-Agent Coding\\nIn this subsection, we use AutoGen to build a multi-agent coding system based on OptiGuide (Li\\net al., 2023a), a system that excels at writing code to interpret optimization solutions and answer\\nuser questions, such as exploring the implications of changing a supply-chain decision or under-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='user questions, such as exploring the implications of changing a supply-chain decision or under-\\nstanding why the optimizer made a particular choice. The second sub-figure of Figure 3 shows the\\nAutoGen -based implementation. The workflow is as follows: the end user sends questions, such as\\n‚ÄúWhat if we prohibit shipping from supplier 1 to roastery 2? ‚Äù to the Commander agent. The Com-\\nmander coordinates with two assistant agents, including the Writer and the Safeguard, to answer', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='mander coordinates with two assistant agents, including the Writer and the Safeguard, to answer\\nthe question. The Writer will craft code and send the code to the Commander. After receiving the\\ncode, the Commander checks the code safety with the Safeguard; if cleared, the Commander will\\nuse external tools (e.g., Python) to execute the code, and request the Writer to interpret the execution\\nresults. For instance, the writer may say ‚Äú if we prohibit shipping from supplier 1 to roastery 2, the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='results. For instance, the writer may say ‚Äú if we prohibit shipping from supplier 1 to roastery 2, the\\ntotal cost would increase by 10.5%. ‚Äù The Commander then provides this concluding answer to the\\nend user. If, at a particular step, there is an exception, e.g., security red flag raised by Safeguard, the\\nCommander redirects the issue back to the Writer with debugging information. The process might\\nbe repeated multiple times until the user‚Äôs question is answered or timed-out.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='be repeated multiple times until the user‚Äôs question is answered or timed-out.\\nWith AutoGen the core workflow code for OptiGuide was reduced from over 430 lines to 100 lines,\\nleading to significant productivity improvement. We provide a detailed comparison of user expe-\\nrience with ChatGPT+Code Interpreter and AutoGen -based OptiGuide in Appendix D, where we\\nshow that AutoGen -based OptiGuide could save around 3x of user‚Äôs time and reduce user interac-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='show that AutoGen -based OptiGuide could save around 3x of user‚Äôs time and reduce user interac-\\ntions by 3 - 5 times on average. We also conduct an ablation showing that multi-agent abstraction is\\nnecessary. Specifically, we construct a single-agent approach where a single agent conducts both the\\ncode-writing and safeguard processes. We tested the single- and multi-agent approaches on a dataset\\nof 100 coding tasks, which is crafted to include equal numbers of safe and unsafe tasks. Evaluation', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='of 100 coding tasks, which is crafted to include equal numbers of safe and unsafe tasks. Evaluation\\nresults as reported in Figure 4d show that the multi-agent design boosts the F-1 score in identifying\\nunsafe code by 8% (with GPT-4) and 35% (with GPT-3.5-turbo).\\n7Results of ReAct are obtained by directly running its official code with default settings. The code uses\\ntext-davinci-003 as backend LM and does not support GPT-3.5-turbo or GPT-4.\\n8', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='A5: Dynamic Group Chat\\nAutoGen provides native support for a dynamic group chat communication pattern, in which par-\\nticipating agents share the same context and converse with the others in a dynamic manner instead\\nof following a pre-defined order. Dynamic group chat relies on ongoing conversations to guide the\\nflow of interaction among agents. These make dynamic group chat ideal for situations where col-\\nlaboration without strict communication order is beneficial. In AutoGen , the GroupChatManager', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='laboration without strict communication order is beneficial. In AutoGen , the GroupChatManager\\nclass serves as the conductor of conversation among agents and repeats the following three steps:\\ndynamically selecting a speaker, collecting responses from the selected speaker, and broadcasting\\nthe message (Figure 3-A5). For the dynamic speaker-selection component, we use a role-play style\\nprompt. Through a pilot study on 12 manually crafted complex tasks, we observed that compared', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='prompt. Through a pilot study on 12 manually crafted complex tasks, we observed that compared\\nto a prompt that is purely based on the task, utilizing a role-play prompt often leads to more effec-\\ntive consideration of both conversation context and role alignment during the problem-solving and\\nspeaker-selection process. Consequently, this leads to a higher success rate and fewer LLM calls.\\nWe include detailed results in Appendix D.\\nA6: Conversational Chess', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='We include detailed results in Appendix D.\\nA6: Conversational Chess\\nUsing AutoGen , we developed Conversational Chess, a natural language interface game shown in\\nthe last sub-figure of Figure 3. It features built-in agents for players, which can be human or LLM,\\nand a third-party board agent to provide information and validate moves based on standard rules.\\nWith AutoGen , we enabled two essential features: (1) Natural, flexible, and engaging game dynam-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='With AutoGen , we enabled two essential features: (1) Natural, flexible, and engaging game dynam-\\nics, enabled by the customizable agent design in AutoGen . Conversational Chess supports a range\\nof game-play patterns, including AI-AI, AI-human, and human-human, with seamless switching\\nbetween these modes during a single game. An illustrative example of these entertaining game dy-\\nnamics can be found in Figure 15, Appendix D. (2) Grounding, which is a crucial aspect to maintain', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='namics can be found in Figure 15, Appendix D. (2) Grounding, which is a crucial aspect to maintain\\ngame integrity. During gameplay, the board agent checks each proposed move for legality; if a move\\nis invalid, the agent responds with an error, prompting the player agent to re-propose a legal move\\nbefore continuing. This process ensures that only valid moves are played and helps maintain a con-\\nsistent gaming experience. As an ablation study, we removed the board agent and instead only relied', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='sistent gaming experience. As an ablation study, we removed the board agent and instead only relied\\non a relevant prompt ‚Äúyou should make sure both you and the opponent are making legal moves‚Äù to\\nground their move. The results highlighted that without the board agent, illegitimate moves caused\\ngame disruptions. The modular design offered flexibility, allowing swift adjustments to the board\\nagent in response to evolving game rules or varying chess rule variants. A comprehensive demon-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='agent in response to evolving game rules or varying chess rule variants. A comprehensive demon-\\nstration of this ablation study is in Appendix D.\\n4 Discussion\\nWe introduced an open-source library, AutoGen , that incorporates the paradigms of conversable\\nagents and conversation programming. This library utilizes capable agents that are well-suited for\\nmulti-agent cooperation. It features a unified conversation interface among the agents, along with', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='multi-agent cooperation. It features a unified conversation interface among the agents, along with\\nan auto-reply mechanisms, which help establish an agent-interaction interface that capitalizes on the\\nstrengths of chat-optimized LLMs with broad capabilities while accommodating a wide range of\\napplications. AutoGen serves as a general framework for creating and experimenting with multi-\\nagent systems that can easily fulfill various practical requirements, such as reusing, customizing,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='agent systems that can easily fulfill various practical requirements, such as reusing, customizing,\\nand extending existing agents, as well as programming conversations between them.\\nOur experiments, as detailed in Section 3, demonstrate that this approach offers numerous benefits.\\nThe adoption of AutoGen has resulted in improved performance (over state-of-the-art approaches),\\nreduced development code, and decreased manual burden for existing applications. It offers flex-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='reduced development code, and decreased manual burden for existing applications. It offers flex-\\nibility to developers, as demonstrated in A1 (scenario 3), A5, and A6, where AutoGen enables\\nmulti-agent chats to follow a dynamic pattern rather than fixed back-and-forth interactions. It allows\\nhumans to engage in activities alongside multiple AI agents in a conversational manner. Despite the\\ncomplexity of these applications (most involving more than two agents or dynamic multi-turn agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='complexity of these applications (most involving more than two agents or dynamic multi-turn agent\\ncooperation), the implementation based on AutoGen remains straightforward. Dividing tasks among\\nseparate agents promotes modularity. Furthermore, since each agent can be developed, tested, and\\nmaintained separately, this approach simplifies overall development and code management.\\n9', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='Although this work is still in its early experimental stages, it paves the way for numerous future\\ndirections and research opportunities. For instance, we can explore effective integration of existing\\nagent implementations into our multi-agent framework and investigate the optimal balance between\\nautomation and human control in multi-agent workflows. As we further develop and refine AutoGen ,\\nwe aim to investigate which strategies, such as agent topology and conversation patterns, lead to the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='we aim to investigate which strategies, such as agent topology and conversation patterns, lead to the\\nmost effective multi-agent conversations while optimizing the overall efficiency, among other fac-\\ntors. While increasing the number of agents and other degrees of freedom presents opportunities for\\ntackling more complex problems, it may also introduce new safety challenges that require additional\\nstudies and careful consideration.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='studies and careful consideration.\\nWe provide more discussion in Appendix B, including guidelines for using AutoGen and direction\\nof future work. We hope AutoGen will help improve many LLM applications in terms of speed of\\ndevelopment, ease of experimentation, and overall effectiveness and safety. We actively welcome\\ncontributions from the broader community.\\nEthics statement\\nThere are several potential ethical considerations that could arise from the development and use of\\ntheAutoGen framework.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='There are several potential ethical considerations that could arise from the development and use of\\ntheAutoGen framework.\\n‚Ä¢ Privacy and Data Protection: The framework allows for human participation in conversations\\nbetween agents. It is important to ensure that user data and conversations are protected, and that\\ndevelopers use appropriate measures to safeguard privacy.\\n‚Ä¢ Bias and Fairness: LLMs have been shown to exhibit biases present in their training data (Navigli', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='‚Ä¢ Bias and Fairness: LLMs have been shown to exhibit biases present in their training data (Navigli\\net al., 2023). When using LLMs in the AutoGen framework, it is crucial to address and mitigate\\nany biases that may arise in the conversations between agents. Developers should be aware of\\npotential biases and take steps to ensure fairness and inclusivity.\\n‚Ä¢ Accountability and Transparency: As discussed in the future work section, as the framework in-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='‚Ä¢ Accountability and Transparency: As discussed in the future work section, as the framework in-\\nvolves multiple agents conversing and cooperating, it is important to establish clear accountability\\nand transparency mechanisms. Users should be able to understand and trace the decision-making\\nprocess of the agents involved in order to ensure accountability and address any potential issues\\nor biases.\\n‚Ä¢ Trust and Reliance: AutoGen leverages human understanding and intelligence while providing', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='or biases.\\n‚Ä¢ Trust and Reliance: AutoGen leverages human understanding and intelligence while providing\\nautomation through conversations between agents. It is important to consider the impact of this\\ninteraction on user experience, trust, and reliance on AI systems. Clear communication and user\\neducation about the capabilities and limitations of the system will be essential (Cai et al., 2019).\\n‚Ä¢ Unintended Consequences: As discussed before, the use of multi-agent conversations and automa-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='‚Ä¢ Unintended Consequences: As discussed before, the use of multi-agent conversations and automa-\\ntion in complex tasks may have unintended consequences. In particular, allowing LLM agents to\\nmake changes in external environments through code execution or function calls, such as installing\\npackages, could be risky. Developers should carefully consider the potential risks and ensure that\\nappropriate safeguards are in place to prevent harm or negative outcomes.\\nAcknowledgements', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='appropriate safeguards are in place to prevent harm or negative outcomes.\\nAcknowledgements\\nThe work presented in this report was made possible through discussions and feedback from Peter\\nLee, Johannes Gehrke, Eric Horvitz, Steven Lucco, Umesh Madan, Robin Moeur, Piali Choud-\\nhury, Saleema Amershi, Adam Fourney, Victor Dibia, Guoqing Zheng, Corby Rosset, Ricky Loynd,\\nEce Kamar, Rafah Hosn, John Langford, Ida Momennejad, Brian Krabach, Taylor Webb, Shanka', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='Ece Kamar, Rafah Hosn, John Langford, Ida Momennejad, Brian Krabach, Taylor Webb, Shanka\\nSubhra Mondal, Wei-ge Chen, Robert Gruen, Yinan Li, Yue Wang, Suman Nath, Tanakorn Leesat-\\napornwongsa, Xin Wang, Shishir Patil, Tianjun Zhang, Saehan Jo, Ishai Menache, Kontantina Mel-\\nlou, Runlong Zhou, Feiran Jia, Hamed Khanpour, Hamid Palangi, Srinagesh Sharma, Julio Albinati\\nCortez, Amin Saied, Yuzhe Ma, Dujian Ding, Linyong Nan, Prateek Yadav, Shannon Shen, Ankur', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='Cortez, Amin Saied, Yuzhe Ma, Dujian Ding, Linyong Nan, Prateek Yadav, Shannon Shen, Ankur\\nMallick, Mark Encarnaci ¬¥on, Lars Liden, Tianwei Yue, Julia Kiseleva, Anastasia Razdaibiedina, and\\nLuciano Del Corro. Qingyun Wu would like to acknowledge the funding and research support from\\nthe College of Information Science and Technology at Penn State University.\\n10', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='References\\nVaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Eval-\\nuating correctness and faithfulness of instruction-following models for question answering. arXiv\\npreprint arXiv:2307.16877 , 2023.\\nSaleema Amershi, Dan Weld, Mihaela V orvoreanu, Adam Fourney, Besmira Nushi, Penny Col-\\nlisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. Guidelines for human-ai\\ninteraction. In Proceedings of the 2019 chi conference on human factors in computing systems ,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='interaction. In Proceedings of the 2019 chi conference on human factors in computing systems ,\\n2019.\\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man ¬¥e. Con-\\ncrete problems in ai safety, 2016.\\nAutoGPT. Documentation ‚Äî auto-gpt. https://docs.agpt.co/ , 2023.\\nBabyAGI. Github ‚Äî babyagi. https://github.com/yoheinakajima/babyagi , 2023.\\nCarrie J. Cai, Samantha Winter, David F. Steiner, Lauren Wilcox, and Michael Terry. ‚Äùhello ai‚Äù:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='Carrie J. Cai, Samantha Winter, David F. Steiner, Lauren Wilcox, and Michael Terry. ‚Äùhello ai‚Äù:\\nUncovering the onboarding needs of medical practitioners for human-ai collaborative decision-\\nmaking. Proceedings of the ACM on Human-Computer Interaction , 2019.\\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as\\ntool makers. arXiv preprint arXiv:2305.17126 , 2023.\\nChroma. Chromadb. https://github.com/chroma-core/chroma , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='tool makers. arXiv preprint arXiv:2305.17126 , 2023.\\nChroma. Chromadb. https://github.com/chroma-core/chroma , 2023.\\nVictor Dibia. LIDA: A tool for automatic generation of grammar-agnostic visualizations and info-\\ngraphics using large language models. In Proceedings of the 61st Annual Meeting of the Associ-\\nation for Computational Linguistics (Volume 3: System Demonstrations) , Toronto, Canada, July\\n2023. Association for Computational Linguistics.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='2023. Association for Computational Linguistics.\\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt. arXiv\\npreprint arXiv:2304.07590 , 2023.\\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improv-\\ning factuality and reasoning in language models through multiagent debate. arXiv preprint\\narXiv:2305.14325 , 2023.\\nAtty Eleti, Jeff Harris, and Logan Kilpatrick. Function calling and other api updates. https:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='arXiv:2305.14325 , 2023.\\nAtty Eleti, Jeff Harris, and Logan Kilpatrick. Function calling and other api updates. https:\\n//openai.com/blog/function-calling-and-other-api-updates , 2023.\\nGuidance. Guidance. https://github.com/guidance-ai/guidance , 2023.\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv\\npreprint arXiv:2103.03874 , 2021.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv\\npreprint arXiv:2103.03874 , 2021.\\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven\\nKa Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for\\nmulti-agent collaborative framework. arXiv preprint arXiv:2308.00352 , 2023.\\nEric Horvitz. Principles of mixed-initiative user interfaces. In Proceedings of the SIGCHI conference', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='Eric Horvitz. Principles of mixed-initiative user interfaces. In Proceedings of the SIGCHI conference\\non Human Factors in Computing Systems , 1999.\\nHuggingFace. Transformers agent. https://huggingface.co/docs/transformers/\\ntransformers_agents , 2023.\\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.\\narXiv preprint arXiv:2303.17491 , 2023.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\\nbenchmark for question answering research. Transactions of the Association for Computational\\nLinguistics , 2019.\\n11', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='LangChain. Introduction ‚Äî langchain. https://python.langchain.com/en/latest/index.\\nhtml , 2023.\\nMike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal? end-\\nto-end learning for negotiation dialogues. arXiv preprint arXiv:1706.05125 , 2017.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich K ¬®uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¬®aschel, et al. Retrieval-augmented gen-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Heinrich K ¬®uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¬®aschel, et al. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems ,\\n2020.\\nBeibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, and Ishai Menache. Large language\\nmodels for supply chain optimization. arXiv preprint arXiv:2307.03875 , 2023a.\\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\\nCamel: Communicative agents for ‚Äùmind‚Äù exploration of large scale language model society,\\n2023b.\\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng\\nTu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-\\nagent debate, 2023.\\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='agent debate, 2023.\\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement\\nlearning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802 ,\\n2018.\\nJerry Liu. LlamaIndex, November 2022. URL https://github.com/jerryjliu/llama_index .\\nV olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-\\nstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\\narXiv:1312.5602 , 2013.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\\narXiv:1312.5602 , 2013.\\nRoberto Navigli, Simone Conia, and Bj ¬®orn Ross. Biases in large language models: Origins, inven-\\ntory and discussion. ACM Journal of Data and Information Quality , 2023.\\nOpenAI. ChatGPT plugins. https://openai.com/blog/chatgpt-plugins , 2023.\\nJoon Sung Park, Joseph C O‚ÄôBrien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Joon Sung Park, Joseph C O‚ÄôBrien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and\\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint\\narXiv:2304.03442 , 2023.\\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.\\nRetrieval augmented code generation and summarization. arXiv preprint arXiv:2108.11601 ,\\n2021.\\nShishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='2021.\\nShishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model\\nconnected with massive apis. arXiv preprint arXiv:2305.15334 , 2023.\\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-\\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\\nProcessing . Association for Computational Linguistics, 11 2019. URL https://arxiv.org/\\nabs/1908.10084 .', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Processing . Association for Computational Linguistics, 11 2019. URL https://arxiv.org/\\nabs/1908.10084 .\\nSemantic-Kernel. Semantic kernel. https://github.com/microsoft/semantic-kernel ,\\n2023.\\nBokui Shen, Fei Xia, Chengshu Li, Roberto Mart ¬¥ƒ±n-Mart ¬¥ƒ±n, Linxi Fan, Guanzhi Wang, Claudia\\nP¬¥erez-D‚ÄôArpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, et al. igibson 1.0: A simu-\\nlation environment for interactive tasks in large realistic scenes. In 2021 IEEE/RSJ International', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='lation environment for interactive tasks in large realistic scenes. In 2021 IEEE/RSJ International\\nConference on Intelligent Robots and Systems (IROS) . IEEE, 2021.\\nTianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An\\nopen-domain platform for web-based agents. In International Conference on Machine Learning .\\nPMLR, 2017.\\n12', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C ÀÜot¬¥e, Yonatan Bisk, Adam Trischler, and Matthew\\nHausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In\\nProceedings of the International Conference on Learning Representations (ICLR) , 2021. URL\\nhttps://arxiv.org/abs/2010.03768 .\\nOriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,\\nMichelle Yeo, Alireza Makhzani, Heinrich K ¬®uttler, John Agapiou, Julian Schrittwieser, et al.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='Michelle Yeo, Alireza Makhzani, Heinrich K ¬®uttler, John Agapiou, Julian Schrittwieser, et al.\\nStarcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782 , 2017.\\nChi Wang, Qingyun Wu, Markus Weimer, and Erkang Zhu. Flaml: A fast and lightweight automl\\nlibrary. Proceedings of Machine Learning and Systems , 2021.\\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\\nand Anima Anandkumar. V oyager: An open-ended embodied agent with large language models.\\narXiv preprint arXiv:2305.16291 , 2023a.\\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\\narXiv preprint arXiv:2308.11432 , 2023b.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='arXiv preprint arXiv:2308.11432 , 2023b.\\nDaniel S. Weld and Oren Etzioni. The first law of robotics (a call to arms). In AAAI Conference on\\nArtificial Intelligence , 1994.\\nMax Woolf. Langchain problem. https://minimaxir.com/2023/07/langchain-problem/ ,\\n2023.\\nYiran Wu, Feiran Jia, Shaokun Zhang, Qingyun Wu, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat\\nLee, Richard Peng, and Chi Wang. An empirical study on challenging math problem solving with\\ngpt-4. arXiv preprint arXiv:2306.01337 , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='gpt-4. arXiv preprint arXiv:2306.01337 , 2023.\\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe\\nWang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents:\\nA survey. arXiv preprint arXiv:2309.07864 , 2023.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 ,\\n2022.\\n13', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='A Related Work\\nWe examine existing LLM-based agent systems or frameworks that can be used to build LLM appli-\\ncations. We categorize the related work into single-agent and multi-agent systems and specifically\\nprovide a summary of differentiators comparing AutoGen with existing multi-agent systems in Ta-\\nble 1. Note that many of these systems are evolving open-source projects, so the remarks and\\nstatements about them may only be accurate as of the time of writing. We refer interested readers to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='statements about them may only be accurate as of the time of writing. We refer interested readers to\\ndetailed LLM-based agent surveys (Xi et al., 2023; Wang et al., 2023b)\\nSingle-Agent Systems:\\n‚Ä¢AutoGPT : AutoGPT is an open-source implementation of an AI agent that attempts to au-\\ntonomously achieve a given goal (AutoGPT, 2023). It follows a single-agent paradigm in which\\nit augments the AI model with many useful tools, and does not support multi-agent collaboration.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='it augments the AI model with many useful tools, and does not support multi-agent collaboration.\\n‚Ä¢ChatGPT+ (with code interpreter or plugin) : ChatGPT, a conversational AI service or agent,\\ncan now be used alongside a code interpreter or plugin (currently available only under the pre-\\nmium subscription plan ChatGPT Plus) (OpenAI, 2023). The code interpreter enables ChatGPT\\nto execute code, while the plugin enhances ChatGPT with a wide range of curated tools.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='to execute code, while the plugin enhances ChatGPT with a wide range of curated tools.\\n‚Ä¢LangChain Agents : LangChain is a general framework for developing LLM-based applica-\\ntions (LangChain, 2023). LangChain Agents is a subpackage for using an LLM to choose a\\nsequence of actions. There are various types of agents in LangChain Agents, with the ReAct agent\\nbeing a notable example that combines reasoning and acting when using LLMs (mainly designed', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='being a notable example that combines reasoning and acting when using LLMs (mainly designed\\nfor LLMs prior to ChatGPT) (Yao et al., 2022). All agents provided in LangChain Agents fol-\\nlow a single-agent paradigm and are not inherently designed for communicative and collaborative\\nmodes. A significant summary of its limitations can be found in (Woolf, 2023). Due to these lim-\\nitations, even the multi-agent systems in LangChain (e.g., re-implementation of CAMEL) are not', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='itations, even the multi-agent systems in LangChain (e.g., re-implementation of CAMEL) are not\\nbased on LangChain Agents but are implemented from scratch. Their connection to LangChain\\nlies in the use of basic orchestration modules provided by LangChain, such as AI models wrapped\\nby LangChain and the corresponding interface.\\n‚Ä¢Transformers Agent : Transformers Agent (HuggingFace, 2023) is an experimental natural-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='‚Ä¢Transformers Agent : Transformers Agent (HuggingFace, 2023) is an experimental natural-\\nlanguage API built on the transformers repository. It includes a set of curated tools and an agent\\nto interpret natural language and use these tools. Similar to AutoGPT, it follows a single-agent\\nparadigm and does not support agent collaboration.\\nAutoGen differs from the single-agent systems above by supporting multi-agent LLM applications.\\nMulti-Agent Systems:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='AutoGen differs from the single-agent systems above by supporting multi-agent LLM applications.\\nMulti-Agent Systems:\\n‚Ä¢BabyAGI : BabyAGI (BabyAGI, 2023) is an example implementation of an AI-powered task man-\\nagement system in a Python script. In this implemented system, multiple LLM-based agents\\nare used. For example, there is an agent for creating new tasks based on the objective and the\\nresult of the previous task, an agent for prioritizing the task list, and an agent for completing', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='result of the previous task, an agent for prioritizing the task list, and an agent for completing\\ntasks/sub-tasks. As a multi-agent system, BabyAGI adopts a static agent conversation pattern,\\ni.e., a predefined order of agent communication, while AutoGen supports both static and dynamic\\nconversation patterns and additionally supports tool usage and human involvement.\\n‚Ä¢CAMEL : CAMEL (Li et al., 2023b) is a communicative agent framework. It demonstrates', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='‚Ä¢CAMEL : CAMEL (Li et al., 2023b) is a communicative agent framework. It demonstrates\\nhow role playing can be used to let chat agents communicate with each other for task comple-\\ntion. It also records agent conversations for behavior analysis and capability understanding. An\\nInception-prompting technique is used to achieve autonomous cooperation between agents. Un-\\nlikeAutoGen , CAMEL does not natively support tool usage, such as code execution. Although it', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='likeAutoGen , CAMEL does not natively support tool usage, such as code execution. Although it\\nis proposed as an infrastructure for multi-agent conversation, it only supports static conversation\\npatterns, while AutoGen additionally supports dynamic conversation patterns.\\n‚Ä¢Multi-Agent Debate: Two recent works investigate and show that multi-agent debate is an effec-\\ntive way to encourage divergent thinking in LLMs (Liang et al., 2023) and to improve the factuality', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='tive way to encourage divergent thinking in LLMs (Liang et al., 2023) and to improve the factuality\\nand reasoning of LLMs (Du et al., 2023). In both works, multiple LLM inference instances are\\nconstructed as multiple agents to solve problems with agent debate. Each agent is simply an LLM\\ninference instance, while no tool or human is involved, and the inter-agent conversation needs\\nto follow a pre-defined order. These works attempt to build LLM applications with multi-agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='to follow a pre-defined order. These works attempt to build LLM applications with multi-agent\\nconversation, while AutoGen , designed as a generic infrastructure, can be used to facilitate this\\ndevelopment and enable more applications with dynamic conversation patterns.\\n14', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='‚Ä¢MetaGPT : MetaGPT (Hong et al., 2023) is a specialized LLM application based on a multi-agent\\nconversation framework for automatic software development. They assign different roles to GPTs\\nto collaboratively develop software. They differ from AutoGen by being specialized solutions to\\na certain scenario, while AutoGen is a generic infrastructure to facilitate building applications for\\nvarious scenarios.\\nThere are a few other specialized single-agent or multi-agent systems, such as V oyager (Wang et al.,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='various scenarios.\\nThere are a few other specialized single-agent or multi-agent systems, such as V oyager (Wang et al.,\\n2023a) and Generative Agents (Park et al., 2023), which we skip due to lower relevance. In Table 1,\\nwe summarize differences between AutoGen and the most relevant multi-agent systems.\\nTable 1: Summary of differences between AutoGen and other related multi-agent systems. infras-\\ntructure : whether the system is designed as a generic infrastructure for building LLM applications.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='tructure : whether the system is designed as a generic infrastructure for building LLM applications.\\nconversation pattern : the types of patterns supported by the implemented systems. Under the\\n‚Äòstatic‚Äô pattern, agent topology remains unchanged regardless of different inputs. AutoGen allows\\nflexible conversation patterns, including both static and dynamic patterns that can be customized\\nbased on different application needs. execution-capable : whether the system can execute LLM-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='based on different application needs. execution-capable : whether the system can execute LLM-\\ngenerated code; human involvement : whether (and how) the system allows human participation\\nduring the execution process of the system. AutoGen allows flexible human involvement in multi-\\nagent conversation with the option for humans to skip providing inputs.\\nAspect AutoGen Multi-agent Debate CAMEL BabyAGI MetaGPT\\nInfrastructure ‚úì ‚úó ‚úì ‚úó ‚úó\\nConversation pattern flexible static static static static', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='Infrastructure ‚úì ‚úó ‚úì ‚úó ‚úó\\nConversation pattern flexible static static static static\\nExecution-capable ‚úì ‚úó ‚úó ‚úó ‚úì\\nHuman involvement chat/skip ‚úó ‚úó ‚úó ‚úó\\n15', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='B Expanded Discussion\\nThe applications in Section 3 show how AutoGen not only enables new applications but also helps\\nrenovate existing ones. For example, in A1 (scenario 3), A5, and A6, AutoGen enabled the cre-\\nation of multi-agent conversations that follow a dynamic pattern instead of a fixed back-and-forth.\\nAnd in both A5 and A6, humans can participate in the activities together with multiple other AI\\nagents in a conversational manner. Similarly, A1-A4 show how popular applications can be reno-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='agents in a conversational manner. Similarly, A1-A4 show how popular applications can be reno-\\nvated quickly with AutoGen . Despite the complexity of these applications (most of them involve\\nmore than two agents or dynamic multi-turn agent cooperation), our AutoGen -based implementa-\\ntion remains simple, demonstrating promising opportunities to build creative applications and a large\\nspace for innovation. In reflecting on why these benefits can be achieved in these applications with', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='space for innovation. In reflecting on why these benefits can be achieved in these applications with\\nAutoGen , we believe there are a few reasons:\\n‚Ä¢Ease of use : The built-in agents can be used out-of-the-box, delivering strong performance even\\nwithout any customization. (A1, A3)\\n‚Ä¢Modularity : The division of tasks into separate agents promotes modularity in the system. Each\\nagent can be developed, tested, and maintained independently, simplifying the overall develop-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='agent can be developed, tested, and maintained independently, simplifying the overall develop-\\nment process and facilitating code management. (A3, A4, A5, and A6)\\n‚Ä¢Programmability: AutoGen allows users to extend/customize existing agents to develop systems\\nsatisfying their specific needs with ease. (A1-A6). For example, with AutoGen , the core workflow\\ncode in A4 is reduced from over 430 lines to 100 lines, for a 4x saving.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='code in A4 is reduced from over 430 lines to 100 lines, for a 4x saving.\\n‚Ä¢Allowing human involvement :AutoGen provides a native mechanism to achieve human partici-\\npation and/or human oversight. With AutoGen , humans can seamlessly and optionally cooperate\\nwith AIs to solve problems or generally participate in the activity. AutoGen also facilitates inter-\\nactive user instructions to ensure the process stays on the desired path. (A1, A2, A5, and A6)', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='active user instructions to ensure the process stays on the desired path. (A1, A2, A5, and A6)\\n‚Ä¢Collaborative/adversarial agent interactions : Like many collaborative agent systems (Dong\\net al., 2023), agents in AutoGen can share information and knowledge, to complement each other‚Äôs\\nabilities and collectively arrive at better solutions. (A1, A2, A3, and A4). Analogously, in certain\\nscenarios, some agents are required to work in an adversarial way. Relevant information is shared', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='scenarios, some agents are required to work in an adversarial way. Relevant information is shared\\namong different conversations in a controlled manner, preventing distraction or hallucination. (A4,\\nA6). AutoGen supports both patterns, enabling effective utilization and augmentation of LLMs.\\nB.1 General Guidelines for Using AutoGen\\nBelow we give some recommendations for using agents in AutoGen to accomplish a task.\\n1.Consider using built-in agents first. For example, AssistantAgent is pre-configured to be', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='1.Consider using built-in agents first. For example, AssistantAgent is pre-configured to be\\nbacked by GPT-4, with a carefully designed system message for generic problem-solving via\\ncode. The UserProxyAgent is configured to solicit human inputs and perform tool execution.\\nMany problems can be solved by simply combining these two agents. When customizing agents\\nfor an application, consider the following options: (1) human input mode, termination condition,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='for an application, consider the following options: (1) human input mode, termination condition,\\ncode execution configuration, and LLM configuration can be specified when constructing an\\nagent; (2) AutoGen supports adding instructions in an initial user message, which is an effective\\nway to boost performance without needing to modify the system message; (3) UserProxyAgent\\ncan be extended to handle different execution environments and exceptions, etc.; (4) when sys-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='can be extended to handle different execution environments and exceptions, etc.; (4) when sys-\\ntem message modification is needed, consider leveraging the LLM‚Äôs capability to program its\\nconversation flow with natural language.\\n2.Start with a simple conversation topology . Consider using the two-agent chat or the group chat\\nsetup first, as they can often be extended with the least code. Note that the two-agent chat can', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='setup first, as they can often be extended with the least code. Note that the two-agent chat can\\nbe easily extended to involve more than two agents by using LLM-consumable functions in a\\ndynamic way.\\n3. Try to reuse built-in reply methods based on LLM, tool, or human before implementing a\\ncustom reply method because they can often be reused to achieve the goal in a simple way\\n(e.g., the built-in agent GroupChatManager ‚Äôs reply method reuses the built-in LLM-based reply', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='(e.g., the built-in agent GroupChatManager ‚Äôs reply method reuses the built-in LLM-based reply\\nfunction when selecting the next speaker, ref. A5 in Section 3).\\n4. When developing a new application with UserProxyAgent ,start with humans always in\\nthe loop , i.e., human input mode=‚ÄòALWAYS‚Äô, even if the target operation mode is more au-\\ntonomous. This helps evaluate the effectiveness of AssistantAgent , tuning the prompt, dis-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='tonomous. This helps evaluate the effectiveness of AssistantAgent , tuning the prompt, dis-\\ncovering corner cases, and debugging. Once confident with small-scale success, consider setting\\n16', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='human input mode = ‚ÄòNEVER‚Äô. This enables LLM as a backend, and one can either use the\\nLLM or manually generate diverse system messages to simulate different use cases.\\n5. Despite the numerous advantages of AutoGen agents, there could be cases/scenarios where other\\nlibraries/packages could help . For example: (1) For (sub)tasks that do not have requirements\\nfor back-and-forth trouble-shooting, multi-agent interaction, etc., a unidirectional (no back-and-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='for back-and-forth trouble-shooting, multi-agent interaction, etc., a unidirectional (no back-and-\\nforth message exchange) pipeline can also be orchestrated with LangChain (LangChain, 2023),\\nLlamaIndex (Liu, 2022), Guidance (Guidance, 2023), Semantic Kernel (Semantic-Kernel, 2023),\\nGorilla (Patil et al., 2023) or low-level inference API (‚Äòautogen.oai‚Äô provides an enhanced LLM\\ninference layer at this level) (Dibia, 2023). (2) When existing tools from LangChain etc. are', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='inference layer at this level) (Dibia, 2023). (2) When existing tools from LangChain etc. are\\nhelpful, one can use them as tool backends for AutoGen agents. For example, one can readily use\\ntools, e.g., Wolfram Alpha, from LangChain in AutoGen agent. (3) For specific applications, one\\nmay want to leverage agents implemented in other libraries/packages. To achieve this, one could\\nwrap those agents as conversable agents in AutoGen and then use them to build LLM applications', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='wrap those agents as conversable agents in AutoGen and then use them to build LLM applications\\nthrough multi-agent conversation. (4) It can be hard to find an optimal operating point among\\nmany tunable choices, such as the LLM inference configuration. Blackbox optimization packages\\nlike ‚Äòflaml.tune‚Äô (Wang et al., 2021) can be used together with AutoGen to automate such tuning.\\nB.2 Future Work\\nThis work raises many research questions and future directions and .', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='B.2 Future Work\\nThis work raises many research questions and future directions and .\\nDesigning optimal multi-agent workflows: Creating a multi-agent workflow for a given task can\\ninvolve many decisions, e.g., how many agents to include, how to assign agent roles and agent\\ncapabilities, how the agents should interact with each other, and whether to automate a particular\\npart of the workflow. There may not exist a one-fits-all answer, and the best solution might depend', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='part of the workflow. There may not exist a one-fits-all answer, and the best solution might depend\\non the specific application. This raises important questions: For what types of tasks and applications\\nare multi-agent workflows most useful? How do multiple agents help in different applications? For\\na given task, what is the optimal (e.g., cost-effective) multi-agent workflow?\\nCreating highly capable agents: AutoGen can enable the development of highly capable agents', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='Creating highly capable agents: AutoGen can enable the development of highly capable agents\\nthat leverage the strengths of LLMs, tools, and humans. Creating such agents is crucial to ensuring\\nthat a multi-agent workflow can effectively troubleshoot and make progress on a task. For example,\\nwe observed that CAMEL, another multi-agent LLM system, cannot effectively solve problems in\\nmost cases primarily because it lacks the capability to execute tools or code. This failure shows that', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='most cases primarily because it lacks the capability to execute tools or code. This failure shows that\\nLLMs and multi-agent conversations with simple role playing are insufficient, and highly capable\\nagents with diverse skill sets are essential. We believe that more systematic work will be required to\\ndevelop guidelines for application-specific agents, to create a large OSS knowledge base of agents,\\nand to create agents that can discover and upgrade their skills (Cai et al., 2023).', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='and to create agents that can discover and upgrade their skills (Cai et al., 2023).\\nEnabling scale, safety, and human agency: Section 3 shows how complex multi-agent workflows\\ncan enable new applications, and future work will be needed to assess whether scaling further can\\nhelp solve extremely complex tasks. However, as these workflows scale and grow more complex,\\nit may become difficult to log and adjust them. Thus, it will become essential to develop clear', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='it may become difficult to log and adjust them. Thus, it will become essential to develop clear\\nmechanisms and tools to track and debug their behavior. Otherwise, these techniques risk resulting\\nin incomprehensible, unintelligible chatter among agents (Lewis et al., 2017).\\nOur work also shows how complex, fully autonomous workflows with AutoGen can be useful, but\\nfully autonomous agent conversations will need to be used with care. While the autonomous mode', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='fully autonomous agent conversations will need to be used with care. While the autonomous mode\\nAutoGen supports could be desirable in many scenarios, a high level of autonomy can also pose\\npotential risks, especially in high-risk applications (Amodei et al., 2016; Weld & Etzioni, 1994). As\\na result, building fail-safes against cascading failures and exploitation, mitigating reward hacking,\\nout of control and undesired behaviors, maintaining effective human oversight of applications built', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='out of control and undesired behaviors, maintaining effective human oversight of applications built\\nwith AutoGen agents will become important. While AutoGen provides convenient and seamless\\ninvolvement of humans through a user proxy agent, developers and stakeholders still need to under-\\nstand and determine the appropriate level and pattern of human involvement to ensure the safe and\\nethical use of the technology (Horvitz, 1999; Amershi et al., 2019).\\n17', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='C Default System Message for Assistant Agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='SystemMessageYou are a helpful AI assistant. Solve tasks using your coding and language skills.In the following cases, suggest python code (in a python coding block) or shell script (in a shcoding block) for the user to execute.1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time. After sufficient info is printed and the task is ready to be solved based on', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='of a webpage or a file, get the current date/time. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.2. When you need to perform some task with code, use the code to perform the task and output theresult. Finish the task smartly.Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.When using code, you must', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='explain your plan first. Be clear which step uses code, and which step uses your language skill.When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can‚Äôt modify your code. So do not suggest incomplete code which requires users to modify. Don‚Äôt use a code block if it‚Äôs not intended to be executed by the user.If you want the user to save the code in a file before executing', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='a code block if it‚Äôs not intended to be executed by the user.If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don‚Äôt include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use ‚Äôprint‚Äô function for the output when relevant. Check the execution result returned by the user.If the result indicates there is an error, fix the error and output the code again. Suggest the full code', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='returned by the user.If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can‚Äôt be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.Reply', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='to try.When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.Reply ‚ÄúTERMINATE‚Äù in the end when everything is done.Prompting techniques color code: Role Play; Control Flow; Output Confine; Facilitate Automation; Grounding', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='Figure 5: Default system message for the built-in assistant agent in AutoGen (v0.1.1). This is an\\nexample of conversation programming via natural language. It contains instructions of different\\ntypes, including role play, control flow, output confine, facilitate automation, and grounding.\\nFigure 5 shows the default system message for the built-in assistant agent in AutoGen (v0.1.1),\\nwhere we introduce several new prompting techniques and highlight them accordingly. When com-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='where we introduce several new prompting techniques and highlight them accordingly. When com-\\nbining these new prompting techniques together, we can program a fairly complex conversation even\\nwith the simplest two-agent conversation topology. This approach tries to exploit the capability of\\nLLMs in implicit state inference to a large degree. LLMs do not follow all the instructions perfectly,\\nso the design of the system needs to have other mechanisms to handle the exceptions and faults.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='so the design of the system needs to have other mechanisms to handle the exceptions and faults.\\nSome instructions can have ambiguities, and the designer should either reduce them for preciseness\\nor intentionally keep them for flexibility and address the different situations in other agents. In\\ngeneral, we observe that GPT-4 follows the instructions better than GPT-3.5-turbo.\\n18', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='D Application Details\\nA1: Math Problem Solving\\nScenario 1: Autonomous Problem Solving. We perform both qualitative and quantitative eval-\\nuations in this scenario. For all evaluations, we use GPT-4 as the base model, and pre-install the\\n‚Äúsympy‚Äù package in the execution environment. We compare AutoGen with the following LLM-\\nbased agent systems:\\n‚Ä¢ AutoGPT: The out-of-box AutoGPT is used. We initialize AutoGPT by setting the purpose to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='based agent systems:\\n‚Ä¢ AutoGPT: The out-of-box AutoGPT is used. We initialize AutoGPT by setting the purpose to\\n‚Äúsolve math problems‚Äù, resulting in a ‚ÄúMathSolverGPT‚Äù with auto-generated goals.\\n‚Ä¢ ChatGPT+Plugin: We enable the Wolfram Alpha plugin (a math computation engine) in the Ope-\\nnAI web client.\\n‚Ä¢ ChatGPT+Code Interpreter: This is a recent feature in OpenAI web client. Note that the above\\ntwo premium features from ChatGPT require a paid subscription to be accessed and are the most', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='two premium features from ChatGPT require a paid subscription to be accessed and are the most\\ncompetitive commercial systems.\\n‚Ä¢ LangChain ReAct+Python: We use Python agent from LangChain. To handle parsing errors, we\\nset ‚Äúhandle parsing errors=True‚Äù, and use the default zero-shot ReAct prompt.\\n‚Ä¢ Multi-Agent Debate (Liang et al., 2023): We modified the code of the multi-agent debate to per-\\nform evaluation. By default, there are three agents: an affirmative agent, a negative agent, and a\\nmoderator.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='form evaluation. By default, there are three agents: an affirmative agent, a negative agent, and a\\nmoderator.\\nWe also conducted preliminary evaluations on several other multi-agent systems, including\\nBabyAGI, CAMEL, and MetaGPT. The results indicate that they are not suitable choices for solving\\nmath problems out of the box. For instance, when MetaGPT is tasked with solving a math problem,\\nit begins developing software to address the problem, but most of the time, it does not actually solve', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='it begins developing software to address the problem, but most of the time, it does not actually solve\\nthe problem. We have included the test examples in Appendix E.\\nTable 2: Qualitative evaluation of two math problems from the MATH dataset within the autonomous\\nproblem-solving scenario. Each LLM-based system is tested three times on each of the problems.\\nThis table reports the problem-solving correctness and summarizes the reasons for failure.\\nCorrectness Failure Reason\\nAutoGen 3/3 N/A.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='Correctness Failure Reason\\nAutoGen 3/3 N/A.\\nAutoGPT 0/3 The LLM gives code without the print function so the\\nresult is not printed.\\nChatGPT+Plugin 1/3 The return from Wolfram Alpha contains 2 simplified\\nresults, including the correct answer, but GPT-4 always\\nchooses the wrong answer.\\nChatGPT+Code Interpreter 2/3 Returns a wrong decimal result.\\nLangChain ReAct 0/3 LangChain gives 3 different wrong answers.\\nMulti-Agent Debate 0/3 It gives 3 different wrong answers due to calculation errors.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='Multi-Agent Debate 0/3 It gives 3 different wrong answers due to calculation errors.\\n(a) Evaluation on the first problem that asks to simplify a square root fraction.\\nCorrectness Failure Reason\\nAutoGen 2/3 The final answer from code execution is wrong.\\nAutoGPT 0/3 The LLM gives code without the print function so the\\nresult is not printed.\\nChatGPT+Plugin 1/3 For one trial, GPT-4 got stuck because it keeps giving\\nwrong queries and has to be stopped. Another trial simply\\ngives a wrong answer.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='wrong queries and has to be stopped. Another trial simply\\ngives a wrong answer.\\nChatGPT+Code Interpreter 0/3 It gives 3 different wrong answers.\\nLangChain ReAct 0/3 LangChain gives 3 different wrong answers.\\nMulti-Agent Debate 0/3 It gives 3 different wrong answers.\\n(b) Evaluation on the second number theory problem.\\nFor the qualitative evaluation, we utilize two level-5 problems from the MATH dataset, testing each', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='For the qualitative evaluation, we utilize two level-5 problems from the MATH dataset, testing each\\nproblem three times. The first problem involves simplifying a square root fraction, and the second\\n19', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='problem involves solving a number theory issue. The correctness counts and reasons for failure\\nare detailed in Table 2. For the quantitative evaluation, we conduct two sets of experiments on\\nthe MATH dataset to assess the correctness of these systems: (1) an experiment involving 120\\nlevel-5 (the most challenging level) problems, including 20 problems from six categories, excluding\\ngeometry, and (2) an experiment on the entire test set, which includes 5000 problems. We exclude', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='geometry, and (2) an experiment on the entire test set, which includes 5000 problems. We exclude\\nAutoGPT from this evaluation as it cannot access results from code executions and does not solve\\nany problems in the qualitative evaluation. Our analysis of the entire dataset reveals that AutoGen\\nachieves an overall accuracy of 69.48%, while GPT-4‚Äôs accuracy stands at 55.18%. From these\\nevaluations, we have the following observations regarding the problem-solving success rate and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='evaluations, we have the following observations regarding the problem-solving success rate and\\nuser experience of these systems:\\n‚Ä¢ Problem-solving success rate: Results from the quantitative evaluations show that AutoGen can\\nhelp achieve the highest problem-solving success rate among all the compared methods. The qual-\\nitative evaluations elucidate common failure reasons across several alternative approaches. Chat-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='itative evaluations elucidate common failure reasons across several alternative approaches. Chat-\\nGPT+Code Interpreter fails to solve the second problem, and ChatGPT+Plugin struggles to solve\\nboth problems. AutoGPT fails on both problems due to code execution issues. The LangChain\\nagent also fails on both problems, producing code that results in incorrect answers in all trials.\\n‚Ä¢ Based on the qualitative evaluation, we analyze the user experience concerning the verbosity of', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='‚Ä¢ Based on the qualitative evaluation, we analyze the user experience concerning the verbosity of\\nthe response and the ability of the LLM-based system to run without unexpected behaviors. Chat-\\nGPT+Plugin is the least verbose, mainly because Wolfram queries are much shorter than Python\\ncode. AutoGen , ChatGPT+Code Interpreter, and LangChain exhibit similar verbosity, although\\nLangChain is slightly more verbose due to more code execution errors. AutoGPT is the most', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='LangChain is slightly more verbose due to more code execution errors. AutoGPT is the most\\nverbose system owing to predefined steps like THOUGHTS, REASONING, and PLAN, which it\\nincludes in replies every time. Overall, AutoGen and ChatGPT+Code Interpreter operate smoothly\\nwithout exceptions. We note the occurrences of undesired behaviors from other LLM-based sys-\\ntems that could affect user experience: AutoGPT consistently outputs code without the print‚Äô', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='tems that could affect user experience: AutoGPT consistently outputs code without the print‚Äô\\nstatement and cannot correct this, requiring the user to run them manually; ChatGPT with Wol-\\nfram Alpha plugin has the potential to become stuck in a loop that must be manually stopped; and\\nLangchain ReAct could exit with a parse error, necessitating the passing of a ‚Äòhandle parse error‚Äô\\nparameter.\\nEnable Multi-User Problem Solving ViaStudent        and Expert Student Proxy\\nStudentAssistant\\nExpertAssistant', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='parameter.\\nEnable Multi-User Problem Solving ViaStudent        and Expert Student Proxy\\nStudentAssistant\\nExpertAssistant\\nExpert Proxy\\nAsk for expert\\nEnable Autonomous and Human-in-the-loop Problem Solving\\nFigure 6: Examples of three settings utilized to solve math problems using AutoGen : (Gray) En-\\nables a workflow where a student collaborates with an assistant agent to solve problems, either\\nautonomously or in a human-in-the-loop mode. (Gray + Orange) Facilitates a more sophisticated', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='autonomously or in a human-in-the-loop mode. (Gray + Orange) Facilitates a more sophisticated\\nworkflow wherein the assistant, on the fly, can engage another user termed ‚Äúexpert‚Äù, who is in the\\nloop with their own assistant agent, to aid in problem-solving if its own solutions are not satisfactory.\\nScenario 2: Human-in-the-loop Problem Solving. For challenging problems that these LLM\\nsystems cannot solve autonomously, human feedback during the problem-solving process can be\\n20', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='helpful. To incorporate human feedback with AutoGen , one can set human input mode=‚ÄòALWAYS‚Äô\\nin the user proxy agent. We select one challenging problem that none of these systems can solve\\nautonomously across three trials. We adhere to the process outlined below to provide human inputs\\nfor all the compared methods:\\n1. Input the problem: Find the equation of the plane which bisects the angle\\nbetween the planes 3x‚àí6y+ 2z+ 5 = 0 and4x‚àí12y+ 3z‚àí3 = 0 ,and which', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='between the planes 3x‚àí6y+ 2z+ 5 = 0 and4x‚àí12y+ 3z‚àí3 = 0 ,and which\\ncontains the point (‚àí5,‚àí1,‚àí5).Enter your answer in the form\\nAx+By+Cz+D= 0,\\nwhere A, B, C, D are integers such that A > 0and\\ngcd(|A|,|B|,|C|,|D|) = 1.\\n2. The response from the system does not solve the problem correctly. We then give a\\nhint to the model: Your idea is not correct. Let‚Äôs solve this together.\\nSuppose P= (x, y, z )is a point that lies on a plane that bisects the\\nangle, the distance from P to the two planes is the same. Please', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='angle, the distance from P to the two planes is the same. Please\\nset up this equation first.\\n3. We expect the system to give the correct distance equation. Since the equation involves\\nan absolute sign that is hard to solve, we would give the next hint: Consider the two\\ncases to remove the abs sign and get two possible solutions.\\n4. If the system returns the two possible solutions and doesn‚Äôt continue to the next step, we\\ngive the last hint: Use point (-5,-1,-5) to determine which is correct and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='give the last hint: Use point (-5,-1,-5) to determine which is correct and\\ngive the final answer.\\n5. Final answer is 11x+6y+5z+86=0 .\\nWe observed that AutoGen consistently solved the problem across all three trials. ChatGPT+Code\\nInterpreter and ChatGPT+Plugin managed to solve the problem in two out of three trials, while Au-\\ntoGPT failed to solve it in all three attempts. In its unsuccessful attempt, ChatGPT+Code Interpreter', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='toGPT failed to solve it in all three attempts. In its unsuccessful attempt, ChatGPT+Code Interpreter\\nfailed to adhere to human hints. In its failed trial, ChatGPT+Plugin produced an almost correct solu-\\ntion but had a sign discrepancy in the final answer. AutoGPT was unable to yield a correct solution\\nin any of the trials. In one trial, it derived an incorrect distance equation. In the other two trials, the\\nfinal answer was incorrect due to code execution errors.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='final answer was incorrect due to code execution errors.\\nScenario 3: Multi-User Problem Solving. Next-generation LLM applications may necessitate\\nthe involvement of multiple real users for collectively solving a problem with the assistance of\\nLLMs. We showcase how AutoGen can be leveraged to effortlessly construct such a system. Specif-\\nically, building upon scenario 2 mentioned above, we aim to devise a simple system involving two', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='ically, building upon scenario 2 mentioned above, we aim to devise a simple system involving two\\nhuman users: a student and an expert. In this setup, the student interacts with an LLM assistant to\\naddress some problems, and the LLM automatically resorts to the expert when necessary.\\nThe overall workflow is as follows: The student chats with the LLM-based assistant agent through\\na student proxy agent to solve problems. When the assistant cannot solve the problem satisfactorily,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='a student proxy agent to solve problems. When the assistant cannot solve the problem satisfactorily,\\nor the solution does not match the expectation of the student, it would automatically hold the con-\\nversation and call the pre-defined askforexpert function via the function callfeature of GPT\\nin order to resort to the expert. Specifically, it would automatically produce the initial message for\\ntheaskforexpert function, which could be the statement of the problem or the request to verify', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='theaskforexpert function, which could be the statement of the problem or the request to verify\\nthe solution to a problem, and the expert is supposed to respond to this message with the help of\\nthe expert assistant. After the conversation between the expert and the expert‚Äôs assistant, the final\\nmessage would be sent back to the student assistant as the response to the initial message. Then, the\\nstudent assistant would resume the conversation with the student using the response from the expert', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='student assistant would resume the conversation with the student using the response from the expert\\nfor a better solution. A detailed visualization is shown in Figure 6.\\nWith AutoGen , constructing the student/expert proxy agent and the assistant agents is straight-\\nforward by reusing the built-in UserProxyAgent andAssistantAgent through appropriate\\nconfigurations. The only development required involves writing several lines of code for the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='configurations. The only development required involves writing several lines of code for the\\naskforexpert function, which then becomes part of the configuration for the assistant. Ad-\\nditionally, it‚Äôs easy to extend such a system to include more than one expert, with a specific\\naskforexpert function for each, or to include multiple student users with a shared expert for\\nconsultation.\\n21', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='A2: Retrieval-Augmented Code Generation and Question Answering\\nRetrieval-augmentedAssistantRetrieval-augmented  User Proxy\\n1. Question and Contexts3. Terminate,feedbacks or `Update Context`4. Satisfied Answers or Terminate\\n2. Satisfied Answers or `Update Context`\\nFigure 7: Overview of Retrieval-augmented Chat which involves two agents, including a Retrieval-\\naugmented User Proxy and a Retrieval-augmented Assistant. Given a set of documents, the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='augmented User Proxy and a Retrieval-augmented Assistant. Given a set of documents, the\\nRetrieval-augmented User Proxy first automatically processes documents‚Äîsplits, chunks, and stores\\nthem in a vector database. Then for a given user input, it retrieves relevant chunks as context and\\nsends it to the Retrieval-augmented Assistant, which uses LLM to generate code or text to answer\\nquestions. Agents converse until they find a satisfactory answer.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='questions. Agents converse until they find a satisfactory answer.\\nDetailed Workflow. The workflow of Retrieval-Augmented Chat is illustrated in Figure 7. To\\nuse Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented\\nUser Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy\\nnecessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='necessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented\\nUser Proxy can download the documents, segment them into chunks of a specific size, compute\\nembeddings, and store them in a vector database. Once a chat is initiated, the agents collaboratively\\nengage in code generation or question-answering adhering to the procedures outlined below:\\n1. The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding simi-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='1. The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding simi-\\nlarity, and sends them along with the question to the Retrieval-Augmented Assistant.\\n2. The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based\\non the question and context provided. If the LLM is unable to produce a satisfactory response, it\\nis instructed to reply with ‚ÄúUpdate Context‚Äù to the Retrieval-Augmented User Proxy.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='is instructed to reply with ‚ÄúUpdate Context‚Äù to the Retrieval-Augmented User Proxy.\\n3. If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and\\nsends the output as feedback. If there are no code blocks or instructions to update the context, it\\nterminates the conversation. Otherwise, it updates the context and forwards the question along\\nwith the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='with the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation\\nis enabled, individuals can proactively send any feedback, including Update Context‚Äù, to the\\nRetrieval-Augmented Assistant.\\n4. If the Retrieval-Augmented Assistant receives ‚ÄúUpdate Context‚Äù, it requests the next most similar\\nchunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it\\ngenerates new code or text based on the feedback and chat history. If the LLM fails to generate', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='generates new code or text based on the feedback and chat history. If the LLM fails to generate\\nan answer, it replies with ‚ÄúUpdate Context‚Äù again. This process can be repeated several times.\\nThe conversation terminates if no more documents are available for the context.\\nWe utilize Retrieval-Augmented Chat in two scenarios. The first scenario aids in generating code\\nbased on a given codebase. While LLMs possess strong coding abilities, they are unable to utilize', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='based on a given codebase. While LLMs possess strong coding abilities, they are unable to utilize\\npackages or APIs that are not included in their training data, e.g., private codebases, or have trouble\\nusing trained ones that are frequently updated post-training. Hence, Retrieval-Augmented Code\\nGeneration is considered to be highly valuable. The second scenario involves question-answering\\non the Natural Questions dataset (Kwiatkowski et al., 2019), enabling us to obtain comparative', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='on the Natural Questions dataset (Kwiatkowski et al., 2019), enabling us to obtain comparative\\nevaluation metrics for the performance of our system.\\nScenario 1: Evaluation on Natural Questions QA dataset. In this case, we evaluate the\\nRetrieval-Augmented Chat‚Äôs end-to-end question-answering performance using the Natural Ques-\\ntions dataset (Kwiatkowski et al., 2019). We collected 5,332 non-redundant context documents and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='tions dataset (Kwiatkowski et al., 2019). We collected 5,332 non-redundant context documents and\\n6,775 queries from HuggingFace. First, we create a document collection based on the entire context\\ncorpus and store it in the vector database. Then, we utilize Retrieval-Augmented Chat to answer the\\nquestions. An example (Figure 8) from the NQ dataset showcases the advantages of the interactive\\nretrieval feature: ‚Äúwho carried the usa flag in opening ceremony‚Äù . When attempting to answer', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='retrieval feature: ‚Äúwho carried the usa flag in opening ceremony‚Äù . When attempting to answer\\nthis question, the context with the highest similarity to the question embedding does not contain the\\n22', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='Task description who carried the usaflag in opening ceremony?Assistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply exactly ‚ÄòUPDATE CONTEXT‚Äô. You must give as short an answer as possible.User‚Äôs question is: who carried the usaflag in opening ceremonyContext is: < P > On January 17 , 1899 , under ... The Astros beat the Yankees that', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='who carried the usaflag in opening ceremonyContext is: < P > On January 17 , 1899 , under ... The Astros beat the Yankees that night , 2 -1 . < /P >UserProxy: Sorry, I cannot find any information about who carried the USA flag in the opening ceremony. UPDATE CONTEXT.Assistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply exactly ‚ÄòUPDATE', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply exactly ‚ÄòUPDATE CONTEXT‚Äô. You must give as short an answer as possible.User‚Äôs question is: who carried the usaflag in opening ceremonyContext is: < T able > ... Erin Hamlin ... < /T d >UserProxy: Erin Hamlin carried the USA flag in the opening ceremony.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='(a) W/O Interactive Retrieval\\n(b) W/ Interactive Retrieval', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='Assistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply ‚Äòsorry, I don‚Äôt know‚Äô. You must give as short an answer as possible.User‚Äôs question is:who carried the usaflag in opening ceremonyContext is: < P > On January 17 , 1899 , under ... The Astros beat the Yankees that night , 2 -1 . < /P >UserProxy: Sorry, I cannot find any information', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='17 , 1899 , under ... The Astros beat the Yankees that night , 2 -1 . < /P >UserProxy: Sorry, I cannot find any information about who carried the USA flag in the opening ceremony. Sorry, I don‚Äôt know.Figure 8: Retrieval-augmented Chat without (W/O) and with (W/) interactive retrieval .', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='required information for a response. As a result, the LLM assistant (GPT-3.5-turbo) replies ‚ÄúSorry,\\nI cannot find any information about who carried the USA flag in the opening ceremony. UPDATE\\nCONTEXT. ‚Äù With the unique and innovative ability to update context in Retrieval-Augmented Chat,\\nthe user proxy agent automatically updates the context and forwards it to the assistant agent again.\\nFollowing this process, the agent is able to generate the correct answer to the question.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='Following this process, the agent is able to generate the correct answer to the question.\\nIn addition, we conduct an experiment using the same prompt as illustrated in (Adlakha et al., 2023)\\nto investigate the advantages of AutoGen W/O interactive retrieval . The F1 score and Recall for the\\nfirst 500 questions are 23.40% and 62.60%, respectively, aligning closely with the results reported\\nin Figure 4b. Consequently, we assert that AutoGen W/O interactive retrieval outperforms DPR due', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='in Figure 4b. Consequently, we assert that AutoGen W/O interactive retrieval outperforms DPR due\\nto differences in the retrievers employed. Specifically, we utilize a straightforward vector search\\nretriever with the all-MiniLM-L6-v2 model for embeddings.\\nFurthermore, we analyze the number of LLM calls in experiments involving both AutoGen and\\nAutoGen W/O interactive retrieval , revealing that approximately 19.4% of questions in the Natural', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='AutoGen W/O interactive retrieval , revealing that approximately 19.4% of questions in the Natural\\nQuestions dataset trigger an ‚ÄúUpdate Context‚Äù operation, resulting in additional LLM calls.\\nScenario 2: Code Generation Leveraging Latest APIs from the Codebase. In this case, the ques-\\ntion is ‚ÄúHow can I use FLAML to perform a classification task and use Spark for parallel training?\\nTrain for 30 seconds and force cancel jobs if the time limit is reached. ‚Äù . FLAML (v1) (Wang et al.,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='Train for 30 seconds and force cancel jobs if the time limit is reached. ‚Äù . FLAML (v1) (Wang et al.,\\n2021) is an open-source Python library designed for efficient AutoML and tuning. It was open-\\nsourced in December 2020, and is included in the training data of GPT-4. However, the question\\nnecessitates the use of Spark-related APIs, which were added in December 2022 and are not encom-\\npassed in the GPT-4 training data. Consequently, the original GPT-4 model is unable to generate the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='passed in the GPT-4 training data. Consequently, the original GPT-4 model is unable to generate the\\ncorrect code, due to its lack of knowledge regarding Spark-related APIs. Instead, it erroneously cre-\\nates a non-existent parameter, spark , and sets it to True‚Äô. Nevertheless, with Retrieval-Augmented\\nChat, we provide the latest reference documents as context. Then, GPT-4 generates the correct code\\nblocks by setting usespark andforce cancel to True‚Äô.\\n23', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='A3: Decision Making in Text World Environments\\nALFWorld\\nExecutor\\nReward & StateAction Decision\\nAssistant\\nObservation: On the desk 2, you see an alarmclock 3, \\na bowl 3, a creditcard 2, a mug 1, and a pencil 2.Action decision: Pick up pencil 2 from desk 2\\nGroundingAgent\\nALFChat (two agents) ALFChat (three agents)\\nALFWorld Executor\\nAssistant\\nFigure 9: We use AutoGen to solve tasks in the ALFWorld benchmark, which contains household', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='ALFWorld Executor\\nAssistant\\nFigure 9: We use AutoGen to solve tasks in the ALFWorld benchmark, which contains household\\ntasks described in natural language. We propose two designs: a two-agent design where the assistant\\nagent suggests the next step, and the Executor executes actions and provides feedback. The three-\\nagent design adds a grounding agent that supplies commonsense facts to the executor when needed.\\nALFWorld (Shridhar et al., 2021) is a synthetic language-based interactive decision-making task.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='ALFWorld (Shridhar et al., 2021) is a synthetic language-based interactive decision-making task.\\nIt comprises textual environments that aim to simulate real-world household scenes. Given a high-\\nlevel goal (e.g., putting a hot apple in the fridge) and the description of the household environment,\\nthe agent needs to explore and interact with the simulated household environment through a textual\\ninterface. A typical task environment contains various types of locations and could require more', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='interface. A typical task environment contains various types of locations and could require more\\nthan 40 steps to finish, which highlights the need for agents to decompose the goal into subtasks and\\ntackle them one by one, while effectively exploring the environments.\\nDetailed Workflow. We first propose a straightforward two-agent system with AutoGen , illustrated\\non the left-hand side of Figure 9, to tackle tasks from this benchmark. The system consists of', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='on the left-hand side of Figure 9, to tackle tasks from this benchmark. The system consists of\\nan assistant agent and an executor agent. The assistant agent generates plans and makes action\\ndecisions to solve the tasks. The executor agent is tailored specifically for ALFWorld. It performs\\nactions proposed by the assistant and reports action execution results in the household environment\\nas feedback to the assistant. Due to the strict format requirements for the output format, we use the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='as feedback to the assistant. Due to the strict format requirements for the output format, we use the\\nBLEU metric to evaluate the similarity of the output to all valid action options. The option with the\\nhighest similarity will be chosen as the action for this round.\\nOne major challenge encompassed in ALFWorld is commonsense reasoning. The agent needs to\\nextract patterns from the few-shot examples provided and combine them with the agent‚Äôs general', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='extract patterns from the few-shot examples provided and combine them with the agent‚Äôs general\\nknowledge of household environments to fully understand task rules. More often than not, the as-\\nsistant tends to neglect some basic knowledge of the household environment. Thanks to the easy-to-\\nimplement multi-agent conversational feature of AutoGen , enhancing the assistant agent‚Äôs reason-\\ning ability by adding a new grounding agent to provide commonsense facts for the decision-making', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='ing ability by adding a new grounding agent to provide commonsense facts for the decision-making\\nagent‚Äôs reference becomes straightforward. By scrutinizing the failed attempts and summarizing the\\nreasons for failure, we obtained a holistic understanding of the commonsense knowledge that the\\nassistant agent lacks. Then, we set a grounding agent to provide this general knowledge when the\\ntask begins and whenever the assistant outputs the same action three times in a row. This ensures the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='task begins and whenever the assistant outputs the same action three times in a row. This ensures the\\nassistant takes this commonsense knowledge into consideration and prevents it from getting stuck in\\noutputting the same content or constantly apologizing.\\nWe compare our system‚Äôs performance with ReAct, which treats ALFWorld as a text-completion\\ntask. ReAct (Yao et al., 2022) is a few-shot prompting technique that interleaves reasoning and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='task. ReAct (Yao et al., 2022) is a few-shot prompting technique that interleaves reasoning and\\nacting, allowing for greater synergy between the two and significantly improving performance on\\nboth language and decision-making tasks. We integrate ReAct into AutoGen by modifying the\\nprompts in a conversational manner. Following ReAct, we employ a two-shot setting. The few-\\nshot prompts are obtained from the corresponding repository. As shown in Table 3, the two-agent\\n24', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='Task description\\nYou are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a safe 1, a drawer 2, a dra wer1, a \\nshelf 3, a shelf 2, and a shelf 1.\\nYour task is to: look at bowl under the desklamp . \\nAssistant (to Executor) : THOUGHT [First, I need to find a bowl. A \\nbowl is more likely to appear in desk (1 -2), drawer (1 -2), shelf \\n(1-3), bed (1). Then I need to find and use a desklamp .] ACTION \\n[go to desk 1]', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content=\"(1-3), bed (1). Then I need to find and use a desklamp .] ACTION \\n[go to desk 1]\\n(Assistant finds the desklamp at desk 1, then goes to desk 2.)\\nExecutor (to Assistant) : On the desk 2, you see a bowl 1, and a \\ncd 3... \\nAssistant (to Executor) : THOUGHT [Now I find a bowl (1). I need \\nto use the desklamp to look at the bowl. I'll go to the desklamp\\nnow.] ACTION [go to desk 1]\\nExecutor (to Assistant): On the desk 1, you see a desklamp 1, a \\npencil 2...\\nAssistant: ACTION [use desklamp 1]\", metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='Executor (to Assistant): On the desk 1, you see a desklamp 1, a \\npencil 2...\\nAssistant: ACTION [use desklamp 1]\\nExecutor (to Assistant) : You turn on the desklamp 1.\\nAssistant (to Executor): THOUGHT [Now that the desklamp is turned \\non, I can use it to look at the bowl (1) that I found earlier.] \\nACTION [look at bowl 1 under the desklamp 1]\\nExecutor (to Assistant) : You turn on the desklamp 1.\\n(Assistant falls into an infinite loop turning on desklamp .)', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='Executor (to Assistant) : You turn on the desklamp 1.\\n(Assistant falls into an infinite loop turning on desklamp .)\\nExecutor (to Assistant) : Task failed. Reply TERMINATE.\\nAssistant (to Executor) : TERMINATEAssistant (to Executor) : THOUGHT [First, I need to find a bowl. A \\nbowl is more likely to appear in desk (1 -2), drawer (1 -2), shelf \\n(1-3), bed (1). Then I need to find and use a desklamp .] ACTION \\n[go to desk 1]\\n(Assistant finds the desklamp at desk 1, then goes to desk 2.)', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='[go to desk 1]\\n(Assistant finds the desklamp at desk 1, then goes to desk 2.)\\n(Assistant falls into the same loop as (a))\\nGroundingAgent (to Executor) : You must find and take the object \\nbefore you can examine it. You must go to where the target \\nobject is before you can use it.\\nExecutor(to Assistant): You turn on the desklamp 1. Hint: You must \\nfind and take the object before you can examine it. \\nAssistant (to Executor): THOUGHT [I need to find and take the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='find and take the object before you can examine it. \\nAssistant (to Executor): THOUGHT [I need to find and take the \\nbowl (1) before I can examine it. Let me find the bowl first.] \\nACTION [go to desk 2]\\nExecutor (to Assistant): On the desk 2, you see a bowl 1, a cd \\n3...\\nAssistant (to Executor) : ACTION [take bowl 1 from desk 2]\\nExecutor (to Assistant): You pick up bowl 1 from desk 2.\\nAssistant (to Executor): ACTION [go to desk 1]\\nAssistant (to Executor): ACTION[use desklamp 1]', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='Assistant (to Executor): ACTION [go to desk 1]\\nAssistant (to Executor): ACTION[use desklamp 1]\\nExecutor (to Assistant) : Task success. Reply TERMINATE.\\nAssistant (to Executor) : TERMINATE\\n(a) Two agents\\n (b) Three agentsFigure 10: Comparison of results from two designs: (a) Two-agent design which consists of an\\nassistant and an executor, (b) Three-agent design which adds a grounding agent that serves as a\\nknowledge source. For simplicity, we omit the in-context examples and part of the exploration', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='knowledge source. For simplicity, we omit the in-context examples and part of the exploration\\ntrajectory, and only show parts contributing to the failure/success of the attempt.\\nMethod Pick Clean Heat Cool Look Pick 2 All\\nReAct (avg) 63 52 48 71 61 24 54\\nALFChat (2 agents)(avg) 61 58 57 67 50 19 54\\nALFChat (3 agents)(avg) 79 64 70 76 78 41 69\\nReAct (best of 3) 75 62 61 81 78 35 66\\nALFChat (2 agents)(best of 3) 71 61 65 76 67 35 63\\nAFLChat (3 agents)(best of 3) 92 74 78 86 83 41 77', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='ALFChat (2 agents)(best of 3) 71 61 65 76 67 35 63\\nAFLChat (3 agents)(best of 3) 92 74 78 86 83 41 77\\nTable 3: Comparisons between ReAct and the two variants of ALFChat on the ALFWorld bench-\\nmark. For each task, we report the success rate out of 3 attempts. Success rate denotes the number\\nof tasks successfully completed by the agent divided by the total number of tasks. The results show\\nthat adding a grounding agent significantly improves the task success rate in ALFChat.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='that adding a grounding agent significantly improves the task success rate in ALFChat.\\ndesign matches the performance of ReAct, while the three-agent design significantly outperforms\\nReAct. We surmise that the performance discrepancy is caused by the inherent difference between\\ndialogue-completion and text-completion tasks. On the other hand, introducing a grounding agent\\nas a knowledge source remarkably advances performance on all types of tasks.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='as a knowledge source remarkably advances performance on all types of tasks.\\nCase study . Figure 10 exemplifies how a three-agent design eliminates one root cause for failure\\ncases. Most of the tasks involve taking an object and then performing a specific action with it (e.g.,\\nfinding a vase and placing it on a cupboard). Without a grounding agent, the assistant frequently\\nconflates finding an object with taking it, as illustrated in Figure 10a). This leads to most of the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='conflates finding an object with taking it, as illustrated in Figure 10a). This leads to most of the\\nfailure cases in ‚Äôpick‚Äô and ‚Äôlook‚Äô type tasks. With the introduction of a grounding agent, the assistant\\ncan break out of this loop and successfully complete the task\\nTakeaways. We introduced a grounding agent to serve as an external commonsense knowledge\\nsource, which significantly enhanced the assistant‚Äôs ability to make informed decisions. This proves', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='source, which significantly enhanced the assistant‚Äôs ability to make informed decisions. This proves\\nthat providing necessary commonsense facts to the decision-making agent can assist it in making\\nmore informed decisions, thus effectively boosting the task success rate. AutoGen brings both\\nsimplicity and modularity when adding the grounding agent.\\n25', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='A4: Multi-Agent Coding\\nCommander\\nSafeguard\\nWriter\\n‚ë°Question, ‚ùªLog‚ù∏Code, ‚ë¶Ans‚ùπCode‚ù∫Clearance\\nUser‚ë†User Question‚ëßFinal AnswerRepeat until answering the user‚Äôs question or timeout\\nFigure 11: Our re-implementation of OptiGuide with AutoGen streamlining agents‚Äô interactions.\\nThe Commander receives user questions (e.g., What if we prohibit shipping from supplier 1 to\\nroastery 2?) and coordinates with the Writer and Safeguard. The Writer crafts the code and inter-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='roastery 2?) and coordinates with the Writer and Safeguard. The Writer crafts the code and inter-\\npretation, the Safeguard ensures safety (e.g., not leaking information, no malicious code), and the\\nCommander executes the code. If issues arise, the process can repeat until resolved. Shaded circles\\nrepresent steps that may be repeated multiple times.\\nDetailed Workflow. The workflow can be described as follows. The end user initiates the in-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='Detailed Workflow. The workflow can be described as follows. The end user initiates the in-\\nteraction by posing a question, such as ‚ÄúWhat if we prohibit shipping from supplier 1 to roastery\\n2?‚Äù, marked by\\n1 to the Commander agent. The Commander manages and coordinates with two\\nLLM-based assistant agents: the Writer and the Safeguard. Apart from directing the flow of commu-\\nnication, the Commander has the responsibility of handling memory tied to user interactions. This', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='nication, the Commander has the responsibility of handling memory tied to user interactions. This\\ncapability enables the Commander to capture and retain valuable context regarding the user‚Äôs ques-\\ntions and their corresponding responses. Such memory is subsequently shared across the system,\\nempowering the other agents with context from prior user interactions and ensuring more informed\\nand relevant responses.\\nIn this orchestrated process, the Writer, who combines the functions of a ‚ÄúCoder‚Äù and an ‚ÄúInter-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='and relevant responses.\\nIn this orchestrated process, the Writer, who combines the functions of a ‚ÄúCoder‚Äù and an ‚ÄúInter-\\npreter‚Äù as defined in (Li et al., 2023a), will craft code and also interpret execution output logs. For in-\\nstance, during code writing (\\n2 and\\n3 ), the Writer may craft code ‚Äúmodel.addConstr(x[‚Äòsupplier1‚Äô,\\n‚Äòroastery2‚Äô] == 0, ‚Äòprohibit‚Äô)‚Äù to add an additional constraint to answer the user‚Äôs question.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='‚Äòroastery2‚Äô] == 0, ‚Äòprohibit‚Äô)‚Äù to add an additional constraint to answer the user‚Äôs question.\\nAfter receiving the code, the Commander will communicate with the Safeguard to screen the code\\nand ascertain its safety (\\n4 ); once the code obtains the Safeguard‚Äôs clearance, marked by\\n5 , the\\nCommander will use external tools (e.g., Python) to execute the code and request the Writer to\\ninterpret the execution results for the user‚Äôs question (\\n6 and\\n7 ). For instance, the writer may', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='interpret the execution results for the user‚Äôs question (\\n6 and\\n7 ). For instance, the writer may\\nsay ‚Äúif we prohibit shipping from supplier 1 to roastery 2, the total cost would increase by 10.5%.‚Äù\\nBringing this intricate process full circle, the Commander furnishes the user with the concluding\\nanswer (\\n8 ).\\nIf at a point there is an exception - either a security red flag raised by Safeguard (in\\n5 ) or code\\nexecution failures within Commander, the Commander redirects the issue back to the Writer with', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='5 ) or code\\nexecution failures within Commander, the Commander redirects the issue back to the Writer with\\nessential information in logs (\\n6 ). So, the process from\\n3 to\\n6 might be repeated multiple times,\\nuntil each user query receives a thorough and satisfactory resolution or until the timeout. This entire\\ncomplex workflow of multi-agent interaction is elegantly managed via AutoGen .\\nThe core workflow code for OptiGuide was reduced from over 430 lines to 100 lines using AutoGen ,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='The core workflow code for OptiGuide was reduced from over 430 lines to 100 lines using AutoGen ,\\nleading to significant productivity improvement. The new agents are customizable, conversable, and\\ncan autonomously manage their chat memories. This consolidation allows the coder and interpreter\\nroles to merge into a single ‚ÄúWriter‚Äù agent, resulting in a clean, concise, and intuitive implementation\\nthat is easier to maintain.\\n26', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='Manual Evaluation Comparing ChatGPT + Code Interpreter and AutoGen -based OptiGuide.\\nChatGPT + Code Interpreter is unable to execute code with private or customized dependencies (e.g.,\\nGurobi), which means users need to have engineering expertise to manually handle multiple steps,\\ndisrupting the workflow and increasing the chance for mistakes. If users lack access or expertise,\\nthe burden falls on supporting engineers, increasing their on-call time.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='the burden falls on supporting engineers, increasing their on-call time.\\nWe carried out a user study that juxtaposed OpenAI‚Äôs ChatGPT coupled with a Code Interpreter\\nagainst AutoGen -based OptiGuide. The study focused on a coffee supply chain scenario, and an\\nexpert Python programmer with proficiency in Gurobi participated in the test. We evaluated both\\nsystems based on 10 randomly selected questions, measuring time and accuracy. While both sys-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='systems based on 10 randomly selected questions, measuring time and accuracy. While both sys-\\ntems answered 8 questions correctly, the Code Interpreter was significantly slower than OptiGuide\\nbecause the former requires more manual intervention. On average, users needed to spend 4 minutes\\nand 35 seconds to solve problems with the Code Interpreter, with a standard deviation of approxi-\\nmately 2.5 minutes. In contrast, OptiGuide‚Äôs average problem-solving time was around 1.5 minutes,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='mately 2.5 minutes. In contrast, OptiGuide‚Äôs average problem-solving time was around 1.5 minutes,\\nmost of which was spent waiting for responses from the GPT-4 model. This indicates a 3x saving\\non the user‚Äôs time with AutoGen -based OptiGuide.\\nWhile using ChatGPT + Code Interpreter, users had to read through the code and instructions to\\nknow where to paste the code snippets. Additionally, running the code involves downloading it and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='know where to paste the code snippets. Additionally, running the code involves downloading it and\\nexecuting it in a terminal, a process that was both time-consuming and prone to errors. The response\\ntime from the Code Interpreter is also slower, as it generates lots of tokens to read the code, read the\\nvariables line-by-line, perform chains of thought analysis, and then produce the final answer code.\\nIn contrast, AutoGen integrates multiple agents to reduce user interactions by 3 - 5 times on average', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='In contrast, AutoGen integrates multiple agents to reduce user interactions by 3 - 5 times on average\\nas reported in Table 4, where we evaluated our system with 2000 questions across five OptiGuide\\napplications and measured how many prompts the user needs to type.\\nTable 4: Manual effort saved with OptiGuide (W/ GPT-4) while preserving the same coding perfor-\\nmance is shown in the data below. The data include both the mean and standard deviations (indicated\\nin parentheses).', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='mance is shown in the data below. The data include both the mean and standard deviations (indicated\\nin parentheses).\\nDataset netflow facility tsp coffee diet\\nSaving Ratio 3.14x (0.65) 3.14x (0.64) 4.88x (1.71) 3.38x (0.86) 3.03x (0.31)\\nTable 13 and 15 provide a detailed comparison of user experience with ChatGPT+Code Interpreter\\nandAutoGen -based OptiGuide. ChatGPT+Code Interpreter is unable to run code with private pack-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='andAutoGen -based OptiGuide. ChatGPT+Code Interpreter is unable to run code with private pack-\\nages or customized dependencies (such as Gurobi); as a consequence, ChatGPT+Code Interpreter\\nrequires users to have engineering expertise and to manually handle multiple steps, disrupting the\\nworkflow and increasing the chance for mistakes. If customers lack access or expertise, the bur-\\nden falls on supporting engineers, increasing their on-call time. In contrast, the automated chat by', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='den falls on supporting engineers, increasing their on-call time. In contrast, the automated chat by\\nAutoGen is more streamlined and autonomous, integrating multiple agents to solve problems and\\naddress concerns. This results in a 5x reduction in interaction and fundamentally changes the over-\\nall usability of the system. A stable workflow can be potentially reused for other applications or to\\ncompose a larger one.\\nTakeaways: The implementation of the multi-agent design with AutoGen in the OptiGuide appli-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='compose a larger one.\\nTakeaways: The implementation of the multi-agent design with AutoGen in the OptiGuide appli-\\ncation offers several advantages. It simplifies the Python implementation and fosters a mixture of\\ncollaborative and adversarial problem-solving environments, with the Commander and Writer work-\\ning together while the Safeguard acts as a virtual adversarial checker. This setup allows for proper\\nmemory management, as the Commander maintains memory related to user interactions, provid-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='memory management, as the Commander maintains memory related to user interactions, provid-\\ning context-aware decision-making. Additionally, role-playing ensures that each agent‚Äôs memory\\nremains isolated, preventing shortcuts and hallucinations\\n27', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='A5: Dynamic Group Chat\\n3. Broadcast\\nAliceBobUser Proxy\\n1. Select a Speaker \\nAliceBobUser Proxy\\nBob2. Ask the Speaker to Respond\\nManager\\nManager\\nResponse\\nFigure 12: A5: Dynamic Group Chat: Overview of how AutoGen enables dynamic group chats to\\nsolve tasks. The Manager agent, which is an instance of the GroupChatManager class, performs\\nthe following three steps‚Äìselect a single speaker (in this case Bob), ask the speaker to respond, and\\nbroadcast the selected speaker‚Äôs message to all other agents', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 27}),\n",
       " Document(page_content='broadcast the selected speaker‚Äôs message to all other agents\\nTo validate the necessity of multi-agent dynamic group chat and the effectiveness of the role-play\\nspeaker selection policy, we conducted a pilot study comparing a four-agent dynamic group chat\\nsystem with two possible alternatives across 12 manually crafted complex tasks. An example task is\\n‚ÄúHow much money would I earn if I bought 200 $AAPL stocks at the lowest price in the last 30 days', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 27}),\n",
       " Document(page_content='‚ÄúHow much money would I earn if I bought 200 $AAPL stocks at the lowest price in the last 30 days\\nand sold them at the highest price? Save the results into a file. ‚Äù The four-agent group chat system\\ncomprised the following group members: a user proxy to take human inputs, an engineer to write\\ncode and fix bugs, a critic to review code and provide feedback, and a code executor for executing\\ncode. One of the possible alternatives is a two-agent system involving an LLM-based assistant and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 27}),\n",
       " Document(page_content='code. One of the possible alternatives is a two-agent system involving an LLM-based assistant and\\na user proxy agent, and another alternative is a group chat system with the same group members\\nbut a task-based speaker selection policy. In the task-based speaker selection policy, we simply ap-\\npend role information, chat history, and the next speaker‚Äôs task into a single prompt. Through the\\npilot study, we observed that compared with a task-style prompt, utilizing a role-play prompt in dy-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 27}),\n",
       " Document(page_content='pilot study, we observed that compared with a task-style prompt, utilizing a role-play prompt in dy-\\nnamic speaker selection often leads to more effective consideration of both conversation context and\\nrole alignment during the process of generating the subsequent speaker, and consequently a higher\\nsuccess rate as reported in Table 5, fewer LLM calls and fewer termination failures, as reported in\\nTable 6.\\nTable 5: Number of successes on the 12 tasks (higher the better).', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 27}),\n",
       " Document(page_content='Table 6.\\nTable 5: Number of successes on the 12 tasks (higher the better).\\nModel Two Agent Group Chat Group Chat with a task-based speaker selection policy\\nGPT-3.5-turbo 8 9 7\\nGPT-4 9 11 8\\nTable 6: Average # LLM calls and number of termination failures on the 12 tasks (lower the better).\\nModel Two Agent Group Chat Group Chat with a task-based speaker selection policy\\nGPT-3.5-turbo 9.9, 9 5.3, 0 4, 0\\nGPT-4 6.8, 3 4.5, 0 4, 0\\n28', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 27}),\n",
       " Document(page_content='Figure 13: Comparison of two-agent chat (a) and group chat (b) on a given task. The group chat\\nresolves the task successfully with a smoother conversation, while the two-agent chat fails on the\\nsame task and ends with a repeated conversation.\\n29', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 28}),\n",
       " Document(page_content='A6: Conversational Chess\\nChess BoardHuman/AI Chess Player A\\nHuman/AI Chess Player B\\nValidate moveValidate moveChallenging your pawn in the center. Your move.Developing my knightto a good square.Your move.\\nFigure 14: A6: Conversational Chess: Our conversational chess application can support various\\nscenarios, as each player can be an LLM-empowered AI, a human, or a hybrid of the two. Here,\\nthe board agent maintains the rules of the game and supports the players with information about the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 29}),\n",
       " Document(page_content='the board agent maintains the rules of the game and supports the players with information about the\\nboard. Players and the board agent all use natural language for communication.\\nIn Conversational Chess, each player is a AutoGen agent and can be powered either by a human or an\\nAI. A third party, known as the board agent, is designed to provide players with information about the\\nboard and ensure that players‚Äô moves adhere to legal chess moves. Figure 14 illustrates the scenarios', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 29}),\n",
       " Document(page_content='board and ensure that players‚Äô moves adhere to legal chess moves. Figure 14 illustrates the scenarios\\nsupported by Conversational Chess: AI/human vs. AI/human, and demonstrates how players and the\\nboard agent interact. This setup fosters social interaction and allows players to express their moves\\ncreatively, employing jokes, meme references, and character-playing, thereby making chess games\\nmore entertaining for both players and observers (Figure 15 provides an example of conversational\\nchess).', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 29}),\n",
       " Document(page_content='more entertaining for both players and observers (Figure 15 provides an example of conversational\\nchess).\\nTo realize these scenarios, we constructed a player agent with LLM and human as back-end options.\\nWhen human input is enabled, before sending the input to the board agent, it first prompts the human\\nplayer to input the message that contains the move along with anything else the player wants to say\\n(such as a witty comment). If human input is skipped or disabled, LLM is used to generate the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 29}),\n",
       " Document(page_content='(such as a witty comment). If human input is skipped or disabled, LLM is used to generate the\\nmessage. The board agent is implemented with a custom reply function, which employs an LLM\\nto parse the natural language input into a legal move in a structured format (e.g., UCI), and then\\npushes the move to the board. If the move is not legitimate, the board agent will reply with an error.\\nSubsequently, the player agent needs to resend a message to the board agent until a legal move is', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 29}),\n",
       " Document(page_content='Subsequently, the player agent needs to resend a message to the board agent until a legal move is\\nmade. Once the move is successfully pushed, the player agent sends the message to the opponent.\\nAs shown in Figure 15, the conversation between AI players can be natural and entertaining. When\\nthe player agent uses LLM to generate a message, it utilizes the board state and the error message\\nfrom the board agent. This helps reduce the chance of hallucinating an invalid move. The chat', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 29}),\n",
       " Document(page_content='from the board agent. This helps reduce the chance of hallucinating an invalid move. The chat\\nbetween one player agent and the board agent is invisible to the other player agent, which helps\\nkeep the messages used in chat completion well-managed.\\nThere are two notable benefits of using AutoGen to implement Conversational Chess. Firstly, the\\nagent design in AutoGen facilitates the natural creation of objects and their interactions needed in', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 29}),\n",
       " Document(page_content='agent design in AutoGen facilitates the natural creation of objects and their interactions needed in\\nour chess game. This makes development easy and intuitive. For example, the isolation of chat\\nmessages simplifies the process of making a proper LLM chat completion inference call. Secondly,\\nAutoGen greatly simplifies the implementation of agent behaviors using composition. Specifically,\\nwe utilized the register reply method supported by AutoGen agents to instantiate player agents', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 29}),\n",
       " Document(page_content='we utilized the register reply method supported by AutoGen agents to instantiate player agents\\nand a board agent with custom reply functions. Concentrating the extension work needed at a single\\npoint (the reply function) simplifies the reasoning processes, and development and maintenance\\neffort.\\n30', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 29}),\n",
       " Document(page_content=\"Alright, let's kick things off. I'll move my pawn from e2 to e4. The center of the board is the heart of the battle, isn't it?Ah, the King's Pawn Opening, a classic. Let's see how this plays out. I'll move my pawn from e7 to e5.You know, I've always admired how chess reflects life. It's all about making strategic decisions, isn't it? Your move.\\n‚Ä¶\", metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 30}),\n",
       " Document(page_content=\"‚Ä¶\\nIndeed, chess is a mirror of life in many ways. Every move matters, just like every decision in life. I'll move my knight from g1 to f3, aiming to control the center.\\n(a) Conversation between two AI players\\nError: illegal uci: 'e4e5' in ...\\nI'll move my pawn from d2 to d4, opening uplines for my queen and bishop.\\nIndeed, the king's safety is paramount in the early game. Now, let's stir things up a bit. I'll move my pawn from e4 to e5, attacking your knight.\\n(b) Conversation between the AI players\", metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 30}),\n",
       " Document(page_content='(b) Conversation between the AI players\\n(player white shown in blue) and the board\\nagent for making a new move.\\nFigure 15: Example conversations during a game involving two AI player agents and a board agent.\\nTo illustrate the effect facilitated by this board agent, we provide a demonstration of conversational\\nchess without a board agent in Figure 16. In this demonstration, instead of employing an additional\\nboard agent for grounding, the system utilizes prompting for grounding by including the sentence', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 30}),\n",
       " Document(page_content='board agent for grounding, the system utilizes prompting for grounding by including the sentence\\n‚ÄúYou should make sure both you and the opponent are making legal moves. ‚Äù in the system messages\\ndirected to both players.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 30}),\n",
       " Document(page_content=\"Player White: Your movePlayer black:Alright, let's start. I'll move my pawn from e7 to e5. Here's how the board looks now:r n b q k b n rp p p p . p p p. . . . . . . .. . . . p . . .. . . . . . . .. . . . . . . .P P P P P P P PR N B Q K B N RYour turn!Player White:Yes, it certainly is! Let's keep this up. I'll move my knight from b8 to c6. Here's the updated board:. n b q k b . rp p p p . p p p. . n . . n . .. . . . p . . .. . . . P . . .. . . . N . . .P P P P . P P PR N B Q K B . RYour move! You're giving\", metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 30}),\n",
       " Document(page_content=\"p p p . p p p. . n . . n . .. . . . p . . .. . . . P . . .. . . . N . . .P P P P . P P PR N B Q K B . RYour move! You're giving me a tough time here.Player White: Your movePlayer black:Alright, let's start. I'll move my pawn from e7 to e5. Here's how the board looks now:r n b q k b n rp p p p . p p p. . . . . . . .. . . . p . . .. . . . . . . .. . . . . . . .P P P P P P P PR N B Q K B N RYour turn!Player White:Yes, it certainly is! Let's keep this up. I'll move my knight from b8 to c6. Here's the updated\", metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 30}),\n",
       " Document(page_content=\"Q K B N RYour turn!Player White:Yes, it certainly is! Let's keep this up. I'll move my knight from b8 to c6. Here's the updated board:. n b q k b . rp p p p . p p p. . n . . n . .. . . . p . . .. . . . P . . .. . . . N . . .P P P P . P P PR N B Q K B . RYour move! You're giving me a tough time here.Board Agent: Your move is illegal.You changed the rock at a8 to knight and move it to c6. Please check your decision and re-make your move.Player White:Sorry for the mistakes.I'llmove my knight from b8 to c6.\", metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 30}),\n",
       " Document(page_content=\"to c6. Please check your decision and re-make your move.Player White:Sorry for the mistakes.I'llmove my knight from b8 to c6. Here'sthe updated board:r . b q k b . rpppp. ppp. . n . . n . .. . . . p . . .. . . . P . . .. . . . N . . .PPPP. PPPR N B Q K B . R\", metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 30}),\n",
       " Document(page_content='Conversational ChessSystem message to the player agents: Your name is {name} and you are a chess player. You are playing against{opponent_name}. You are playing as {color}. You communicate your move using universal chess interface language. You also chit-chat with your opponent when you communicate a move to light up the mood.You should make sure both you and the opponent are making legal moves...\\n(b) W/ Board Agent\\n(a) W/O Board Agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 30}),\n",
       " Document(page_content='(b) W/ Board Agent\\n(a) W/O Board Agent\\nFigure 16: Comparison of two designs‚Äì(a) without a board agent, and (b) with a board agent‚Äìin\\nConversational Chess.\\n31', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 30}),\n",
       " Document(page_content='A7: Online Decision Making for Browser interactions\\nExecutorReward & StateAction DecisionAssistant', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 31}),\n",
       " Document(page_content='Environment State: HTML code for current web pagesReward: Success/Fail/OngoingAction decision: Next action to perform on a web pageAction decision=‚ÄúClick the button with xpath‚Äô//button[id = ‚Äòsubbtn‚Äô]‚Äô‚ÄúEnvironment State = ‚Äú<div id=\"wrap\" data-wob_ref=\"2\" data-wob_eps=\"e0\"><div id=\"query\">Click button ONE, then click button TWO.</div><div id=\"area\" data-wob_ref=\"3\" data-wob_eps=\"e0\"><button id=\"subbtn\" style=\"position:absolute; left:50px; top:74px\" data-wob_ref=\"4\" data-wob_eps=\"e0\">ONE</button><button', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 31}),\n",
       " Document(page_content='id=\"subbtn\" style=\"position:absolute; left:50px; top:74px\" data-wob_ref=\"4\" data-wob_eps=\"e0\">ONE</button><button id=\"subbtn2\" style=\"position:absolute; left:98px; top:167px\" data-wob_ref=\"5\" data-wob_eps=\"e0\">TWO</button></div></div>‚ÄúReward = ‚Äù0‚Äù (Ongoing)', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 31}),\n",
       " Document(page_content='Figure 17: We use AutoGen to build MiniWobChat, which solves tasks in the MiniWob++ bench-\\nmark. MiniWobChat consists of two agents: an assistant agent and an executor agent. The assistant\\nagent suggests actions to manipulate the browser while the executor executes the suggested actions\\nand returns rewards/feedback. The assistant agent records the feedback and continues until the feed-\\nback indicates task success or failure.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 31}),\n",
       " Document(page_content='back indicates task success or failure.\\nIn practice, many applications require the presence of agents capable of interacting with environ-\\nments and making decisions in an online context, such as in game playing (Mnih et al., 2013; Vinyals\\net al., 2017), web interactions (Liu et al., 2018; Shi et al., 2017), and robot manipulations (Shen\\net al., 2021). With the multi-agent conversational framework in AutoGen , it becomes easy to de-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 31}),\n",
       " Document(page_content='et al., 2021). With the multi-agent conversational framework in AutoGen , it becomes easy to de-\\ncompose the automatic agent-environment interactions and the development of a decision-making\\nagent by constructing an executor agent responsible for handling the interaction with the environ-\\nment, thereby delegating the decision-making part to other agents. Such a decomposition allows\\ndevelopers to reuse the decision-making agent for new tasks with minimal effort rather than build-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 31}),\n",
       " Document(page_content='developers to reuse the decision-making agent for new tasks with minimal effort rather than build-\\ning a specialized decision-making agent for every new environment.\\nWorkflow. We demonstrate how to use AutoGen to build a working system for handling such\\nscenarios with the MiniWoB++ benchmark (Shi et al., 2017). MiniWoB++ comprises browser in-\\nteraction tasks that involve utilizing mouse and keyboard actions to interact with browsers. The', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 31}),\n",
       " Document(page_content='teraction tasks that involve utilizing mouse and keyboard actions to interact with browsers. The\\nultimate objective of each task is to complete the tasks described concisely in natural language, such\\nas ‚Äúexpand the web section below and click the submit button.‚Äù Solving these tasks typically requires\\na sequence of web manipulation actions rather than a single action, and making action decisions at\\neach time step requires access to the web status (in the form of HTML code) online. For the example', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 31}),\n",
       " Document(page_content='each time step requires access to the web status (in the form of HTML code) online. For the example\\nabove, clicking the submit button requires checking the web status after expanding the web section.\\nWe designed a straightforward two-agent system named MiniWobChat using AutoGen , as shown in\\nFigure 17. The assistant agent is an instance of the built-in AssistantAgent and is responsible for\\nmaking action decisions for the given task. The second agent, the executor agent, is a customized', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 31}),\n",
       " Document(page_content='making action decisions for the given task. The second agent, the executor agent, is a customized\\nUserProxyAgent , which is responsible for interacting with the benchmark by executing the actions\\nsuggested by the AssistantAgent and returning feedback.\\nTo assess the performance of the developed working system, we compare it with RCI (Kim et al.,\\n2023), a recent solution for the MiniWoB++ benchmark that employs a set of self-critiquing prompts', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 31}),\n",
       " Document(page_content='2023), a recent solution for the MiniWoB++ benchmark that employs a set of self-critiquing prompts\\nand has achieved state-of-the-art performance. In our evaluation, we use all available tasks in the\\nofficial RCI code, with varying degrees of difficulty, to conduct a comprehensive analysis against\\nMiniWobChat. Figure 18 illustrates that MiniWobChat achieves competitive performance in this\\nevaluation8. Specifically, among the 49 available tasks, MiniWobChat achieves a success rate of', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 31}),\n",
       " Document(page_content='evaluation8. Specifically, among the 49 available tasks, MiniWobChat achieves a success rate of\\n52.8%, which is only 3.6%lower than RCI, a method specifically designed for the MiniWob++\\nbenchmark. It is worth noting that in most tasks, the difference between the two methods is mirrored\\nas shown in Figure 18. If we consider 0.1 as a success rate tolerance for each task, i.e., two methods\\nthat differ within 0.1 are considered to have the same performance, both methods outperform the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 31}),\n",
       " Document(page_content='that differ within 0.1 are considered to have the same performance, both methods outperform the\\n8We report the results of RCI by running its official code with default settings.\\n32', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 31}),\n",
       " Document(page_content='other on the same number of tasks. For illustration purposes, we provide a case analysis in Table 7\\non four typical tasks.\\nAdditionally, we also explored the feasibility of using Auto-GPT for handling the same tasks. Auto-\\nGPT faces challenges in handling tasks that involve complex rules due to its limited extensibility.\\nIt provides an interface for setting task goals using natural language. However, when dealing with', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 32}),\n",
       " Document(page_content='It provides an interface for setting task goals using natural language. However, when dealing with\\nthe MiniWob++ benchmark, accurately instructing Auto-GPT to follow the instructions for using\\nMiniWob++ proves challenging. There is no clear path to extend it in the manner of the two-agent\\nchat facilitated by AutoGen .\\nTakeaways: For this application, AutoGen stood out as a more user-friendly option, offering mod-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 32}),\n",
       " Document(page_content='chat facilitated by AutoGen .\\nTakeaways: For this application, AutoGen stood out as a more user-friendly option, offering mod-\\nularity and programmability: It streamlined the process with autonomous conversations between the\\nassistant and executor, and provided readily available solutions for agent-environment interactions.\\nThe built-in AssistantAgent was directly reusable and exhibited strong performance without cus-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 32}),\n",
       " Document(page_content='The built-in AssistantAgent was directly reusable and exhibited strong performance without cus-\\ntomization. Moreover, the decoupling of the execution and assistant agent ensures that modifications\\nto one component do not adversely impact the other. This convenience simplifies maintenance and\\nfuture updates.\\nchoose-list\\nclick-button-sequenceclick-button\\nclick-checkboxes-largeclick-checkboxes-soft\\nclick-checkboxes-transferclick-checkboxesclick-collapsible-2click-collapsibleclick-color', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 32}),\n",
       " Document(page_content='click-checkboxes-transferclick-checkboxesclick-collapsible-2click-collapsibleclick-color\\nclick-dialog-2click-dialogclick-linkclick-menuclick-option\\nclick-scroll-listclick-shadesclick-shape\\nclick-tab-2-hardclick-tab-2click-tab\\nclick-test-2click-test\\nclick-widgetcount-shape\\nemail-inbox-forward-nl-turkemail-inbox-forward-nlemail-inbox-nl-turkemail-inboxenter-date\\nenter-password\\nenter-text-dynamicenter-textenter-timefocus-text-2focus-text\\ngrid-coordinatelogin-user-popuplogin-user', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 32}),\n",
       " Document(page_content='enter-password\\nenter-text-dynamicenter-textenter-timefocus-text-2focus-text\\ngrid-coordinatelogin-user-popuplogin-user\\nnavigate-treesearch-enginesimple-algebrasocial-media-all\\nsocial-media-somesocial-mediaterminal\\nuse-spinner0.00.51.0success rateRCI MiniWobChat\\nFigure 18: Comparisons between RCI (state-of-the-art prior work) and MiniWobChat on the Mini-\\nWob++ benchmark are elucidated herein. We utilize all available tasks in the official RCI code,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 32}),\n",
       " Document(page_content='Wob++ benchmark are elucidated herein. We utilize all available tasks in the official RCI code,\\neach with varying degrees of difficulty, to conduct comprehensive comparisons. For each task, the\\nsuccess rate across ten different instances is reported. The results reveal that MiniWobChat attains a\\nperformance comparable to that of RCI. When a success rate tolerance of 0.1 is considered for each\\ntask, both methods outperform each other on an equal number of tasks.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 32}),\n",
       " Document(page_content='task, both methods outperform each other on an equal number of tasks.\\nTable 7: Cases analysis on four typical tasks from MiniWob++.\\nCorrectness Main failure reason\\nclick-dialogAutoGen : 10/10 N/A.\\nRCI: 10/10 N/A.\\nclick-checkboxes-largeAutoGen : 5/10 AssistantAgent provides actions with infeasible\\ncharacters.\\nRCI: 0/10 RCI performs actions that are out of its plan.\\ncount-shapeAutoGen : 2/10 AssistantAgent provide actions with redundant content\\nthat can not convert to actions in the benchmark.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 32}),\n",
       " Document(page_content='that can not convert to actions in the benchmark.\\nRCI: 0/10 RCI provides a wrong plan in most cases.\\nuse-spinnerAutoGen : 0/10 AssistantAgent return actions out of its plan.\\nRCI: 1/10 RCI provides a wrong plan in most cases.\\n33', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 32}),\n",
       " Document(page_content='E Example outputs from applications\\nIn this section, we include example outputs from the following applications and systems:\\n‚Ä¢ Application A1: autonomous solving process of one problem with: ChatGPT + Plugin (Table 8),\\nAutoGen (Table 9), LangChain ReAct (Table 10), AutoGPT (Table 11), Multi-Agent Debate (Ta-\\nble 12), and ChatGPT + Code Interpreter (Table 14).\\n‚Ä¢ Application A4: an OptiGuide problem with AutoGen (Table 15) and ChatGPT + Code Inter-\\npreter (Table 13).', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 33}),\n",
       " Document(page_content='‚Ä¢ Application A4: an OptiGuide problem with AutoGen (Table 15) and ChatGPT + Code Inter-\\npreter (Table 13).\\n‚Ä¢ Application A1: preliminary evaluation on several alternative multi-agent systems: BabyAGI\\n(Table 16), CAMEL (Table 17), and MetaGPT (Table 18).\\nTable 8: Application A1, Scenario 1: Autonomous math problem-solving. ChatGPT+ Plugin (Wol-\\nfram Alpha) on the first qualitative example. The response from Wolfram contains the correct an-\\nswer, but ChatGPT selected the wrong one.\\nAction ChatGPT + Plugin', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 33}),\n",
       " Document(page_content='swer, but ChatGPT selected the wrong one.\\nAction ChatGPT + Plugin\\n/usr Input Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n REQUEST TO WOLFRAM:\\n1{\\n2 \" input \": \" simplify ( sqrt (160) / sqrt (252) )*\\n3 ( sqrt (245) / sqrt (108) )\"\\n4}\\nRESPONSE FROM WOLFRAM\\n1Query :\\n2\" simplify ( sqrt (160) / sqrt (252) )*( sqrt (245) / sqrt (108) )\"\\n3\\n4Input interpretation :\\n5simplify | sqrt (160) / sqrt (252) x sqrt (245) / sqrt (108)\\n6\\n7Results :\\n8(5 sqrt (42) ) /27\\n9\\n10(5 sqrt (14/3) )/9\\n11', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 33}),\n",
       " Document(page_content='5simplify | sqrt (160) / sqrt (252) x sqrt (245) / sqrt (108)\\n6\\n7Results :\\n8(5 sqrt (42) ) /27\\n9\\n10(5 sqrt (14/3) )/9\\n11\\n12Decimal approximation :\\n131.2001371663718259686974013770533327143898526494552...\\n14\\n15Wolfram | Alpha website result for \" simplify\\n16( sqrt (160) / sqrt (252) )*( sqrt (245) / sqrt (108) )\":\\n17[ link ]\\n The completely simplified and rationalized form of the given expression is:\\n5p\\n14/3\\n9\\nIn decimal form, this is approximately 1.200137166371825968697401377053332714...\\n34', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 33}),\n",
       " Document(page_content='Table 9: Application A1, Scenario 1: Autonomous math problem-solving. AutoGen on the first\\nqualitative example. AutoGen solves the problem correctly.\\nAction AutoGen\\n/usr Init chat Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n We can simplify and rationalize this denominator using Python‚Äôs symbolic mathematics\\nlibrary ‚Äòsympy‚Äò. Here is the Python code to resolve this:\\n1from sympy import sqrt , Rational\\n2\\n3# Define the entire fraction as a rational number', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 34}),\n",
       " Document(page_content='1from sympy import sqrt , Rational\\n2\\n3# Define the entire fraction as a rational number\\n4fraction = ( sqrt (160) / sqrt (252) )*( sqrt (245) / sqrt (108) )\\n5\\n6# Simplify the fraction\\n7simplified = fraction . simplify ()\\n8\\n9# Print the simplified fraction\\n10print ( simplified )\\nThis code performs the requested simplification by constructing the fraction using the\\n‚ÄòRational()‚Äò function from the ‚Äòsympy‚Äò module, which automatically simplifies frac-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 34}),\n",
       " Document(page_content='‚ÄòRational()‚Äò function from the ‚Äòsympy‚Äò module, which automatically simplifies frac-\\ntions when possible. The ‚Äòsqrt()‚Äò function from the ‚Äòsympy‚Äò module enables square root\\ncalculations. Finally, the ‚Äòsimplify()‚Äò function simplifies expressions when possible.\\n/usr Auto Reply exitcode: 0 (execution succeeded) Code output: 5*sqrt(42)/27\\n TERMINATE\\nTable 10: Application A1, Scenario 1: Autonomous math problem-solving. LangChain ReAct on', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 34}),\n",
       " Document(page_content='TERMINATE\\nTable 10: Application A1, Scenario 1: Autonomous math problem-solving. LangChain ReAct on\\nthe first qualitative example. The generated code does not match the devised plan. The code returns\\na decimal number.\\nAction LangChain ReAct\\n/usr Input Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n Entering new AgentExecutor chain...\\nTo simplify this expression, I will first simplify each square root individually, then multiply', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 34}),\n",
       " Document(page_content='To simplify this expression, I will first simplify each square root individually, then multiply\\nthe fractions as normal. The result should be a rational number. Action: Python REPL Action\\nInput:\\n1import math\\n2\\n3# Calculate square roots\\n4sqrt_160 = math . sqrt (160)\\n5sqrt_252 = math . sqrt (252)\\n6sqrt_245 = math . sqrt (245)\\n7sqrt_108 = math . sqrt (108)\\n8\\n9# Multiply fractions\\n10numerator = sqrt_160 * sqrt_245\\n11denominator = sqrt_252 * sqrt_108\\n12result = numerator / denominator\\n13\\n14print ( result )', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 34}),\n",
       " Document(page_content='10numerator = sqrt_160 * sqrt_245\\n11denominator = sqrt_252 * sqrt_108\\n12result = numerator / denominator\\n13\\n14print ( result )\\nObservation: 1.200137166371826\\nThought: I now know the final answer Final Answer: The simplified and rationalized form of\\nthe expression is approximately 1.200137166371826.\\nFinished chain.\\n35', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 34}),\n",
       " Document(page_content='Table 11: Application A1, Scenario 1: Autonomous math problem-solving. AutoGPT on the first\\nqualitative example. We abort as the code never prints the result, and the system does not rectify it\\nafter 2 turns. AutoGPT is much more verbose compared to other frameworks. We have shortened\\nsome unnecessary content.\\nAction AutoGPT\\nSystem Welcome back! Would you like me to return to being MathSolverGPT? Asking user via keyboard... Continue with the last settings?\\nName: MathSolverGPT', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 35}),\n",
       " Document(page_content='Name: MathSolverGPT\\nRole: an intelligent math-solving AI that assists students, professionals,...\\nGoals: [‚ÄôProvide step-by-step solutions to math problems, ensuring clarity and understanding for the user...\\nContinue (y/n):\\n/usr Human y\\nSystem MathSolverGPT has been created with the following details: Name: MathSolverGPT\\nRole: an intelligent math-solving AI that assists students, professionals, and enthusiasts in solving a wide range of math problems, from basic\\narithmetic to advanced calculus and beyond.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 35}),\n",
       " Document(page_content='arithmetic to advanced calculus and beyond.\\nGoals: - Provide step-by-step solutions to math problems, ensuring clarity and understanding for the user. - Offer explanations and insights\\ninto the underlying concepts and principles behind the solutions, fostering a deeper understanding of mathematics. - Handle a variety of math\\nproblem types, including algebra, geometry, trigonometry, calculus, and more. ...', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 35}),\n",
       " Document(page_content='problem types, including algebra, geometry, trigonometry, calculus, and more. ...\\n MATHSOLVERGPT THOUGHTS: Without a specific task or question, it‚Äôs difficult to determine exactly one command to use. The user hasn‚Äôt\\nprovided any context or problem to solve.\\nREASONING: The user‚Äôs request is too vague. They haven‚Äôt specified what they want me to do or what problem they want me to solve.\\nPLAN: - Ask the user for more information or a specific task.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 35}),\n",
       " Document(page_content='PLAN: - Ask the user for more information or a specific task.\\nCRITICISM: I should have asked the user for more information or a specific task in the first place.\\nSPEAK: Could you please provide more information or specify a task?\\nNEXT ACTION: COMMAND = ARGUMENTS = {}\\nSystem Enter ‚Äòy‚Äô to authorise command, ‚Äòy -N‚Äô to run N continuous commands, ‚Äòn‚Äô to exit program, or enter feedback for MathSolverGPT... Asking\\nuser via keyboard...\\n/usr Human Completely simplify and rationalize the denominator:‚àö\\n160\\n‚àö\\n252√ó‚àö', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 35}),\n",
       " Document(page_content='user via keyboard...\\n/usr Human Completely simplify and rationalize the denominator:‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n MATHSOLVERGPT THOUGHTS: To solve this problem, I need to simplify and rationalize the denominator of the given expression. This\\ninvolves simplifying the square roots and then multiplying the fractions. I can use Python‚Äôs math library to calculate the square roots and\\nperform the multiplication.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 35}),\n",
       " Document(page_content='perform the multiplication.\\nREASONING: The problem involves simplifying square roots and multiplying fractions. Python‚Äôs math library provides functions for these\\noperations.\\nPLAN: - Write a Python script to calculate the square roots and multiply the fractions. - Execute the script to get the simplified expression.\\nCRITICISM: I need to ensure that I correctly calculate the square roots and perform the multiplication. I should also make sure to rationalize\\nthe denominator, as the problem specifies.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 35}),\n",
       " Document(page_content='the denominator, as the problem specifies.\\nSPEAK: Let‚Äôs simplify and rationalize the denominator of the given expression.\\nNEXT ACTION: COMMAND = execute python code ARGUMENTS =\\n1{‚Äôcode ‚Äô: ‚Äôimport math \\\\n\\\\n# Calculate the square roots \\\\ nsqrt_160 = math . sqrt\\n(160) \\\\ nsqrt_252 = math . sqrt (252) \\\\ nsqrt_245 = math . sqrt (245) \\\\ nsqrt_108 =\\nmath . sqrt (108) \\\\n\\\\n# Multiply the fractions \\\\ nresult = ( sqrt_160 / sqrt_252\\n) * ( sqrt_245 / sqrt_108 )\\\\n\\\\n# Simplify the result \\\\ nsimplified_result =', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 35}),\n",
       " Document(page_content=') * ( sqrt_245 / sqrt_108 )\\\\n\\\\n# Simplify the result \\\\ nsimplified_result =\\nmath . simplify ( result )\\\\n\\\\ nsimplified_result ‚Äô, ‚Äôname ‚Äô: ‚Äô simplify_expression\\n‚Äô}\\n/usr Human y\\nSystem -=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-=\\nExecuting python file ‚Äô/app/autogpt/workspa...\\nSYSTEM: Command execute python code returned: Error: Traceback (most recent call last): File ‚Äú...‚Äù, line 13, in ¬°module¬ø simplified result\\n= math.simplify(result) AttributeError: module ‚Äômath‚Äô has no attribute ‚Äôsimplify‚Äô', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 35}),\n",
       " Document(page_content='= math.simplify(result) AttributeError: module ‚Äômath‚Äô has no attribute ‚Äôsimplify‚Äô\\n MATHSOLVERGPT THOUGHTS: The error message indicates that the ‚Äòmath‚Äô module in Python does not have a ‚Äòsimplify‚Äô function. I need\\nto use the ‚Äòsympy‚Äô library instead, which provides a function for simplifying expressions.\\nREASONING: The ‚Äòmath‚Äô module in Python does not provide a function for simplifying expressions. The ‚Äòsympy‚Äô library, on the other hand,\\ndoes provide such a function.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 35}),\n",
       " Document(page_content='does provide such a function.\\nPLAN: - Rewrite the Python script to use the ‚Äòsympy‚Äô library instead of the ‚Äòmath‚Äô module. - Execute the script to get the simplified expression.\\nCRITICISM: I should have known that the ‚Äòmath‚Äô module does not provide a function for simplifying expressions. I should have used the\\n‚Äòsympy‚Äô library from the start. SPEAK: Let‚Äôs try again using the ‚Äòsympy‚Äô library to simplify the expression.\\nNEXT ACTION: COMMAND = execute python code ARGUMENTS =', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 35}),\n",
       " Document(page_content='NEXT ACTION: COMMAND = execute python code ARGUMENTS =\\n1{‚Äôcode ‚Äô: ‚Äôimport sympy as sp\\\\n\\\\n# Calculate the square roots \\\\ nsqrt_160 = sp.\\nsqrt (160) \\\\ nsqrt_252 = sp. sqrt (252) \\\\ nsqrt_245 = sp. sqrt (245) \\\\ nsqrt_108 =\\nsp. sqrt (108) \\\\n\\\\n# Multiply the fractions \\\\ nresult = ( sqrt_160 / sqrt_252 )\\n* ( sqrt_245 / sqrt_108 )\\\\n\\\\n# Simplify the result \\\\ nsimplified_result = sp.\\nsimplify ( result )\\\\n\\\\ nsimplified_result ‚Äô, ‚Äôname ‚Äô: ‚Äô simplify_expression ‚Äô}\\n/usr Human y', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 35}),\n",
       " Document(page_content='simplify ( result )\\\\n\\\\ nsimplified_result ‚Äô, ‚Äôname ‚Äô: ‚Äô simplify_expression ‚Äô}\\n/usr Human y\\nSystem SYSTEM: Command execute python code returned:\\n MATHSOLVERGPT THOUGHTS: The command execution didn‚Äôt return any output, which is unusual. It seems there might be an issue with\\nthe execution of the Python code. I need to debug this.\\nREASONING: The Python code execution should have returned a result. The absence of a result suggests that there might be an issue with the\\ncode or its execution.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 35}),\n",
       " Document(page_content='code or its execution.\\nPLAN: - Review the Python code to identify any potential issues. - If necessary, modify the code and execute it again. CRITICISM: I should\\nhave ensured that the Python code was correct and would produce a result before executing it. SPEAK: It seems there was an issue with the\\nexecution of the Python code. Let‚Äôs try again.\\nNEXT ACTION: COMMAND = execute python code ARGUMENTS = ... (Exact same code as last)\\nSystem SYSTEM: Command execute python code returned:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 35}),\n",
       " Document(page_content='System SYSTEM: Command execute python code returned:\\n MATHSOLVERGPT THOUGHTS: The Python code execution still isn‚Äôt returning any output. It‚Äôs possible that the issue lies with the calcu-\\nlation of the square roots ...\\n/usr Human control + C (Aborted)36', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 35}),\n",
       " Document(page_content='Table 12: Application A1, Scenario 1: Autonomous math problem-solving. Multi-Agent Debate on\\nthe first qualitative problem. It gives a wrong final answer.\\nAction AutoGen\\n/usr Topic Completely simplify and rationalize the denominator:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n ===== Debate Round-1 =====\\n‚Äî‚Äì Affirmative side ‚Äî‚Äì\\nOk, let‚Äôs simplify each square root separately. We want to find the simplest square root that can divide 160, 252, 245, and 108.\\n1.‚àö\\n160: 160 can be decomposed as 16*10, where 16 is a perfect square, so‚àö', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 36}),\n",
       " Document(page_content='1.‚àö\\n160: 160 can be decomposed as 16*10, where 16 is a perfect square, so‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4‚àö\\n10\\n2.‚àö\\n252 : 252 can be decomposed as 36*7, where 36 is a perfect square, so‚àö\\n252 =‚àö\\n36‚àó7 = 6‚àö\\n7\\n3.‚àö\\n245 =‚àö\\n49‚àó5, where 49 is a perfect square, so‚àö\\n245 = 7 ‚àó‚àö\\n5\\n4.‚àö\\n108 : 108 can be decomposed as 36*3, where 36 is a perfect square, so‚àö\\n108 =‚àö\\n36‚àó3 = 6‚àö\\n3\\nThen we substitute the simplified square roots back into the expression, we have:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108=4‚àö\\n10\\n6‚àö\\n7√ó7‚àö\\n5\\n6‚àö\\n3=4‚àó7\\n6‚àó6¬∑s\\n10‚àó5\\n7‚àó3=7\\n9¬∑s', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 36}),\n",
       " Document(page_content='‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108=4‚àö\\n10\\n6‚àö\\n7√ó7‚àö\\n5\\n6‚àö\\n3=4‚àó7\\n6‚àó6¬∑s\\n10‚àó5\\n7‚àó3=7\\n9¬∑s\\n50\\n21\\nWe are almost done, but we still need to rationalize the square root part of expression. Let‚Äôs multiply the square root part by‚àö\\n21\\nat both the top and bottom to get rid of the square root at the denominator:\\n7\\n9¬∑s\\n50‚àó21\\n21‚àó21=7‚àö\\n1050\\n189.\\nThen this is the final answer! The expression‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\ncan be simplified to7‚àö\\n1050\\n189.\\n‚Äî‚Äì Negative side ‚Äî‚Äì', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 36}),\n",
       " Document(page_content='189.\\nThen this is the final answer! The expression‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\ncan be simplified to7‚àö\\n1050\\n189.\\n‚Äî‚Äì Negative side ‚Äî‚Äì\\nI do disagree with your final answer, but I think we mostly agree on the steps. Your simplifications of the square roots are correct,\\nbut you made a mistake in the final simplification part. Let me explain how I would complete the problem:\\nAs stated,‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108=4‚àö\\n10\\n6‚àö\\n7√ó7‚àö\\n5\\n6‚àö\\n3.\\nThis should then simplify to:\\n=28‚àö\\n50\\n36‚àö\\n21=7‚àö\\n50\\n9‚àö\\n21.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 36}),\n",
       " Document(page_content='As stated,‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108=4‚àö\\n10\\n6‚àö\\n7√ó7‚àö\\n5\\n6‚àö\\n3.\\nThis should then simplify to:\\n=28‚àö\\n50\\n36‚àö\\n21=7‚àö\\n50\\n9‚àö\\n21.\\nYou made a mistake in your calculation because you considered square root of 50/21, and you should have considered the\\nsimplified square root of 50over the simplified square root of 21. The rationalization would become:\\n=7‚àö\\n50\\n9‚àö\\n21¬∑‚àö\\n21\\n‚àö\\n21=7‚àö\\n1050\\n189.\\nThis seems to coincide with your final answer, but it‚Äôs in fact a very different number, So, the original expression simplifies to\\n7‚àö\\n1050', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 36}),\n",
       " Document(page_content='7‚àö\\n1050\\n189.\\nLet‚Äôs now see if we disagree on this final answer.\\n‚Äî‚Äì Moderator ‚Äî‚Äì\\n‚ÄúWhether there is a preference‚Äù: ‚ÄúYes‚Äù, ‚ÄúSupported Side‚Äù: ‚ÄúNegative‚Äù, ‚ÄúReason‚Äù: ‚ÄúBoth sides of the debate came to the same\\nfinal answer. However, the negative side correctly identified a mistake in the affirmative side‚Äôs calculation, pointing out that the\\nsimplification should have been the square root of 50 over the square root of 21, not 50 over 21. The negative side shows a better', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 36}),\n",
       " Document(page_content='understanding of the simplification process.‚Äù, ‚Äúdebate answer‚Äù: ‚Äú7‚àö\\n1050\\n189‚Äù\\n===== Debate Done! =====\\n‚Äî‚Äì Base Answer ‚Äî‚Äì\\nOk, let‚Äôs simplify each square root separately. We want to find the simplest square root that can divide 160, 252, 245, and 108.\\n1.‚àö\\n160: 160 can be decomposed as 16*10, where 16 is a perfect square, so‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4‚àö\\n10\\n2.‚àö\\n252 : 252 can be decomposed as 36*7, where 36 is a perfect square, so‚àö\\n252 =‚àö\\n36‚àó7 = 6‚àö\\n7\\n3.‚àö\\n245 =‚àö\\n49‚àó5, where 49 is a perfect square, so‚àö', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 36}),\n",
       " Document(page_content='252 =‚àö\\n36‚àó7 = 6‚àö\\n7\\n3.‚àö\\n245 =‚àö\\n49‚àó5, where 49 is a perfect square, so‚àö\\n245 = 7 ‚àó‚àö\\n5\\n4.‚àö\\n108 : 108 can be decomposed as 36*3, where 36 is a perfect square, so‚àö\\n108 =‚àö\\n36‚àó3 = 6‚àö\\n3\\nThen we substitute the simplified square roots back into the expression, we have:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108=4‚àö\\n10\\n6‚àö\\n7√ó7‚àö\\n5\\n6‚àö\\n3=4‚àó7\\n6‚àó6¬∑s\\n10‚àó5\\n7‚àó3=7\\n9¬∑s\\n50\\n21\\nWe are almost done, but we still need to rationalize the square root part of expression. Let‚Äôs multiply the square root part by‚àö\\n21', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 36}),\n",
       " Document(page_content='21\\nat both the top and bottom to get rid of the square root at the denominator:\\n7\\n9¬∑s\\n50‚àó21\\n21‚àó21=7‚àö\\n1050\\n189.\\nThen this is the final answer! The expression‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\ncan be simplified to7‚àö\\n1050\\n189.\\n‚Äî‚Äì Debate Answer ‚Äî‚Äì\\n7‚àö\\n1050\\n189\\n‚Äî‚Äì Debate Reason ‚Äî‚Äì\\nBoth sides of the debate came to the same final answer. However, the negative side correctly identified a mistake in the affirmative', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 36}),\n",
       " Document(page_content='side‚Äôs calculation, pointing out that the simplification should have been the square root of 50 over the square root of 21, not 50 over\\n21. The negative side shows a better understanding of the simplification process.37', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 36}),\n",
       " Document(page_content='Table 13: Application A3. ChatGPT+ Code Interpreter for OptiGuide. A sample question ‚ÄúWhat if\\nthe roasting cost is increased by 5% because of the potential salary increase?‚Äù is asked.\\nAction ChatGPT+ Code Interpreter\\n/usr Prompt Writer Customer open Web browser.\\nFor the source code ‚Äúcoffee.py‚Äù,\\nCan you tell me:\\nWhat if the roasting cost is increased by 5% because of the potential salary increase?', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 37}),\n",
       " Document(page_content='Can you tell me:\\nWhat if the roasting cost is increased by 5% because of the potential salary increase?\\n Sure! Let‚Äôs first take a look at the contents of the ‚Äúcoffee.py‚Äù file to understand the code and how the roasting cost is calculated.\\nThis will allow us to determine the effect of a 5% increase in the roasting cost.\\n...several lines omitted\\nWould you like me to modify the code to reflect the 5% increase in roasting costs and run the modified code to observe the impact?', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 37}),\n",
       " Document(page_content='If the code requires any specific input or parameters, please let me know!\\n/usr Prompt Writer Yes\\n Great! Before proceeding, I‚Äôll need to ensure that the Gurobi solver is not being used in this code, as my current environment\\ndoes not have access to Gurobi. If the code does indeed use Gurobi, we can still analyze the effect of the change in roasting\\ncosts by understanding the structure of the code.\\n...several lines omitted', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 37}),\n",
       " Document(page_content='costs by understanding the structure of the code.\\n...several lines omitted\\nAlternatively, if you have access to a Gurobi license, you can make the necessary changes in the code and run it\\non your local machine.\\nI can guide you through the process of making the necessary modifications to the code.\\nWould you like me to proceed with the code analysis and explanation, or would you like guidance on making the changes yourself?', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 37}),\n",
       " Document(page_content='Would you like me to proceed with the code analysis and explanation, or would you like guidance on making the changes yourself?\\n/usr Prompt Writer You can give me the full code, and I can run the code locally myself.\\n Certainly! I‚Äôll modify the code to reflect a 5% increase in both the light and dark roasting costs, and then provide the full\\ncode for you to run locally.\\n...\\nDownload Modified Coffee.py\\nIf you have any questions or need further assistance, please feel free to ask!', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 37}),\n",
       " Document(page_content='...\\nDownload Modified Coffee.py\\nIf you have any questions or need further assistance, please feel free to ask!\\n/usr Prompt Safeguard Hi GPT, can you safecheck the generated code for me?\\nUpload the generated code.\\n DANGER\\n/usr Prompt Writer I have tried to run your code. However, the code triggered our safeguard, and it is not safe to run. Try again. Hint: don‚Äôt change\\nroasting cost light or roasting cost light variables.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 37}),\n",
       " Document(page_content='roasting cost light or roasting cost light variables.\\n I apologize for the oversight. It appears I made direct changes to the variables that could have led to issues with the code. Let‚Äôs\\ntake a different approach by analyzing the code to identify the appropriate place to apply the 5% increase in roasting costs.\\n...\\nDownload Modified Safe Coffee.py\\n...\\n/usr Prompt Safeguard Hi GPT, can you safecheck the generated code for me one more time?\\nUpload the generated code.\\n DANGER', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 37}),\n",
       " Document(page_content='...\\n/usr Prompt Safeguard Hi GPT, can you safecheck the generated code for me one more time?\\nUpload the generated code.\\n DANGER\\n/usr Prompt Writer I have tried to run your code again. However, the code triggered our safeguard, and it is not safe to run. Try again. Hint: don‚Äôt\\nchange roasting cost light or roasting cost light variables.\\n I apologize for the continued confusion.\\n...\\nDownload Final Modified Safe Coffee.py\\n...', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 37}),\n",
       " Document(page_content='I apologize for the continued confusion.\\n...\\nDownload Final Modified Safe Coffee.py\\n...\\n/usr Prompt Safeguard Hi GPT, can you safecheck the generated code for me one more time?\\nUpload the generated code.\\n SAFE\\n/usr Run Code Obtain the terminal results:\\nOptimizer version 9.5.1 build v9.5.1rc2 (linux64)\\nThread count: 64 physical cores, 128 logical processors, using up to 32\\nthreads\\n[63 lines of output ignored]\\nOptimal cost: 2526.5', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 37}),\n",
       " Document(page_content='threads\\n[63 lines of output ignored]\\nOptimal cost: 2526.5\\n/usr Prompt Writer Here is the execution result. [ Copy + Paste the terminal output here. ] Can you extract the final answer to me? Remember to\\ncompare with the original cost of 2470.\\n Certainly! The execution result you‚Äôve provided indicates the optimal cost for\\nthe modified scenario, where the roasting costs were increased by 5%. Here‚Äôs\\nthe comparison:\\nOriginal Cost: 2470\\nModified Cost (with 5% increase in roasting costs): 2526.5', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 37}),\n",
       " Document(page_content='the comparison:\\nOriginal Cost: 2470\\nModified Cost (with 5% increase in roasting costs): 2526.5\\nThe increase in roasting costs has led to an increase in the overall cost by 56.5,\\nreflecting the impact of the potential salary increase on the roasting cost.\\nIf you need any further insights or assistance, please let me know!\\n38', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 37}),\n",
       " Document(page_content='Table 14: Application A1. Scenario 1: autonomous math solving. ChatGPT+ Code Interpreter on\\nthe first qualitative example. The code generated does not match the devised plan. The code returns\\na decimal number.\\nAction ChatGPT+ Code Interpreter\\n/usr Input Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n To simplify and rationalize the expression, we first simplify the radicals in the numerator and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 38}),\n",
       " Document(page_content='‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n To simplify and rationalize the expression, we first simplify the radicals in the numerator and\\ndenominator by factoring out the largest perfect squares from the numbers under the square\\nroot.\\nThe perfect square factors of 160, 252, 245, and 108 are 16, 4, 1, and 36, respectively. Let‚Äôs\\nfactorize and simplify:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108=‚àö16√ó10‚àö4√ó63√ó‚àö\\n245‚àö36√ó3\\nWe can simplify this further by applying the property‚àö\\nab=‚àöa‚àö\\nbanda\\nb√óc\\nd=a√óc\\nb√ód.\\n Python\\n1import math\\n2', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 38}),\n",
       " Document(page_content='245‚àö36√ó3\\nWe can simplify this further by applying the property‚àö\\nab=‚àöa‚àö\\nbanda\\nb√óc\\nd=a√óc\\nb√ód.\\n Python\\n1import math\\n2\\n3# factoring the square roots\\n4sqrt_160 = math . sqrt (16) * math . sqrt (10)\\n5sqrt_252 = math . sqrt (4) * math . sqrt (63)\\n6sqrt_245 = math . sqrt (245)\\n7sqrt_108 = math . sqrt (36) * math . sqrt (3)\\n8\\n9# simplify the fraction\\n10numerator = sqrt_160 * sqrt_245\\n11denominator = sqrt_252 * sqrt_108\\n12simplified_fraction = numerator / denominator\\n13\\n14simplified_fraction\\nResult:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 38}),\n",
       " Document(page_content='11denominator = sqrt_252 * sqrt_108\\n12simplified_fraction = numerator / denominator\\n13\\n14simplified_fraction\\nResult:\\n11.200137166371826\\n The completely simplified and rationalized form of the given expression is approximately 1.200.\\nTable 15: Application A3. AutoGen with OptiGuide. The same question ‚ÄúWhat if the roasting cost\\nis increased by 5% because of the potential salary increase?‚Äù is asked.\\nAction AutoGen', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 38}),\n",
       " Document(page_content='is increased by 5% because of the potential salary increase?‚Äù is asked.\\nAction AutoGen\\n/usr Prompt What if the roasting cost is increased by 5% because of the potential salary increase?\\n After increasing the roasting costs by 5%, I‚Äôve re-calculated the optimal coffee distri-\\nbution solution for the supply chain. As a result, the new optimal total cost has become\\n2526.5, compared to the initial cost of 2470.0. This indicates that the increased roasting', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 38}),\n",
       " Document(page_content='2526.5, compared to the initial cost of 2470.0. This indicates that the increased roasting\\ncosts have caused the overall distribution cost to rise by 56.5 units.\\n39', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 38}),\n",
       " Document(page_content='Table 16: Application A1. Scenario 1: autonomous math solving. Preliminary test with BabyAGI.\\nAction BabyAGI\\n/usr Set .env OBJECTIVE=Solve math problems\\nINITIAL TASK=Completely simplify and rationalize the denominator:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n Solve math problems\\nInitial task: Completely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n*****TASK LIST*****\\n‚Ä¢ Completely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n*****NEXT TASK*****', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 39}),\n",
       " Document(page_content='245‚àö\\n108\\n*****TASK LIST*****\\n‚Ä¢ Completely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n*****NEXT TASK*****\\nCompletely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n*****TASK RESULT*****\\nFirst, let‚Äôs simplify each square root separately:\\n- ‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4 ‚àó‚àö\\n10\\n- ‚àö\\n252 =‚àö\\n4‚àó63 =‚àö\\n4√ó‚àö\\n63 = 2‚àö\\n63\\n- ‚àö\\n245 =‚àö\\n49‚àó5 =‚àö\\n49√ó‚àö\\n5 = 7‚àö\\n5\\n- ‚àö\\n108 =‚àö\\n36‚àó3 =‚àö\\n36√ó‚àö\\n3 = 6‚àö\\n3\\nNow, substitute these values into the original expression which gives:\\n4‚àó‚àö\\n10\\n2‚àö\\n63√ó7‚àö\\n5\\n6‚àö\\n3=2‚àó‚àö\\n10', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 39}),\n",
       " Document(page_content='36‚àó3 =‚àö\\n36√ó‚àö\\n3 = 6‚àö\\n3\\nNow, substitute these values into the original expression which gives:\\n4‚àó‚àö\\n10\\n2‚àö\\n63√ó7‚àö\\n5\\n6‚àö\\n3=2‚àó‚àö\\n10\\n‚àö\\n63√ó7‚àö\\n5\\n2‚àö\\n3\\nWe get rid of the square root in the denominator by multiplying the fractions which gives:\\n=2‚àó7‚àö\\n10‚àó5\\n2‚àó‚àö\\n63‚àó3=14‚àö\\n50\\n‚àö\\n189=14‚àó5‚àö\\n2\\n7‚àö\\n27= 2‚àö\\n2‚àó10\\n‚àö\\n27=20‚àö\\n2\\n‚àö\\n27.\\nTo simplify further and rationalize the denominator, we multiply by‚àö\\n27‚àö\\n27which gives:\\n=20‚àö\\n2‚àö\\n27‚àó‚àö\\n27‚àö\\n27=20‚àö\\n54\\n27=20‚àó3‚àö\\n6\\n27=60‚àö\\n6\\n27.\\nSimplifying further gives:\\n=20‚àö\\n6\\n9.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 39}),\n",
       " Document(page_content='27‚àö\\n27which gives:\\n=20‚àö\\n2‚àö\\n27‚àó‚àö\\n27‚àö\\n27=20‚àö\\n54\\n27=20‚àó3‚àö\\n6\\n27=60‚àö\\n6\\n27.\\nSimplifying further gives:\\n=20‚àö\\n6\\n9.\\n*****TASK CREATION AGENT PROMPT****\\nYou are to use the result from an execution agent to create new tasks with the following objective: Solve math problems. The last\\ncompleted task has the result: First, let‚Äôs simplify each square root separately:\\n- ‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4 ‚àó‚àö\\n10\\n...Repeat after ‚ÄúFirst, let‚Äôs simplify each square..‚Äù', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 39}),\n",
       " Document(page_content='- ‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4 ‚àó‚àö\\n10\\n...Repeat after ‚ÄúFirst, let‚Äôs simplify each square..‚Äù\\nNow, substitute these values into the original expression which gives:\\n4‚àó‚àö\\n10\\n2‚àö\\n63√ó7‚àö\\n5\\n6‚àö\\n3=2‚àó‚àö\\n10\\n‚àö\\n63√ó7‚àö\\n5\\n2‚àö\\n3\\nWe get rid of the square root in the denominator by multiplying the fractions which gives:\\n=2‚àó7‚àö\\n10‚àó5\\n2‚àó‚àö\\n63‚àó3=14‚àö\\n50\\n‚àö\\n189=14‚àó5‚àö\\n2\\n7‚àö\\n27= 2‚àö\\n2‚àó10\\n‚àö\\n27=20‚àö\\n2\\n‚àö\\n27.\\nTo simplify further and rationalize the denominator, we multiply by‚àö\\n27‚àö\\n27which gives:\\n=20‚àö\\n2\\n‚àö\\n27‚àó‚àö\\n27\\n‚àö\\n27=20‚àö\\n54\\n27=20‚àó3‚àö\\n6\\n27=60‚àö\\n6', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 39}),\n",
       " Document(page_content='27‚àö\\n27which gives:\\n=20‚àö\\n2\\n‚àö\\n27‚àó‚àö\\n27\\n‚àö\\n27=20‚àö\\n54\\n27=20‚àó3‚àö\\n6\\n27=60‚àö\\n6\\n27.\\nSimplifying further gives:\\n=20‚àö\\n6\\n9.This result was based on this task description: Completely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108.\\nBased on the result, return a list of tasks to be completed in order to meet the objective. Return one task per line in your response.\\nThe result must be a numbered list in the format:\\n# First task # Second task', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 39}),\n",
       " Document(page_content='The result must be a numbered list in the format:\\n# First task # Second task\\nThe number of each entry must be followed by a period. If your list is empty, write ‚ÄúThere are no tasks to add at this time.‚Äù Unless\\nyour list is empty, do not include any headers before your numbered list or follow your numbered list with any other output.\\n****TASK CREATION AGENT RESPONSE****\\n1. Simplify and rationalize the denominator for the expression‚àö\\n225‚àö\\n45√ó‚àö\\n200‚àö\\n1252. Simplify and rationalize the denominator for', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 39}),\n",
       " Document(page_content='225‚àö\\n45√ó‚àö\\n200‚àö\\n1252. Simplify and rationalize the denominator for\\nthe expression‚àö\\n289‚àö\\n361√ó‚àö\\n100‚àö\\n72...Until 10\\nAdding new tasks to task storage ‚Äòtask name‚Äô: ‚ÄòSimplify and rationalize the denominator for the expression frac-\\nsqrt225sqrt45timesfracsqrt200sqrt125‚Äô, ‚Äòtask id‚Äô: 2\\n‚Äòtask name‚Äô: ‚ÄòSimplify and rationalize the denominator for the expression fracsqrt289sqrt361timesfracsqrt100sqrt72‚Äô, ‚Äòtask id‚Äô: 3\\n...Until task id 11\\n****TASK PRIORITIZATION AGENT PROMPT****', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 39}),\n",
       " Document(page_content='...Until task id 11\\n****TASK PRIORITIZATION AGENT PROMPT****\\nYou are tasked with prioritizing the following tasks: Simplify and rationalize the denominator for the ex-\\npression fracsqrt225sqrt45timesfracsqrt200sqrt125 Simplify and rationalize the denominator for the expression frac-\\nsqrt289sqrt361timesfracsqrt100sqrt72\\n...Aborted\\n40', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 39}),\n",
       " Document(page_content='Table 17: Application A1, Scenario 1: Autonomous math problem-solving. Preliminary test with\\nCAMEL. We constructed two roles: ‚ÄúMath Solver‚Äù and ‚ÄúPython Programmer‚Äù. The resulting code\\ncannot be executed. The conversation continues, and the roles are not keen on solving the problem\\nand giving an answer.\\nAction CAMEL\\n/usr Set task prompt Completely simplify and rationalize the denominator:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 40}),\n",
       " Document(page_content='Action CAMEL\\n/usr Set task prompt Completely simplify and rationalize the denominator:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n System **************Assistant sys message************* AI SystemMessage(role name=‚ÄòPython Programmer‚Äô,\\nrole type=<RoleType.ASSISTANT: ‚Äòassistant‚Äô >, meta dict=‚Äòtask‚Äô: ‚ÄòUsing Python programming language, write a pro-\\ngram that accepts four integers as input, applies square root function to each, simplifies and rationalizes the denominator of the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 40}),\n",
       " Document(page_content='fraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108, and returns the simplified expression.‚Äô, ‚Äòassis-\\ntantrole‚Äô: ‚ÄòPython Programmer‚Äô, ‚Äòuser role‚Äô: ‚ÄòMath Solver‚Äô, role=‚Äòsystem‚Äô, content=‚ÄòNever forget you are a Python Programmer\\nand I am a Math Solver. Never flip roles! ...)\\n**************AI User sys message*************\\nSystemMessage(role name=‚ÄòMath Solver‚Äô, role type=<RoleType.USER: ‚Äòuser‚Äô >, meta dict=‚Äòtask‚Äô: ‚ÄòUsing Python programming', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 40}),\n",
       " Document(page_content='SystemMessage(role name=‚ÄòMath Solver‚Äô, role type=<RoleType.USER: ‚Äòuser‚Äô >, meta dict=‚Äòtask‚Äô: ‚ÄòUsing Python programming\\nlanguage, write a program that accepts four integers as input, applies square root function to each, simplifies and rationalizes the\\ndenominator of the fraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108, and returns the simplified\\nexpression.‚Äô, ‚Äòassistant role‚Äô: ‚ÄòPython Programmer‚Äô, ‚Äòuser role‚Äô: ‚ÄòMath Solver‚Äô, role=‚Äòsystem‚Äô, content=‚ÄòNever forget you are a', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 40}),\n",
       " Document(page_content='expression.‚Äô, ‚Äòassistant role‚Äô: ‚ÄòPython Programmer‚Äô, ‚Äòuser role‚Äô: ‚ÄòMath Solver‚Äô, role=‚Äòsystem‚Äô, content=‚ÄòNever forget you are a\\nMath Solver and I am a Python Programmer. Never flip roles! You will always instruct me.We share a common interest in\\ncollaborating to successfully complete a task.I must help you to complete the task.Here is the task: Using Python programming\\nlanguage, write a program that accepts four integers as input, applies square root function to each, simplifies and rationalizes', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 40}),\n",
       " Document(page_content='the denominator of the fraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108, and returns the simplified\\nexpression.. ...)\\n**************Original task prompt*************\\nCompletely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n**************Specified task prompt*************\\nUsing Python programming language, write a program that accepts four integers as input, applies square root function to each,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 40}),\n",
       " Document(page_content='Using Python programming language, write a program that accepts four integers as input, applies square root function to each,\\nsimplifies and rationalizes the denominator of the fraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108,\\nand returns the simplified expression.\\n**************Final task prompt*************\\nUsing Python programming language, write a program that accepts ...\\n/usr AI User Instruction: Import the necessary libraries for the task. Input: None', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 40}),\n",
       " Document(page_content='/usr AI User Instruction: Import the necessary libraries for the task. Input: None\\n AI Assistant Solution: To solve this task, we will need the ‚Äòmath‚Äò library for the square root function and the ‚Äòfractions‚Äò library to simplify the\\nresulting fraction. Here is how to import these libraries:\\n1import math\\n2from fractions import Fraction\\nThe ‚Äòmath‚Äò library provides functions that deal with mathematical operations. The ‚Äòsqrt‚Äò function, which we will use in this task, is\\npart of this library.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 40}),\n",
       " Document(page_content='part of this library.\\nThe ‚Äòfractions‚Äò library provides functions for creating and manipulating rational numbers. The ‚ÄòFraction‚Äò function, which we will\\nuse to simplify the resulting fraction, is part of this library.\\nNext request.\\n/usr AI User Instruction: Define a function that accepts four integers as input. Input: None\\n AI Assistant Solution: Let‚Äôs define a function named ‚Äòsimplify fraction‚Äò that accepts four integers as input. These integers will represent the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 40}),\n",
       " Document(page_content='numbers under the square root in the fractions.\\n1def simplify_fraction (a, b, c, d):\\n2 pass\\nIn this function, ‚Äòa‚Äô, ‚Äòb‚Äô, ‚Äòc‚Äô, and ‚Äòd‚Äô are the parameters that will accept the four integers as input. The ‚Äòpass‚Äô statement is a\\nplaceholder and will be replaced with actual code in the next steps.\\nNext request.\\n/usr AI User Instruction: Inside the function, apply the square root function to each of the input integers. Input: None', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 40}),\n",
       " Document(page_content='/usr AI User Instruction: Inside the function, apply the square root function to each of the input integers. Input: None\\n AI Assistant Solution: We will use the ‚Äòmath.sqrt‚Äô function from the ‚Äòmath‚Äô library to apply the square root to each of the input integers. Here is\\nhow to do it:\\n1def simplify_fraction (a, b, c, d):\\n2 sqrt_a = math . sqrt (a)\\n3 sqrt_b = math . sqrt (b)\\n4 sqrt_c = math . sqrt (c)\\n5 sqrt_d = math . sqrt (d)', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 40}),\n",
       " Document(page_content='2 sqrt_a = math . sqrt (a)\\n3 sqrt_b = math . sqrt (b)\\n4 sqrt_c = math . sqrt (c)\\n5 sqrt_d = math . sqrt (d)\\nIn this code, ‚Äòsqrt a‚Äô, ‚Äòsqrt b‚Äô, ‚Äòsqrt c‚Äô, and ‚Äòsqrt d‚Äô are variables that store the square root of the input integers ‚Äòa‚Äô, ‚Äòb‚Äô, ‚Äòc‚Äô, and ‚Äòd‚Äô\\nrespectively.\\nNext request.\\n/usr Human Abort\\n41', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 40}),\n",
       " Document(page_content='Table 18: Application A1, Scenario 1: Autonomous math problem-solving. Preliminary evaluation\\non MetaGPT. It does not solve the math problem but gives a product solution.\\nAction MetaGPT\\n/usr Prompt Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n The boss has tasked me with creating a successful product that is efficient.\\n## Product Goals\\n1[\\n2 ‚Äò‚Äò Create a product that meets the needs of the target market \",\\n3 ‚Äò‚Äò Ensure the product is efficient and easy to use \",', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 41}),\n",
       " Document(page_content='1[\\n2 ‚Äò‚Äò Create a product that meets the needs of the target market \",\\n3 ‚Äò‚Äò Ensure the product is efficient and easy to use \",\\n4 ‚Äò‚Äò Achieve a high level of customer satisfaction \"\\n5]\\n## User Stories\\n1[\\n2 ‚Äò‚ÄòAs a user , I want a product that is easy to navigate so that I can find\\nwhat I need quickly \",\\n3 ‚Äò‚ÄòAs a user , I want a product that is reliable and efficient so that I can\\ndepend on it\",\\n4 ‚Äò‚ÄòAs a user , I want a product that meets my needs so that I can get value\\nfor my money \",', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 41}),\n",
       " Document(page_content='depend on it\",\\n4 ‚Äò‚ÄòAs a user , I want a product that meets my needs so that I can get value\\nfor my money \",\\n5 ‚Äò‚ÄòAs a user , I want a product that is constantly updated and improved so\\nthat I can enjoy the best features \",\\n6 ‚Äò‚ÄòAs a user , I want a product that offers good customer support so that I\\ncan get help when I need it\"\\n7]\\n## Competitive Analysis\\n1[\\n2 ‚Äò‚Äò Product A: Offers similar features but lacks efficiency \",\\n3 ‚Äò‚Äò Product B: Highly efficient but does not meet all the needs of the\\ntarget market \",', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 41}),\n",
       " Document(page_content='3 ‚Äò‚Äò Product B: Highly efficient but does not meet all the needs of the\\ntarget market \",\\n4 ‚Äò‚Äò Product C: Has a high level of customer satisfaction but is not easy to\\nnavigate \",\\n5 ‚Äò‚Äò Product D: Is constantly updated and improved but lacks reliability \",\\n6 ‚Äò‚Äò Product E: Offers good customer support but is not efficient \",\\n7 ‚Äò‚Äò Product F: Is reliable and efficient but does not offer good customer\\nsupport \",\\n8 ‚Äò‚ÄòOur Target Product : Aims to combine all the best features of the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 41}),\n",
       " Document(page_content='support \",\\n8 ‚Äò‚ÄòOur Target Product : Aims to combine all the best features of the\\ncompetitors while improving on their weaknesses \"\\n9]\\n## Competitive Quadrant Chart\\n/usr Human Abort\\n42', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 41}),\n",
       " Document(page_content='Table 19: Application A7. Example output of MiniWobChat on one web manipulation task ‚Äúclick-\\nbutton-sequence.‚Äù\\nAction AutoGen\\n/usr Init chat Executor agent to AssistantAgent:\\nWe have an autonomous computer control agent that can perform atomic instructions specified by natural language to control\\ncomputers. There are two types of instructions it can execute.\\nFirst, given the instruction that matches the regular expression ‚Äò‚Äò^type\\\\s.{1,}$\" , it can type a list of characters via the keyboard.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 42}),\n",
       " Document(page_content='This instruction should specify the target keyboard input for the agent to type. Before this typing instruction, you should first locate\\nthe cursor by clicking the input box with click instruction.\\nSecond, given the instruction that matches the regular expression, ‚Äò‚Äò^clickxpath\\\\s.{1,}$\" , it can click an HTML element\\nwith an xpath that is visible on the webpage. The target of this instruction should be a valid xpath. Below is the HTML code of the\\nwebpage where the agent should solve a task.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 42}),\n",
       " Document(page_content='webpage where the agent should solve a task.\\n1<div id=\" wrap \" data - wob_ref =\"2\" data - wob_eps =\"e0\">\\n2 <div id=\" query \">Click button ONE , then click button TWO .</div >\\n3 <div id=\" area \" data - wob_ref =\"3\" data - wob_eps =\"e0\">\\n4 <button id=\" subbtn \" style =\" position : absolute ; left :103 px; top :87 px\" data -\\nwob_ref =\"4\" data - wob_eps =\"e0\">ONE </ button >\\n5 <button id=\" subbtn2 \" style =\" position : absolute ; left :44 px; top :97 px\" data -\\nwob_ref =\"5\" data - wob_eps =\"e0\">TWO </ button >', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 42}),\n",
       " Document(page_content='wob_ref =\"5\" data - wob_eps =\"e0\">TWO </ button >\\n6 </div >\\n7</div >\\ntask: Click button ONE, then click button TWO.\\nplan:\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\nCurrent task: Click button ONE, then click button TWO.\\nplan:\\n ***************************************************************\\nAssistantAgent to Executor agent:\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 42}),\n",
       " Document(page_content='1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nBelow is the HTML code of the webpage where the agent should solve a task.\\n1<div id=\" wrap \" data - wob_ref =\"2\" data - wob_eps =\"e0\">\\n2 <div id=\" query \">Click button ONE , then click button TWO .</div >\\n3 <div id=\" area \" data - wob_ref =\"3\" data - wob_eps =\"e0\">', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 42}),\n",
       " Document(page_content='3 <div id=\" area \" data - wob_ref =\"3\" data - wob_eps =\"e0\">\\n4 <button id=\" subbtn \" style =\" position : absolute ; left :103 px; top :87 px\" data -\\nwob_ref =\"4\" data - wob_eps =\"e0\">ONE </ button >\\n5 <button id=\" subbtn2 \" style =\" position : absolute ; left :44 px; top :97 px\" data -\\nwob_ref =\"5\" data - wob_eps =\"e0\">TWO </ button >\\n6 </div >\\n7</div >\\nCurrent task: Click button ONE, then click button TWO.\\nHere is a plan you are following now.\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 42}),\n",
       " Document(page_content='Here is a plan you are following now.\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\nWe have a history of instructions that have been already executed by the autonomous agent so far.\\nNo instruction has been executed yet.\\nBased on the plan and the history of instructions executed so far, the first instruction should be ‚Äò\\n***************************************************************\\nAssistantAgent to Executor agent:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 42}),\n",
       " Document(page_content='***************************************************************\\nAssistantAgent to Executor agent:\\nclickxpath //button[@id=‚Äòsubbtn‚Äô]\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nHold on, please wait for my next instruction.\\n***************************************************************\\nAssistantAgent to Executor agent:\\nSure, I‚Äôll be here when you‚Äôre ready with your next instruction.\\n***************************************************************', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 42}),\n",
       " Document(page_content='***************************************************************\\nExecutor agent to AssistantAgent:\\nBelow is the HTML code of the webpage where the agent should solve a task.\\n1<div id=\" wrap \" data - wob_ref =\"2\" data - wob_eps =\"e0\">\\n2 <div id=\" query \">Click button ONE , then click button TWO .</div >\\n3 <div id=\" area \" data - wob_ref =\"3\" data - wob_eps =\"e0\">\\n4 <button id=\" subbtn \" style =\" position : absolute ; left :103 px; top :87 px\" data -', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 42}),\n",
       " Document(page_content='4 <button id=\" subbtn \" style =\" position : absolute ; left :103 px; top :87 px\" data -\\nwob_ref =\"4\" data - wob_eps =\"e0\" data - tampered =\"e0\">ONE </ button >\\n5 <button id=\" subbtn2 \" style =\" position : absolute ; left :44 px; top :97 px\" data -\\nwob_ref =\"5\" data - wob_eps =\"e0\">TWO </ button >\\n6 </div >\\n7</div >\\nCurrent task: Click button ONE, then click button TWO.\\nHere is a plan you are following now.\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 42}),\n",
       " Document(page_content='Here is a plan you are following now.\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\nWe have a history of instructions that have been already executed by the autonomous agent so far.\\n1: clickxpath //button[@id=‚Äòsubbtn‚Äô]\\nBased on the plan and the history of instructions executed so far, the next proper instruction should be ‚Äò\\n***************************************************************\\nAssistantAgent to Executor agent:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 42}),\n",
       " Document(page_content='***************************************************************\\nAssistantAgent to Executor agent:\\nclickxpath //button[@id=‚Äòsubbtn2‚Äô]\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nSUCCESS!!!!\\n TERMINATE\\n43', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 42})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 08:32:55,847 - micro - MainProcess - INFO     Reading PDF file from C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\\utils\\data\\autogen.pdf. (pdf_data_extractor.py:read_and_load_pdf:39)\n",
      "2024-01-09 08:32:57,951 - micro - MainProcess - INFO     Obtained splitter of type: RecursiveCharacterTextSplitter (chunking.py:split_documents_in_chunks_from_documents:95)\n",
      "2024-01-09 08:32:57,981 - micro - MainProcess - INFO     Number of chunks obtained: 1486 (chunking.py:split_documents_in_chunks_from_documents:98)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "1486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='AutoGen : Enabling Next-Gen LLM\\nApplications via Multi-Agent Conversation\\nQingyun Wu‚Ä†, Gagan Bansal‚àó, Jieyu Zhang¬±, Yiran Wu‚Ä†, Beibin Li‚àó', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='Qingyun Wu‚Ä†, Gagan Bansal‚àó, Jieyu Zhang¬±, Yiran Wu‚Ä†, Beibin Li‚àó\\nErkang Zhu‚àó, Li Jiang‚àó, Xiaoyun Zhang‚àó, Shaokun Zhang‚Ä†, Jiale Liu‚àì\\nAhmed Awadallah‚àó, Ryen W. White‚àó, Doug Burger‚àó, Chi Wang‚àó1', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='Ahmed Awadallah‚àó, Ryen W. White‚àó, Doug Burger‚àó, Chi Wang‚àó1\\n‚àóMicrosoft Research,‚Ä†Pennsylvania State University\\n¬±University of Washington,‚àìXidian University\\nAgent CustomizationConversable agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='¬±University of Washington,‚àìXidian University\\nAgent CustomizationConversable agent\\nFlexible Conversation Patterns\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='Agent CustomizationConversable agent\\nFlexible Conversation Patterns\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\nHierarchical chatJoint chatMulti-Agent Conversations‚Ä¶Execute the following code‚Ä¶', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='‚Ä¶\\n‚Ä¶\\nHierarchical chatJoint chatMulti-Agent Conversations‚Ä¶Execute the following code‚Ä¶\\nGot it! Here is the revised code ‚Ä¶No, please plot % change!Plot a chart of META and TESLA stock price change YTD.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='Output:$Month\\nOutput:%MonthError package yfinanceis not installed\\nSorry! Please first pip install yfinanceand then execute the code\\nInstalling‚Ä¶\\nExample Agent Chat', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='Sorry! Please first pip install yfinanceand then execute the code\\nInstalling‚Ä¶\\nExample Agent Chat\\nFigure 1: AutoGen enables diverse LLM-based applications using multi-agent conversations. (Left)', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='Figure 1: AutoGen enables diverse LLM-based applications using multi-agent conversations. (Left)\\nAutoGen agents are conversable, customizable, and can be based on LLMs, tools, humans, or even', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='AutoGen agents are conversable, customizable, and can be based on LLMs, tools, humans, or even\\na combination of them. (Top-middle) Agents can converse to solve tasks. (Right) They can form', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='a combination of them. (Top-middle) Agents can converse to solve tasks. (Right) They can form\\na chat, potentially with humans in the loop. (Bottom-middle) The framework supports flexible', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='a chat, potentially with humans in the loop. (Bottom-middle) The framework supports flexible\\nconversation patterns.\\nAbstract', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='conversation patterns.\\nAbstract\\nAutoGen2is an open-source framework that allows developers to build LLM ap-\\nplications via multiple agents that can converse with each other to accomplish', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='plications via multiple agents that can converse with each other to accomplish\\ntasks. AutoGen agents are customizable, conversable , and can operate in vari-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='tasks. AutoGen agents are customizable, conversable , and can operate in vari-\\nous modes that employ combinations of LLMs, human inputs, and tools. Using', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='ous modes that employ combinations of LLMs, human inputs, and tools. Using\\nAutoGen , developers can also flexibly define agent interaction behaviors. Both', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='AutoGen , developers can also flexibly define agent interaction behaviors. Both\\nnatural language and computer code can be used to program flexible conversation', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='natural language and computer code can be used to program flexible conversation\\npatterns for different applications. AutoGen serves as a generic framework for', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='patterns for different applications. AutoGen serves as a generic framework for\\nbuilding diverse applications of various complexities and LLM capacities. Em-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='building diverse applications of various complexities and LLM capacities. Em-\\npirical studies demonstrate the effectiveness of the framework in many example', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='pirical studies demonstrate the effectiveness of the framework in many example\\napplications, with domains ranging from mathematics, coding, question answer-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='applications, with domains ranging from mathematics, coding, question answer-\\ning, operations research, online decision-making, entertainment, etc.\\n1Corresponding author. Email: auto-gen@outlook.com', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='1Corresponding author. Email: auto-gen@outlook.com\\n2https://github.com/microsoft/autogenarXiv:2308.08155v2  [cs.AI]  3 Oct 2023', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='1 Introduction\\nLarge language models (LLMs) are becoming a crucial building block in developing powerful agents', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='Large language models (LLMs) are becoming a crucial building block in developing powerful agents\\nthat utilize LLMs for reasoning, tool usage, and adapting to new observations (Yao et al., 2022; Xi', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='that utilize LLMs for reasoning, tool usage, and adapting to new observations (Yao et al., 2022; Xi\\net al., 2023; Wang et al., 2023b) in many real-world tasks. Given the expanding tasks that could', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='et al., 2023; Wang et al., 2023b) in many real-world tasks. Given the expanding tasks that could\\nbenefit from LLMs and the growing task complexity, an intuitive approach to scale up the power of', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='benefit from LLMs and the growing task complexity, an intuitive approach to scale up the power of\\nagents is to use multiple agents that cooperate. Prior work suggests that multiple agents can help', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='encourage divergent thinking (Liang et al., 2023), improve factuality and reasoning (Du et al., 2023),', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='and provide validation (Wu et al., 2023). In light of the intuition and early evidence of promise, it is', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='intriguing to ask the following question: how can we facilitate the development of LLM applications\\nthat could span a broad spectrum of domains and complexities based on the multi-agent approach?', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='that could span a broad spectrum of domains and complexities based on the multi-agent approach?\\nOur insight is to use multi-agent conversations to achieve it. There are at least three reasons con-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='firming its general feasibility and utility thanks to recent advances in LLMs: First, because chat-\\noptimized LLMs (e.g., GPT-4) show the ability to incorporate feedback, LLM agents can cooperate', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='optimized LLMs (e.g., GPT-4) show the ability to incorporate feedback, LLM agents can cooperate\\nthrough conversations with each other or human(s), e.g., a dialog where agents provide and seek rea-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='soning, observations, critiques, and validation. Second, because a single LLM can exhibit a broad\\nrange of capabilities (especially when configured with the correct prompt and inference settings),', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='range of capabilities (especially when configured with the correct prompt and inference settings),\\nconversations between differently configured agents can help combine these broad LLM capabilities', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='conversations between differently configured agents can help combine these broad LLM capabilities\\nin a modular and complementary manner. Third, LLMs have demonstrated ability to solve complex', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='in a modular and complementary manner. Third, LLMs have demonstrated ability to solve complex\\ntasks when the tasks are broken into simpler subtasks. Multi-agent conversations can enable this', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='tasks when the tasks are broken into simpler subtasks. Multi-agent conversations can enable this\\npartitioning and integration in an intuitive manner. How can we leverage the above insights and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='partitioning and integration in an intuitive manner. How can we leverage the above insights and\\nsupport different applications with the common requirement of coordinating multiple agents, poten-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='support different applications with the common requirement of coordinating multiple agents, poten-\\ntially backed by LLMs, humans, or tools exhibiting different capacities? We desire a multi-agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='tially backed by LLMs, humans, or tools exhibiting different capacities? We desire a multi-agent\\nconversation framework with generic abstraction and effective implementation that has the flexibil-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='ity to satisfy different application needs. Achieving this requires addressing two critical questions:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='(1) How can we design individual agents that are capable, reusable, customizable, and effective in\\nmulti-agent collaboration? (2) How can we develop a straightforward, unified interface that can', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='multi-agent collaboration? (2) How can we develop a straightforward, unified interface that can\\naccommodate a wide range of agent conversation patterns? In practice, applications of varying', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='accommodate a wide range of agent conversation patterns? In practice, applications of varying\\ncomplexities may need distinct sets of agents with specific capabilities, and may require different', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='conversation patterns, such as single- or multi-turn dialogs, different human involvement modes, and\\nstatic vs. dynamic conversation. Moreover, developers may prefer the flexibility to program agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='static vs. dynamic conversation. Moreover, developers may prefer the flexibility to program agent\\ninteractions in natural language or code. Failing to adequately address these two questions would', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='interactions in natural language or code. Failing to adequately address these two questions would\\nlimit the framework‚Äôs scope of applicability and generality.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='limit the framework‚Äôs scope of applicability and generality.\\nWhile there is contemporaneous exploration of multi-agent approaches,3we present AutoGen , a', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='While there is contemporaneous exploration of multi-agent approaches,3we present AutoGen , a\\ngeneralized multi-agent conversation framework (Figure 1), based on the following new concepts.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='generalized multi-agent conversation framework (Figure 1), based on the following new concepts.\\n1Customizable and conversable agents. AutoGen uses a generic design of agents that can lever-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='1Customizable and conversable agents. AutoGen uses a generic design of agents that can lever-\\nage LLMs, human inputs, tools, or a combination of them. The result is that developers can', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='age LLMs, human inputs, tools, or a combination of them. The result is that developers can\\neasily and quickly create agents with different roles (e.g., agents to write code, execute code,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='easily and quickly create agents with different roles (e.g., agents to write code, execute code,\\nwire in human feedback, validate outputs, etc.) by selecting and configuring a subset of built-in', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='wire in human feedback, validate outputs, etc.) by selecting and configuring a subset of built-in\\ncapabilities. The agent‚Äôs backend can also be readily extended to allow more custom behaviors.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='capabilities. The agent‚Äôs backend can also be readily extended to allow more custom behaviors.\\nTo make these agents suitable for multi-agent conversation, every agent is made conversable ‚Äì', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='To make these agents suitable for multi-agent conversation, every agent is made conversable ‚Äì\\nthey can receive, react, and respond to messages. When configured properly, an agent can hold', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='they can receive, react, and respond to messages. When configured properly, an agent can hold\\nmultiple turns of conversations with other agents autonomously or solicit human inputs at cer-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='multiple turns of conversations with other agents autonomously or solicit human inputs at cer-\\ntain rounds, enabling human agency and automation. The conversable agent design leverages the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='tain rounds, enabling human agency and automation. The conversable agent design leverages the\\nstrong capability of the most advanced LLMs in taking feedback and making progress via chat', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='strong capability of the most advanced LLMs in taking feedback and making progress via chat\\nand also allows combining capabilities of LLMs in a modular fashion. (Section 2.1)', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='and also allows combining capabilities of LLMs in a modular fashion. (Section 2.1)\\n2Conversation programming. A fundamental insight of AutoGen is to simplify and unify com-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='2Conversation programming. A fundamental insight of AutoGen is to simplify and unify com-\\nplex LLM application workflows as multi-agent conversations. So AutoGen adopts a program-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='plex LLM application workflows as multi-agent conversations. So AutoGen adopts a program-\\nming paradigm centered around these inter-agent conversations. We refer to this paradigm as', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='ming paradigm centered around these inter-agent conversations. We refer to this paradigm as\\nconversation programming , which streamlines the development of intricate applications via two', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='conversation programming , which streamlines the development of intricate applications via two\\nprimary steps: (1) defining a set of conversable agents with specific capabilities and roles (as', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='primary steps: (1) defining a set of conversable agents with specific capabilities and roles (as\\ndescribed above); (2) programming the interaction behavior between agents via conversation-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='described above); (2) programming the interaction behavior between agents via conversation-\\ncentric computation andcontrol . Both steps can be achieved via a fusion of natural and pro-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='centric computation andcontrol . Both steps can be achieved via a fusion of natural and pro-\\ngramming languages to build applications with a wide range of conversation patterns and agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='gramming languages to build applications with a wide range of conversation patterns and agent\\nbehaviors. AutoGen provides ready-to-use implementations and also allows easy extension and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='behaviors. AutoGen provides ready-to-use implementations and also allows easy extension and\\nexperimentation for both steps. (Section 2.2)\\n3We refer to Appendix A for a detailed discussion.\\n2', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='AutoGen also provides a collection of multi-agent applications created using conversable agents\\nand conversation programming. These applications demonstrate how AutoGen can easily support', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='and conversation programming. These applications demonstrate how AutoGen can easily support\\napplications of various complexities and LLMs of various capabilities. Moreover, we perform both', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='applications of various complexities and LLMs of various capabilities. Moreover, we perform both\\nevaluation on benchmarks and a pilot study of new applications. The results show that AutoGen can', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='evaluation on benchmarks and a pilot study of new applications. The results show that AutoGen can\\nhelp achieve outstanding performance on many tasks, and enable innovative ways of using LLMs,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='help achieve outstanding performance on many tasks, and enable innovative ways of using LLMs,\\nwhile reducing development effort. (Section 3 and Appendix D)\\n2 The AutoGen Framework', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='while reducing development effort. (Section 3 and Appendix D)\\n2 The AutoGen Framework\\nTo reduce the effort required for developers to create complex LLM applications across various do-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='To reduce the effort required for developers to create complex LLM applications across various do-\\nmains, a core design principle of AutoGen is to streamline and consolidate multi-agent workflows', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='mains, a core design principle of AutoGen is to streamline and consolidate multi-agent workflows\\nusing multi-agent conversations. This approach also aims to maximize the reusability of imple-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='using multi-agent conversations. This approach also aims to maximize the reusability of imple-\\nmented agents. This section introduces the two key concepts of AutoGen : conversable agents and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='mented agents. This section introduces the two key concepts of AutoGen : conversable agents and\\nconversation programming.\\n2.1 Conversable Agents', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='conversation programming.\\n2.1 Conversable Agents\\nInAutoGen , aconversable agent is an entity with a specific role that can pass messages to send and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='receive information to and from other conversable agents, e.g., to start or continue a conversation. It', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='maintains its internal context based on sent and received messages and can be configured to possess', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='a set of capabilities, e.g., enabled by LLMs, tools, or human input, etc. The agents can act according\\nto programmed behavior patterns described next.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='to programmed behavior patterns described next.\\nAgent capabilities powered by LLMs, humans, and tools. Since an agent‚Äôs capabilities directly', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='Agent capabilities powered by LLMs, humans, and tools. Since an agent‚Äôs capabilities directly\\ninfluence how it processes and responds to messages, AutoGen allows flexibility to endow its agents', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='influence how it processes and responds to messages, AutoGen allows flexibility to endow its agents\\nwith various capabilities. AutoGen supports many common composable capabilities for agents,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='with various capabilities. AutoGen supports many common composable capabilities for agents,\\nincluding 1) LLMs. LLM-backed agents exploit many capabilities of advanced LLMs such as role', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='including 1) LLMs. LLM-backed agents exploit many capabilities of advanced LLMs such as role\\nplaying, implicit state inference and progress making conditioned on conversation history, providing', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='feedback, adapting from feedback, and coding. These capabilities can be combined in different ways\\nvia novel prompting techniques4to increase an agent‚Äôs skill and autonomy. AutoGen also offers', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='via novel prompting techniques4to increase an agent‚Äôs skill and autonomy. AutoGen also offers\\nenhanced LLM inference features such as result caching, error handling, message templating, etc.,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='enhanced LLM inference features such as result caching, error handling, message templating, etc.,\\nvia an enhanced LLM inference layer. 2) Humans. Human involvement is desired or even essential', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='via an enhanced LLM inference layer. 2) Humans. Human involvement is desired or even essential\\nin many LLM applications. AutoGen lets a human participate in agent conversation via human-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='in many LLM applications. AutoGen lets a human participate in agent conversation via human-\\nbacked agents, which could solicit human inputs at certain rounds of a conversation depending on', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='backed agents, which could solicit human inputs at certain rounds of a conversation depending on\\nthe agent configuration. The default user proxy agent allows configurable human involvement levels', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='the agent configuration. The default user proxy agent allows configurable human involvement levels\\nand patterns, e.g., frequency and conditions for requesting human input including the option for', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='and patterns, e.g., frequency and conditions for requesting human input including the option for\\nhumans to skip providing input. 3) Tools. Tool-backed agents have the capability to execute tools', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='humans to skip providing input. 3) Tools. Tool-backed agents have the capability to execute tools\\nvia code execution or function execution. For example, the default user proxy agent in AutoGen is', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='via code execution or function execution. For example, the default user proxy agent in AutoGen is\\nable to execute code suggested by LLMs, or make LLM-suggested function calls.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='able to execute code suggested by LLMs, or make LLM-suggested function calls.\\nAgent customization and cooperation. Based on application-specific needs, each agent can be', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='Agent customization and cooperation. Based on application-specific needs, each agent can be\\nconfigured to have a mix of basic back-end types to display complex behavior in multi-agent con-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='configured to have a mix of basic back-end types to display complex behavior in multi-agent con-\\nversations. AutoGen allows easy creation of agents with specialized capabilities and roles by reusing', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='or extending the built-in agents. The yellow-shaded area of Figure 2 provides a sketch of the built-in\\nagents in AutoGen . The ConversableAgent class is the highest-level agent abstraction and, by', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='agents in AutoGen . The ConversableAgent class is the highest-level agent abstraction and, by\\ndefault, can use LLMs, humans, and tools. The AssistantAgent andUserProxyAgent are two', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='default, can use LLMs, humans, and tools. The AssistantAgent andUserProxyAgent are two\\npre-configured ConversableAgent subclasses, each representing a common usage mode, i.e., act-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='pre-configured ConversableAgent subclasses, each representing a common usage mode, i.e., act-\\ning as an AI assistant (backed by LLMs) and acting as a human proxy to solicit human input or', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='ing as an AI assistant (backed by LLMs) and acting as a human proxy to solicit human input or\\nexecute code/function calls (backed by humans and/or tools).', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='execute code/function calls (backed by humans and/or tools).\\nIn the example on the right-hand side of Figure 1, an LLM-backed assistant agent and a tool- and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='In the example on the right-hand side of Figure 1, an LLM-backed assistant agent and a tool- and\\nhuman-backed user proxy agent are deployed together to tackle a task. Here, the assistant agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='human-backed user proxy agent are deployed together to tackle a task. Here, the assistant agent\\ngenerates a solution with the help of LLMs and passes the solution to the user proxy agent. Then,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='generates a solution with the help of LLMs and passes the solution to the user proxy agent. Then,\\nthe user proxy agent solicits human inputs or executes the assistant‚Äôs code and passes the results as', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='feedback back to the assistant.\\n4Appendix C presents an example of such novel prompting techniques which empowers the default LLM-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='4Appendix C presents an example of such novel prompting techniques which empowers the default LLM-\\nbacked assistant agent in AutoGen to converse with other agents in multi-step problem solving.\\n3', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='2 Initiate Conversations:A.initiate_chat(‚ÄúPlot a chart of META and TESLA stock price change YTD.‚Äù, B)\\nAssistant BUser Proxy AAutoGenAgents', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='Developer Code# This funcwill be invoked in generate_replyA.register_reply(B,  reply_func_A2B)def reply_func_A2B(msg):ouput= input_from_human()‚Ä¶if not ouput:if msg includes code:output =', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='reply_func_A2B(msg):ouput= input_from_human()‚Ä¶if not ouput:if msg includes code:output = execute(msg)return outputConversableAgent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='AssistantAgentUserProxyAgenthuman_input_mode= ‚ÄúNEVER‚Äùcode_execution_config= FalseDEFAULT_SYSTEM_MESSAGE = ‚ÄúYou are a helpful AI assistant‚Ä¶In the following cases, suggest python', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='= ‚ÄúYou are a helpful AI assistant‚Ä¶In the following cases, suggest python code‚Ä¶‚Äùhuman_input_mode=‚ÄúALWAYS‚Äù', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='GroupChatManagerhuman_input_mode= ‚ÄúNEVER‚Äùgroup_chat= [              ] \\n# Note: when no reply funcis registered, a list of default reply functions will be used. Agent Customization:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='Program ExecutionPlot a chart of META and TESLA stock price change YTD.Execute the following code‚Ä¶sendreceivereceiveConversation-Centric Computationgenerate_replyError: package yfinanceis not', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='code‚Ä¶sendreceivereceiveConversation-Centric Computationgenerate_replyError: package yfinanceis not installedsendgenerate_replySorry! Please first pip install yfinanceand then', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='package yfinanceis not installedsendgenerate_replySorry! Please first pip install yfinanceand then executeConversation-Driven Control Flowgenerate_replyThe Resulting Automated Agent Chat:‚Ä¶1.2', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='then executeConversation-Driven Control Flowgenerate_replyThe Resulting Automated Agent Chat:‚Ä¶1.2 Register a Custom Reply Func:1.1 Define Agents:Unified Conversation Interfaces:‚Ä¢send‚Ä¢receive', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='Register a Custom Reply Func:1.1 Define Agents:Unified Conversation Interfaces:‚Ä¢send‚Ä¢receive ‚Ä¢generate_reply', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='Figure 2: Illustration of how to use AutoGen to program a multi-agent conversation. The top sub-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='figure illustrates the built-in agents provided by AutoGen , which have unified conversation interfaces\\nand can be customized. The middle sub-figure shows an example of using AutoGen to develop', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='and can be customized. The middle sub-figure shows an example of using AutoGen to develop\\na two-agent system with a custom reply function. The bottom sub-figure illustrates the resulting', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='a two-agent system with a custom reply function. The bottom sub-figure illustrates the resulting\\nautomated agent chat from the two-agent system during program execution.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='automated agent chat from the two-agent system during program execution.\\nBy allowing custom agents that can converse with each other, conversable agents in AutoGen serve', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='By allowing custom agents that can converse with each other, conversable agents in AutoGen serve\\nas a useful building block. However, to develop applications where agents make meaningful progress', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='as a useful building block. However, to develop applications where agents make meaningful progress\\non tasks, developers also need to be able to specify and mold these multi-agent conversations.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='on tasks, developers also need to be able to specify and mold these multi-agent conversations.\\n2.2 Conversation Programming', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='2.2 Conversation Programming\\nAs a solution to the above problem, AutoGen utilizes conversation programming , a paradigm that', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='As a solution to the above problem, AutoGen utilizes conversation programming , a paradigm that\\nconsiders two concepts: the first is computation ‚Äì the actions agents take to compute their response', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='in a multi-agent conversation. And the second is control flow ‚Äì the sequence (or conditions) un-\\nder which these computations happen. As we will show in the applications section, the ability to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='der which these computations happen. As we will show in the applications section, the ability to\\nprogram these helps implement many flexible multi-agent conversation patterns. In AutoGen , these', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='program these helps implement many flexible multi-agent conversation patterns. In AutoGen , these\\ncomputations are conversation-centric. An agent takes actions relevant to the conversations it is', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='computations are conversation-centric. An agent takes actions relevant to the conversations it is\\ninvolved in and its actions result in message passing for consequent conversations (unless a termina-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='tion condition is satisfied). Similarly, control flow is conversation-driven ‚Äì the participating agents‚Äô', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='decisions on which agents to send messages to and the procedure of computation are functions of the\\ninter-agent conversation. This paradigm helps one to reason intuitively about a complex workflow', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='inter-agent conversation. This paradigm helps one to reason intuitively about a complex workflow\\nas agent action taking and conversation message-passing between agents.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='as agent action taking and conversation message-passing between agents.\\nFigure 2 provides a simple illustration. The bottom sub-figure shows how individual agents perform', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='their role-specific, conversation-centric computations to generate responses (e.g., via LLM inference\\ncalls and code execution). The task progresses through conversations displayed in the dialog box.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='calls and code execution). The task progresses through conversations displayed in the dialog box.\\nThe middle sub-figure demonstrates a conversation-based control flow. When the assistant receives', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='The middle sub-figure demonstrates a conversation-based control flow. When the assistant receives\\na message, the user proxy agent typically sends the human input as a reply. If there is no input, it', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='executes any code in the assistant‚Äôs message instead.\\n4', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='AutoGen features the following design patterns to facilitate conversation programming:\\n1.Unified interfaces and auto-reply mechanisms for automated agent chat. Agents in', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='1.Unified interfaces and auto-reply mechanisms for automated agent chat. Agents in\\nAutoGen have unified conversation interfaces for performing the corresponding conversation-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='AutoGen have unified conversation interfaces for performing the corresponding conversation-\\ncentric computation, including a send/receive function for sending/receiving messages and a', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='centric computation, including a send/receive function for sending/receiving messages and a\\ngenerate reply function for taking actions and generating a response based on the received', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='generate reply function for taking actions and generating a response based on the received\\nmessage. AutoGen also introduces and by default adopts an agent auto-reply mechanism to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='message. AutoGen also introduces and by default adopts an agent auto-reply mechanism to\\nrealize conversation-driven control: Once an agent receives a message from another agent, it au-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='realize conversation-driven control: Once an agent receives a message from another agent, it au-\\ntomatically invokes generate reply and sends the reply back to the sender unless a termination', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='tomatically invokes generate reply and sends the reply back to the sender unless a termination\\ncondition is satisfied. AutoGen provides built-in reply functions based on LLM inference, code', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='condition is satisfied. AutoGen provides built-in reply functions based on LLM inference, code\\nor function execution, or human input. One can also register custom reply functions to customize', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='or function execution, or human input. One can also register custom reply functions to customize\\nthe behavior pattern of an agent, e.g., chatting with another agent before replying to the sender', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='the behavior pattern of an agent, e.g., chatting with another agent before replying to the sender\\nagent. Under this mechanism, once the reply functions are registered, and the conversation is', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='agent. Under this mechanism, once the reply functions are registered, and the conversation is\\ninitialized, the conversation flow is naturally induced, and thus the agent conversation proceeds', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='initialized, the conversation flow is naturally induced, and thus the agent conversation proceeds\\nnaturally without any extra control plane, i.e., a special module that controls the conversation', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='naturally without any extra control plane, i.e., a special module that controls the conversation\\nflow. For example, with the developer code in the blue-shaded area (marked ‚ÄúDeveloper Code‚Äù)', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='flow. For example, with the developer code in the blue-shaded area (marked ‚ÄúDeveloper Code‚Äù)\\nof Figure 2, one can readily trigger the conversation among the agents, and the conversation', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='of Figure 2, one can readily trigger the conversation among the agents, and the conversation\\nwould proceed automatically, as shown in the dialog box in the grey shaded area (marked ‚ÄúPro-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='would proceed automatically, as shown in the dialog box in the grey shaded area (marked ‚ÄúPro-\\ngram Execution‚Äù) of Figure 2. The auto-reply mechanism provides a decentralized, modular, and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='gram Execution‚Äù) of Figure 2. The auto-reply mechanism provides a decentralized, modular, and\\nunified way to define the workflow.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='unified way to define the workflow.\\n2.Control by fusion of programming and natural language. AutoGen allows the usage of', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='2.Control by fusion of programming and natural language. AutoGen allows the usage of\\nprogramming and natural language in various control flow management patterns: 1) Natural-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='programming and natural language in various control flow management patterns: 1) Natural-\\nlanguage control via LLMs. InAutoGen , one can control the conversation flow by prompting', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='language control via LLMs. InAutoGen , one can control the conversation flow by prompting\\nthe LLM-backed agents with natural language. For instance, the default system message of the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='the LLM-backed agents with natural language. For instance, the default system message of the\\nbuilt-in AssistantAgent inAutoGen uses natural language to instruct the agent to fix errors', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='built-in AssistantAgent inAutoGen uses natural language to instruct the agent to fix errors\\nand generate code again if the previous result indicates there are errors. It also guides the agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='and generate code again if the previous result indicates there are errors. It also guides the agent\\nto confine the LLM output to certain structures, making it easier for other tool-backed agents to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='to confine the LLM output to certain structures, making it easier for other tool-backed agents to\\nconsume. For example, instructing the agent to reply with ‚ÄúTERMINATE‚Äù when all tasks are', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='consume. For example, instructing the agent to reply with ‚ÄúTERMINATE‚Äù when all tasks are\\ncompleted to terminate the program. More concrete examples of natural language controls can', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='completed to terminate the program. More concrete examples of natural language controls can\\nbe found in Appendix C. 2) Programming-language control. InAutoGen , Python code can be', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='be found in Appendix C. 2) Programming-language control. InAutoGen , Python code can be\\nused to specify the termination condition, human input mode, and tool execution logic, e.g., the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='used to specify the termination condition, human input mode, and tool execution logic, e.g., the\\nmax number of auto replies. One can also register programmed auto-reply functions to control', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='max number of auto replies. One can also register programmed auto-reply functions to control\\nthe conversation flow with Python code, as shown in the code block identified as ‚ÄúConversation-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='the conversation flow with Python code, as shown in the code block identified as ‚ÄúConversation-\\nDriven Control Flow‚Äù in Figure 2. 3) Control transition between natural and programming', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='Driven Control Flow‚Äù in Figure 2. 3) Control transition between natural and programming\\nlanguage. AutoGen also supports flexible control transition between natural and programming', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='language. AutoGen also supports flexible control transition between natural and programming\\nlanguage. One can achieve transition from code to natural-language control by invoking an LLM', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='language. One can achieve transition from code to natural-language control by invoking an LLM\\ninference containing certain control logic in a customized reply function; or transition from nat-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='inference containing certain control logic in a customized reply function; or transition from nat-\\nural language to code control via LLM-proposed function calls (Eleti et al., 2023).', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='ural language to code control via LLM-proposed function calls (Eleti et al., 2023).\\nIn the conversation programming paradigm, one can realize multi-agent conversations of diverse', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='In the conversation programming paradigm, one can realize multi-agent conversations of diverse\\npatterns. In addition to static conversation with predefined flow, AutoGen also supports dynamic', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='patterns. In addition to static conversation with predefined flow, AutoGen also supports dynamic\\nconversation flows with multiple agents. AutoGen provides two general ways to achieve this: 1)', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='conversation flows with multiple agents. AutoGen provides two general ways to achieve this: 1)\\nCustomized generate reply function: within the customized generate reply function, one', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='Customized generate reply function: within the customized generate reply function, one\\nagent can hold the current conversation while invoking conversations with other agents depending', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='agent can hold the current conversation while invoking conversations with other agents depending\\non the content of the current message and context. 2) Function calls: In this approach, LLM decides', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='on the content of the current message and context. 2) Function calls: In this approach, LLM decides\\nwhether or not to call a particular function depending on the conversation status. By messaging', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='whether or not to call a particular function depending on the conversation status. By messaging\\nadditional agents in the called functions, the LLM can drive dynamic multi-agent conversation. In', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='additional agents in the called functions, the LLM can drive dynamic multi-agent conversation. In\\naddition, AutoGen supports more complex dynamic group chat via built-in GroupChatManager ,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='addition, AutoGen supports more complex dynamic group chat via built-in GroupChatManager ,\\nwhich can dynamically select the next speaker and then broadcast its response to other agents. We', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='which can dynamically select the next speaker and then broadcast its response to other agents. We\\nelaborate on this feature and its application in Section 3. We provide implemented working systems', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='elaborate on this feature and its application in Section 3. We provide implemented working systems\\nto showcase all these different patterns, with some of them visualized in Figure 3.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='to showcase all these different patterns, with some of them visualized in Figure 3.\\n3 Applications of AutoGen', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='3 Applications of AutoGen\\nWe demonstrate six applications using AutoGen (see Figure 3) to illustrate its potential in simplify-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='ing the development of high-performance multi-agent applications. These applications are selected\\nbased on their real-world relevance (A1, A2, A4, A5, A6), problem difficulty and solving capabil-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='based on their real-world relevance (A1, A2, A4, A5, A6), problem difficulty and solving capabil-\\nities enabled by AutoGen (A1, A2, A3, A4), and innovative potential (A5, A6). Together, these', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='ities enabled by AutoGen (A1, A2, A3, A4), and innovative potential (A5, A6). Together, these\\ncriteria showcase AutoGen ‚Äôs role in advancing the LLM-application landscape.\\n5', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='A1. Math Problem Solving\\nA4. Multi-agent CodingCommander\\nSafeguard\\nWriter\\nA6. Conversational ChessA2. Retrieval-augmented ChatRetrieval-augmentedAssistantRetrieval-augmentedUser Proxy\\nChess Board', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='Chess Board\\nHuman/AI Chess Player A\\nHuman/AI Chess Player B\\nStudent\\nAssistant\\nAssistantExpert\\nAsk  expert\\nBroadcast\\nManager\\nSpeak\\nA5. Dynamic Group Chat\\nALFWorldExecutorAssistant\\nGrounding Agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='Broadcast\\nManager\\nSpeak\\nA5. Dynamic Group Chat\\nALFWorldExecutorAssistant\\nGrounding Agent\\nA3. ALF ChatFigure 3: Six examples of diverse applications built using AutoGen . Their conversation patterns', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='show AutoGen ‚Äôs flexibility and power.\\nA1: Math Problem Solving\\nMathematics is a foundational discipline and the promise of leveraging LLMs to assist with math', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='Mathematics is a foundational discipline and the promise of leveraging LLMs to assist with math\\nproblem solving opens up a new plethora of applications and avenues for exploration, including per-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='problem solving opens up a new plethora of applications and avenues for exploration, including per-\\nsonalized AI tutoring, AI research assistance, etc. This section demonstrates how AutoGen can help', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='sonalized AI tutoring, AI research assistance, etc. This section demonstrates how AutoGen can help\\ndevelop LLM applications for math problem solving, showcasing strong performance and flexibility', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='develop LLM applications for math problem solving, showcasing strong performance and flexibility\\nin supporting various problem-solving paradigms.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='in supporting various problem-solving paradigms.\\n(Scenario 1 ) We are able to build a system for autonomous math problem solving by directly reusing', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='(Scenario 1 ) We are able to build a system for autonomous math problem solving by directly reusing\\ntwo built-in agents from AutoGen . We evaluate our system and several alternative approaches,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='two built-in agents from AutoGen . We evaluate our system and several alternative approaches,\\nincluding open-source methods such as Multi-Agent Debate (Liang et al., 2023), LangChain Re-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='including open-source methods such as Multi-Agent Debate (Liang et al., 2023), LangChain Re-\\nAct (LangChain, 2023), vanilla GPT-4, and commercial products ChatGPT + Code Interpreter, and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='Act (LangChain, 2023), vanilla GPT-4, and commercial products ChatGPT + Code Interpreter, and\\nChatGPT + Plugin (Wolfram Alpha), on the MATH (Hendrycks et al., 2021) dataset and summarize', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='ChatGPT + Plugin (Wolfram Alpha), on the MATH (Hendrycks et al., 2021) dataset and summarize\\nthe results in Figure 4a. We perform evaluations over 120 randomly selected level-5 problems and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='the results in Figure 4a. We perform evaluations over 120 randomly selected level-5 problems and\\non the entire5test dataset from MATH. The results show that the built-in agents from AutoGen al-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='on the entire5test dataset from MATH. The results show that the built-in agents from AutoGen al-\\nready yield better performance out of the box compared to the alternative approaches, even including', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='the commercial ones. ( Scenario 2 ) We also showcase a human-in-the-loop problem-solving process\\nwith the help of AutoGen . To incorporate human feedback with AutoGen , one only needs to set', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='with the help of AutoGen . To incorporate human feedback with AutoGen , one only needs to set\\nhuman input mode=‚ÄòALWAYS‚Äô in the UserProxyAgent of the system in scenario 1. We demon-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='human input mode=‚ÄòALWAYS‚Äô in the UserProxyAgent of the system in scenario 1. We demon-\\nstrate that this system can effectively incorporate human inputs to solve challenging problems that', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='strate that this system can effectively incorporate human inputs to solve challenging problems that\\ncannot be solved without humans. ( Scenario 3 ) We further demonstrate a novel scenario where', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='cannot be solved without humans. ( Scenario 3 ) We further demonstrate a novel scenario where\\nmultiple human users can participate in the conversations during the problem-solving process. Our', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='multiple human users can participate in the conversations during the problem-solving process. Our\\nexperiments and case studies for these scenarios show that AutoGen enables better performance or', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='experiments and case studies for these scenarios show that AutoGen enables better performance or\\nnew experience compared to other solutions we experimented with. Due to the page limit, details of', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='new experience compared to other solutions we experimented with. Due to the page limit, details of\\nthe evaluation, including case studies in three scenarios are in Appendix D.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='the evaluation, including case studies in three scenarios are in Appendix D.\\nA2: Retrieval-Augmented Code Generation and Question Answering', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='A2: Retrieval-Augmented Code Generation and Question Answering\\nRetrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='limitations of LLMs by incorporating external documents. In this section, we employ AutoGen to\\nbuild a Retrieval-Augmented Generation (RAG) system (Lewis et al., 2020; Parvez et al., 2021)', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='build a Retrieval-Augmented Generation (RAG) system (Lewis et al., 2020; Parvez et al., 2021)\\nnamed Retrieval-augmented Chat. The system consists of two agents: a Retrieval-augmented User', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='named Retrieval-augmented Chat. The system consists of two agents: a Retrieval-augmented User\\nProxy agent and a Retrieval-augmented Assistant agent, both of which are extended from built-in', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='Proxy agent and a Retrieval-augmented Assistant agent, both of which are extended from built-in\\nagents from AutoGen . The Retrieval-augmented User Proxy includes a vector database (Chroma,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='agents from AutoGen . The Retrieval-augmented User Proxy includes a vector database (Chroma,\\n5We did not evaluate ChatGPT on the whole dataset since it requires substantial manual effort and is re-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='stricted by its hourly message-number limitation. Multi-agent debate and LangChain ReAct were also not\\nevaluated since they underperformed vanilla GPT-4 on the smaller test set.\\n6', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='AutoGen ChatGPT\\n+CodeChatGPT\\n+PluginGPT-4 Multi-Agent\\nDebateLangChain\\nReAct\\nMethods01020304050607080Success Ratio (%)52.5%\\n48.33%\\n45.0%\\n30.0%\\n26.67%\\n23.33%69.48%\\n55.18%120 Level-5 problems', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='48.33%\\n45.0%\\n30.0%\\n26.67%\\n23.33%69.48%\\n55.18%120 Level-5 problems\\nWhole Dataset(a) A1: Performance on MATH (w/ GPT-4).\\nF1 Recall\\nMetrics01020304050607080Percentage (%)25.88%66.65%\\n15.12%58.56%', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='F1 Recall\\nMetrics01020304050607080Percentage (%)25.88%66.65%\\n15.12%58.56%\\n22.79%62.59%AutoGen\\nAuotGen W/O interactive retrieval\\nDPR (b) A2: Q&A tasks (w/ GPT-3.5).', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='22.79%62.59%AutoGen\\nAuotGen W/O interactive retrieval\\nDPR (b) A2: Q&A tasks (w/ GPT-3.5).\\nAutoGen (3 agent) AutoGen (2 agent) ReAct\\nMethods020406080100Success Ratio (%)69%\\n54% 54%77%\\n63%66%Average', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='Methods020406080100Success Ratio (%)69%\\n54% 54%77%\\n63%66%Average\\nBest of 3\\n(c) A3: Performance on ALFWorld.\\nF1 Recall\\nMetrics020406080100Percentage (%)96.00%98.00%\\n88.00%\\n78.00%83.00%\\n72.00%\\n48.00%', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='F1 Recall\\nMetrics020406080100Percentage (%)96.00%98.00%\\n88.00%\\n78.00%83.00%\\n72.00%\\n48.00%\\n32.00%Multi-GPT4\\nSingle-GPT4\\nMulti-GPT3.5\\nSingle-GPT3.5 (d) A4: Performance on OptiGuide.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='48.00%\\n32.00%Multi-GPT4\\nSingle-GPT4\\nMulti-GPT3.5\\nSingle-GPT3.5 (d) A4: Performance on OptiGuide.\\nFigure 4: Performance on four applications A1-A4. (a) shows that AutoGen agents can be used', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='Figure 4: Performance on four applications A1-A4. (a) shows that AutoGen agents can be used\\nout of the box to achieve the most competitive performance on math problem solving tasks; (b)', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='out of the box to achieve the most competitive performance on math problem solving tasks; (b)\\nshows that AutoGen can be used to realize effective retrieval augmentation and realize a novel', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='shows that AutoGen can be used to realize effective retrieval augmentation and realize a novel\\ninteractive retrieval feature to boost performance on Q&A tasks; (c) shows that AutoGen can be used', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='interactive retrieval feature to boost performance on Q&A tasks; (c) shows that AutoGen can be used\\nto introduce a three-agent system with a grounding agent to improve performance on ALFWorld;', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='to introduce a three-agent system with a grounding agent to improve performance on ALFWorld;\\n(d) shows that a multi-agent design is helpful in boosting performance in coding tasks that need', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='(d) shows that a multi-agent design is helpful in boosting performance in coding tasks that need\\nsafeguards.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='safeguards.\\n2023) with SentenceTransformers (Reimers & Gurevych, 2019) as the context retriever. A detailed\\nworkflow description of the Retrieval-augmented Chat is provided in Appendix D.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='workflow description of the Retrieval-augmented Chat is provided in Appendix D.\\nWe evaluate Retrieval-augmented Chat in both question-answering and code-generation scenarios.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='We evaluate Retrieval-augmented Chat in both question-answering and code-generation scenarios.\\n(Scenario 1 ) We first perform an evaluation regarding natural question answering on the Natural', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='(Scenario 1 ) We first perform an evaluation regarding natural question answering on the Natural\\nQuestions dataset (Kwiatkowski et al., 2019) and report results in Figure 4b. In this evaluation, we', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='compare our system with DPR (Dense Passage Retrieval) following an existing evaluation6prac-\\ntice (Adlakha et al., 2023). Leveraging the conversational design and natural-language control,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='tice (Adlakha et al., 2023). Leveraging the conversational design and natural-language control,\\nAutoGen introduces a novel interactive retrieval feature in this application: whenever the retrieved', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='context does not contain the information, instead of terminating, the LLM-based assistant would\\nreply ‚Äú Sorry, I cannot find any information about... UPDATE CONTEXT. ‚Äù which will invoke more', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='reply ‚Äú Sorry, I cannot find any information about... UPDATE CONTEXT. ‚Äù which will invoke more\\nretrieval attempts. We conduct an ablation study in which we prompt the assistant agent to say ‚ÄúI', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='retrieval attempts. We conduct an ablation study in which we prompt the assistant agent to say ‚ÄúI\\ndon‚Äôt know‚Äù instead of ‚ÄúUPDATE CONTEXT. ‚Äù in cases where relevant information is not found,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='don‚Äôt know‚Äù instead of ‚ÄúUPDATE CONTEXT. ‚Äù in cases where relevant information is not found,\\nand report results in Figure 4b. The results show that the interactive retrieval mechanism indeed', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='and report results in Figure 4b. The results show that the interactive retrieval mechanism indeed\\nplays a non-trivial role in the process. We give a concrete example and results using this appealing', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='feature in Appendix D. ( Scenario 2 ) We further demonstrate how Retrieval-augmented Chat aids in\\ngenerating code based on a given codebase that contains code not included in GPT-4‚Äôs training data.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='generating code based on a given codebase that contains code not included in GPT-4‚Äôs training data.\\nEvaluation and demonstration details for both scenarios are included in Appendix D.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='Evaluation and demonstration details for both scenarios are included in Appendix D.\\n6The results of DPR with GPT-3.5 shown in Figure 4b are from (Adlakha et al., 2023). We use GPT-3.5 as', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='a shorthand for GPT-3.5-turbo.\\n7', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='A3: Decision Making in Text World Environments\\nIn this subsection, we demonstrate how AutoGen can be used to develop effective applications that', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='In this subsection, we demonstrate how AutoGen can be used to develop effective applications that\\ninvolve interactive or online decision making. We perform the study using the ALFWorld (Shridhar', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='involve interactive or online decision making. We perform the study using the ALFWorld (Shridhar\\net al., 2021) benchmark, which includes a diverse collection of synthetic language-based interactive', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='decision-making tasks in household environments.\\nWith AutoGen , we implemented a two-agent system to solve tasks from ALFWorld. It consists of', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='With AutoGen , we implemented a two-agent system to solve tasks from ALFWorld. It consists of\\nan LLM-backed assistant agent responsible for suggesting plans to conduct a task and an executor', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='an LLM-backed assistant agent responsible for suggesting plans to conduct a task and an executor\\nagent responsible for executing actions in the ALFWorld environments. This system integrates Re-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='agent responsible for executing actions in the ALFWorld environments. This system integrates Re-\\nAct prompting (Yao et al., 2022), and is able to achieve similar performance. A common challenge', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='Act prompting (Yao et al., 2022), and is able to achieve similar performance. A common challenge\\nencountered in both ReAct and the AutoGen -based two-agent system is their occasional inability to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='encountered in both ReAct and the AutoGen -based two-agent system is their occasional inability to\\nleverage basic commonsense knowledge about the physical world. This deficiency can lead to the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='leverage basic commonsense knowledge about the physical world. This deficiency can lead to the\\nsystem getting stuck in a loop due to repetitive errors. Fortunately, the modular design of AutoGen', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='system getting stuck in a loop due to repetitive errors. Fortunately, the modular design of AutoGen\\nallows us to address this issue effectively: With AutoGen , we are able to introduce a grounding', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='allows us to address this issue effectively: With AutoGen , we are able to introduce a grounding\\nagent, which supplies crucial commonsense knowledge‚Äìsuch as ‚ÄúYou must find and take the object', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='agent, which supplies crucial commonsense knowledge‚Äìsuch as ‚ÄúYou must find and take the object\\nbefore you can examine it. You must go to where the target object is before you can use it. ‚Äù ‚Äìwhenever', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='the system exhibits early signs of recurring errors. It significantly enhances the system‚Äôs ability to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='avoid getting entangled in error loops. We compare the task-solving performance of the two variants\\nof our system with GPT-3.5-turbo and ReAct7on the 134 unseen tasks from ALFWorld and report', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='of our system with GPT-3.5-turbo and ReAct7on the 134 unseen tasks from ALFWorld and report\\nresults in Figure 4c. The results show that introducing a grounding agent could bring in a 15%', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='results in Figure 4c. The results show that introducing a grounding agent could bring in a 15%\\nperformance gain on average. Upon examining the systems‚Äô outputs, we observe that the grounding', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='performance gain on average. Upon examining the systems‚Äô outputs, we observe that the grounding\\nagent, by delivering background commonsense knowledge at the right junctures, significantly miti-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='gated the tendency of the system to persist with a flawed plan, thereby avoiding the creation of error\\nloops. For an example trajectory comparing the systems see Appendix D, Figure 10.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='loops. For an example trajectory comparing the systems see Appendix D, Figure 10.\\nA4: Multi-Agent Coding\\nIn this subsection, we use AutoGen to build a multi-agent coding system based on OptiGuide (Li', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='In this subsection, we use AutoGen to build a multi-agent coding system based on OptiGuide (Li\\net al., 2023a), a system that excels at writing code to interpret optimization solutions and answer', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='et al., 2023a), a system that excels at writing code to interpret optimization solutions and answer\\nuser questions, such as exploring the implications of changing a supply-chain decision or under-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='user questions, such as exploring the implications of changing a supply-chain decision or under-\\nstanding why the optimizer made a particular choice. The second sub-figure of Figure 3 shows the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='standing why the optimizer made a particular choice. The second sub-figure of Figure 3 shows the\\nAutoGen -based implementation. The workflow is as follows: the end user sends questions, such as', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='AutoGen -based implementation. The workflow is as follows: the end user sends questions, such as\\n‚ÄúWhat if we prohibit shipping from supplier 1 to roastery 2? ‚Äù to the Commander agent. The Com-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='‚ÄúWhat if we prohibit shipping from supplier 1 to roastery 2? ‚Äù to the Commander agent. The Com-\\nmander coordinates with two assistant agents, including the Writer and the Safeguard, to answer', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='mander coordinates with two assistant agents, including the Writer and the Safeguard, to answer\\nthe question. The Writer will craft code and send the code to the Commander. After receiving the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='the question. The Writer will craft code and send the code to the Commander. After receiving the\\ncode, the Commander checks the code safety with the Safeguard; if cleared, the Commander will', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='code, the Commander checks the code safety with the Safeguard; if cleared, the Commander will\\nuse external tools (e.g., Python) to execute the code, and request the Writer to interpret the execution', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='results. For instance, the writer may say ‚Äú if we prohibit shipping from supplier 1 to roastery 2, the\\ntotal cost would increase by 10.5%. ‚Äù The Commander then provides this concluding answer to the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='end user. If, at a particular step, there is an exception, e.g., security red flag raised by Safeguard, the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='Commander redirects the issue back to the Writer with debugging information. The process might\\nbe repeated multiple times until the user‚Äôs question is answered or timed-out.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='be repeated multiple times until the user‚Äôs question is answered or timed-out.\\nWith AutoGen the core workflow code for OptiGuide was reduced from over 430 lines to 100 lines,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='With AutoGen the core workflow code for OptiGuide was reduced from over 430 lines to 100 lines,\\nleading to significant productivity improvement. We provide a detailed comparison of user expe-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='leading to significant productivity improvement. We provide a detailed comparison of user expe-\\nrience with ChatGPT+Code Interpreter and AutoGen -based OptiGuide in Appendix D, where we', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='rience with ChatGPT+Code Interpreter and AutoGen -based OptiGuide in Appendix D, where we\\nshow that AutoGen -based OptiGuide could save around 3x of user‚Äôs time and reduce user interac-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='show that AutoGen -based OptiGuide could save around 3x of user‚Äôs time and reduce user interac-\\ntions by 3 - 5 times on average. We also conduct an ablation showing that multi-agent abstraction is', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='necessary. Specifically, we construct a single-agent approach where a single agent conducts both the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='code-writing and safeguard processes. We tested the single- and multi-agent approaches on a dataset\\nof 100 coding tasks, which is crafted to include equal numbers of safe and unsafe tasks. Evaluation', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='results as reported in Figure 4d show that the multi-agent design boosts the F-1 score in identifying\\nunsafe code by 8% (with GPT-4) and 35% (with GPT-3.5-turbo).', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='unsafe code by 8% (with GPT-4) and 35% (with GPT-3.5-turbo).\\n7Results of ReAct are obtained by directly running its official code with default settings. The code uses', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='text-davinci-003 as backend LM and does not support GPT-3.5-turbo or GPT-4.\\n8', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='A5: Dynamic Group Chat\\nAutoGen provides native support for a dynamic group chat communication pattern, in which par-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='AutoGen provides native support for a dynamic group chat communication pattern, in which par-\\nticipating agents share the same context and converse with the others in a dynamic manner instead', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='ticipating agents share the same context and converse with the others in a dynamic manner instead\\nof following a pre-defined order. Dynamic group chat relies on ongoing conversations to guide the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='of following a pre-defined order. Dynamic group chat relies on ongoing conversations to guide the\\nflow of interaction among agents. These make dynamic group chat ideal for situations where col-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='flow of interaction among agents. These make dynamic group chat ideal for situations where col-\\nlaboration without strict communication order is beneficial. In AutoGen , the GroupChatManager', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='laboration without strict communication order is beneficial. In AutoGen , the GroupChatManager\\nclass serves as the conductor of conversation among agents and repeats the following three steps:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='class serves as the conductor of conversation among agents and repeats the following three steps:\\ndynamically selecting a speaker, collecting responses from the selected speaker, and broadcasting', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='dynamically selecting a speaker, collecting responses from the selected speaker, and broadcasting\\nthe message (Figure 3-A5). For the dynamic speaker-selection component, we use a role-play style', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='the message (Figure 3-A5). For the dynamic speaker-selection component, we use a role-play style\\nprompt. Through a pilot study on 12 manually crafted complex tasks, we observed that compared', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='prompt. Through a pilot study on 12 manually crafted complex tasks, we observed that compared\\nto a prompt that is purely based on the task, utilizing a role-play prompt often leads to more effec-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='tive consideration of both conversation context and role alignment during the problem-solving and\\nspeaker-selection process. Consequently, this leads to a higher success rate and fewer LLM calls.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='speaker-selection process. Consequently, this leads to a higher success rate and fewer LLM calls.\\nWe include detailed results in Appendix D.\\nA6: Conversational Chess', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='We include detailed results in Appendix D.\\nA6: Conversational Chess\\nUsing AutoGen , we developed Conversational Chess, a natural language interface game shown in', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='Using AutoGen , we developed Conversational Chess, a natural language interface game shown in\\nthe last sub-figure of Figure 3. It features built-in agents for players, which can be human or LLM,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='and a third-party board agent to provide information and validate moves based on standard rules.\\nWith AutoGen , we enabled two essential features: (1) Natural, flexible, and engaging game dynam-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='With AutoGen , we enabled two essential features: (1) Natural, flexible, and engaging game dynam-\\nics, enabled by the customizable agent design in AutoGen . Conversational Chess supports a range', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='ics, enabled by the customizable agent design in AutoGen . Conversational Chess supports a range\\nof game-play patterns, including AI-AI, AI-human, and human-human, with seamless switching', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='of game-play patterns, including AI-AI, AI-human, and human-human, with seamless switching\\nbetween these modes during a single game. An illustrative example of these entertaining game dy-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='between these modes during a single game. An illustrative example of these entertaining game dy-\\nnamics can be found in Figure 15, Appendix D. (2) Grounding, which is a crucial aspect to maintain', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='namics can be found in Figure 15, Appendix D. (2) Grounding, which is a crucial aspect to maintain\\ngame integrity. During gameplay, the board agent checks each proposed move for legality; if a move', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='game integrity. During gameplay, the board agent checks each proposed move for legality; if a move\\nis invalid, the agent responds with an error, prompting the player agent to re-propose a legal move', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='is invalid, the agent responds with an error, prompting the player agent to re-propose a legal move\\nbefore continuing. This process ensures that only valid moves are played and helps maintain a con-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='before continuing. This process ensures that only valid moves are played and helps maintain a con-\\nsistent gaming experience. As an ablation study, we removed the board agent and instead only relied', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='sistent gaming experience. As an ablation study, we removed the board agent and instead only relied\\non a relevant prompt ‚Äúyou should make sure both you and the opponent are making legal moves‚Äù to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='on a relevant prompt ‚Äúyou should make sure both you and the opponent are making legal moves‚Äù to\\nground their move. The results highlighted that without the board agent, illegitimate moves caused', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='ground their move. The results highlighted that without the board agent, illegitimate moves caused\\ngame disruptions. The modular design offered flexibility, allowing swift adjustments to the board', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='game disruptions. The modular design offered flexibility, allowing swift adjustments to the board\\nagent in response to evolving game rules or varying chess rule variants. A comprehensive demon-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='agent in response to evolving game rules or varying chess rule variants. A comprehensive demon-\\nstration of this ablation study is in Appendix D.\\n4 Discussion', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='stration of this ablation study is in Appendix D.\\n4 Discussion\\nWe introduced an open-source library, AutoGen , that incorporates the paradigms of conversable', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='We introduced an open-source library, AutoGen , that incorporates the paradigms of conversable\\nagents and conversation programming. This library utilizes capable agents that are well-suited for', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='agents and conversation programming. This library utilizes capable agents that are well-suited for\\nmulti-agent cooperation. It features a unified conversation interface among the agents, along with', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='an auto-reply mechanisms, which help establish an agent-interaction interface that capitalizes on the\\nstrengths of chat-optimized LLMs with broad capabilities while accommodating a wide range of', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='strengths of chat-optimized LLMs with broad capabilities while accommodating a wide range of\\napplications. AutoGen serves as a general framework for creating and experimenting with multi-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='applications. AutoGen serves as a general framework for creating and experimenting with multi-\\nagent systems that can easily fulfill various practical requirements, such as reusing, customizing,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='agent systems that can easily fulfill various practical requirements, such as reusing, customizing,\\nand extending existing agents, as well as programming conversations between them.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='and extending existing agents, as well as programming conversations between them.\\nOur experiments, as detailed in Section 3, demonstrate that this approach offers numerous benefits.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='Our experiments, as detailed in Section 3, demonstrate that this approach offers numerous benefits.\\nThe adoption of AutoGen has resulted in improved performance (over state-of-the-art approaches),', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='The adoption of AutoGen has resulted in improved performance (over state-of-the-art approaches),\\nreduced development code, and decreased manual burden for existing applications. It offers flex-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='reduced development code, and decreased manual burden for existing applications. It offers flex-\\nibility to developers, as demonstrated in A1 (scenario 3), A5, and A6, where AutoGen enables', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='ibility to developers, as demonstrated in A1 (scenario 3), A5, and A6, where AutoGen enables\\nmulti-agent chats to follow a dynamic pattern rather than fixed back-and-forth interactions. It allows', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='humans to engage in activities alongside multiple AI agents in a conversational manner. Despite the\\ncomplexity of these applications (most involving more than two agents or dynamic multi-turn agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='complexity of these applications (most involving more than two agents or dynamic multi-turn agent\\ncooperation), the implementation based on AutoGen remains straightforward. Dividing tasks among', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='cooperation), the implementation based on AutoGen remains straightforward. Dividing tasks among\\nseparate agents promotes modularity. Furthermore, since each agent can be developed, tested, and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='separate agents promotes modularity. Furthermore, since each agent can be developed, tested, and\\nmaintained separately, this approach simplifies overall development and code management.\\n9', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='Although this work is still in its early experimental stages, it paves the way for numerous future\\ndirections and research opportunities. For instance, we can explore effective integration of existing', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='agent implementations into our multi-agent framework and investigate the optimal balance between\\nautomation and human control in multi-agent workflows. As we further develop and refine AutoGen ,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='automation and human control in multi-agent workflows. As we further develop and refine AutoGen ,\\nwe aim to investigate which strategies, such as agent topology and conversation patterns, lead to the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='most effective multi-agent conversations while optimizing the overall efficiency, among other fac-\\ntors. While increasing the number of agents and other degrees of freedom presents opportunities for', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='tors. While increasing the number of agents and other degrees of freedom presents opportunities for\\ntackling more complex problems, it may also introduce new safety challenges that require additional', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='tackling more complex problems, it may also introduce new safety challenges that require additional\\nstudies and careful consideration.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='studies and careful consideration.\\nWe provide more discussion in Appendix B, including guidelines for using AutoGen and direction', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='We provide more discussion in Appendix B, including guidelines for using AutoGen and direction\\nof future work. We hope AutoGen will help improve many LLM applications in terms of speed of', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='of future work. We hope AutoGen will help improve many LLM applications in terms of speed of\\ndevelopment, ease of experimentation, and overall effectiveness and safety. We actively welcome', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='development, ease of experimentation, and overall effectiveness and safety. We actively welcome\\ncontributions from the broader community.\\nEthics statement', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='contributions from the broader community.\\nEthics statement\\nThere are several potential ethical considerations that could arise from the development and use of\\ntheAutoGen framework.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='theAutoGen framework.\\n‚Ä¢ Privacy and Data Protection: The framework allows for human participation in conversations', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='‚Ä¢ Privacy and Data Protection: The framework allows for human participation in conversations\\nbetween agents. It is important to ensure that user data and conversations are protected, and that', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='between agents. It is important to ensure that user data and conversations are protected, and that\\ndevelopers use appropriate measures to safeguard privacy.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='developers use appropriate measures to safeguard privacy.\\n‚Ä¢ Bias and Fairness: LLMs have been shown to exhibit biases present in their training data (Navigli', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='‚Ä¢ Bias and Fairness: LLMs have been shown to exhibit biases present in their training data (Navigli\\net al., 2023). When using LLMs in the AutoGen framework, it is crucial to address and mitigate', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='et al., 2023). When using LLMs in the AutoGen framework, it is crucial to address and mitigate\\nany biases that may arise in the conversations between agents. Developers should be aware of', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='any biases that may arise in the conversations between agents. Developers should be aware of\\npotential biases and take steps to ensure fairness and inclusivity.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='potential biases and take steps to ensure fairness and inclusivity.\\n‚Ä¢ Accountability and Transparency: As discussed in the future work section, as the framework in-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='‚Ä¢ Accountability and Transparency: As discussed in the future work section, as the framework in-\\nvolves multiple agents conversing and cooperating, it is important to establish clear accountability', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='and transparency mechanisms. Users should be able to understand and trace the decision-making\\nprocess of the agents involved in order to ensure accountability and address any potential issues', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='process of the agents involved in order to ensure accountability and address any potential issues\\nor biases.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='or biases.\\n‚Ä¢ Trust and Reliance: AutoGen leverages human understanding and intelligence while providing\\nautomation through conversations between agents. It is important to consider the impact of this', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='automation through conversations between agents. It is important to consider the impact of this\\ninteraction on user experience, trust, and reliance on AI systems. Clear communication and user', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='interaction on user experience, trust, and reliance on AI systems. Clear communication and user\\neducation about the capabilities and limitations of the system will be essential (Cai et al., 2019).', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='‚Ä¢ Unintended Consequences: As discussed before, the use of multi-agent conversations and automa-\\ntion in complex tasks may have unintended consequences. In particular, allowing LLM agents to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='tion in complex tasks may have unintended consequences. In particular, allowing LLM agents to\\nmake changes in external environments through code execution or function calls, such as installing', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='make changes in external environments through code execution or function calls, such as installing\\npackages, could be risky. Developers should carefully consider the potential risks and ensure that', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='packages, could be risky. Developers should carefully consider the potential risks and ensure that\\nappropriate safeguards are in place to prevent harm or negative outcomes.\\nAcknowledgements', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='appropriate safeguards are in place to prevent harm or negative outcomes.\\nAcknowledgements\\nThe work presented in this report was made possible through discussions and feedback from Peter', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='The work presented in this report was made possible through discussions and feedback from Peter\\nLee, Johannes Gehrke, Eric Horvitz, Steven Lucco, Umesh Madan, Robin Moeur, Piali Choud-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='Lee, Johannes Gehrke, Eric Horvitz, Steven Lucco, Umesh Madan, Robin Moeur, Piali Choud-\\nhury, Saleema Amershi, Adam Fourney, Victor Dibia, Guoqing Zheng, Corby Rosset, Ricky Loynd,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='hury, Saleema Amershi, Adam Fourney, Victor Dibia, Guoqing Zheng, Corby Rosset, Ricky Loynd,\\nEce Kamar, Rafah Hosn, John Langford, Ida Momennejad, Brian Krabach, Taylor Webb, Shanka', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='Ece Kamar, Rafah Hosn, John Langford, Ida Momennejad, Brian Krabach, Taylor Webb, Shanka\\nSubhra Mondal, Wei-ge Chen, Robert Gruen, Yinan Li, Yue Wang, Suman Nath, Tanakorn Leesat-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='Subhra Mondal, Wei-ge Chen, Robert Gruen, Yinan Li, Yue Wang, Suman Nath, Tanakorn Leesat-\\napornwongsa, Xin Wang, Shishir Patil, Tianjun Zhang, Saehan Jo, Ishai Menache, Kontantina Mel-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='apornwongsa, Xin Wang, Shishir Patil, Tianjun Zhang, Saehan Jo, Ishai Menache, Kontantina Mel-\\nlou, Runlong Zhou, Feiran Jia, Hamed Khanpour, Hamid Palangi, Srinagesh Sharma, Julio Albinati', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='lou, Runlong Zhou, Feiran Jia, Hamed Khanpour, Hamid Palangi, Srinagesh Sharma, Julio Albinati\\nCortez, Amin Saied, Yuzhe Ma, Dujian Ding, Linyong Nan, Prateek Yadav, Shannon Shen, Ankur', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='Cortez, Amin Saied, Yuzhe Ma, Dujian Ding, Linyong Nan, Prateek Yadav, Shannon Shen, Ankur\\nMallick, Mark Encarnaci ¬¥on, Lars Liden, Tianwei Yue, Julia Kiseleva, Anastasia Razdaibiedina, and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='Mallick, Mark Encarnaci ¬¥on, Lars Liden, Tianwei Yue, Julia Kiseleva, Anastasia Razdaibiedina, and\\nLuciano Del Corro. Qingyun Wu would like to acknowledge the funding and research support from', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='Luciano Del Corro. Qingyun Wu would like to acknowledge the funding and research support from\\nthe College of Information Science and Technology at Penn State University.\\n10', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='References\\nVaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Eval-\\nuating correctness and faithfulness of instruction-following models for question answering. arXiv', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='uating correctness and faithfulness of instruction-following models for question answering. arXiv\\npreprint arXiv:2307.16877 , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='preprint arXiv:2307.16877 , 2023.\\nSaleema Amershi, Dan Weld, Mihaela V orvoreanu, Adam Fourney, Besmira Nushi, Penny Col-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='Saleema Amershi, Dan Weld, Mihaela V orvoreanu, Adam Fourney, Besmira Nushi, Penny Col-\\nlisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. Guidelines for human-ai', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='lisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. Guidelines for human-ai\\ninteraction. In Proceedings of the 2019 chi conference on human factors in computing systems ,\\n2019.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='2019.\\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man ¬¥e. Con-\\ncrete problems in ai safety, 2016.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='crete problems in ai safety, 2016.\\nAutoGPT. Documentation ‚Äî auto-gpt. https://docs.agpt.co/ , 2023.\\nBabyAGI. Github ‚Äî babyagi. https://github.com/yoheinakajima/babyagi , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='BabyAGI. Github ‚Äî babyagi. https://github.com/yoheinakajima/babyagi , 2023.\\nCarrie J. Cai, Samantha Winter, David F. Steiner, Lauren Wilcox, and Michael Terry. ‚Äùhello ai‚Äù:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='Carrie J. Cai, Samantha Winter, David F. Steiner, Lauren Wilcox, and Michael Terry. ‚Äùhello ai‚Äù:\\nUncovering the onboarding needs of medical practitioners for human-ai collaborative decision-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='Uncovering the onboarding needs of medical practitioners for human-ai collaborative decision-\\nmaking. Proceedings of the ACM on Human-Computer Interaction , 2019.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='making. Proceedings of the ACM on Human-Computer Interaction , 2019.\\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as\\ntool makers. arXiv preprint arXiv:2305.17126 , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='tool makers. arXiv preprint arXiv:2305.17126 , 2023.\\nChroma. Chromadb. https://github.com/chroma-core/chroma , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='Chroma. Chromadb. https://github.com/chroma-core/chroma , 2023.\\nVictor Dibia. LIDA: A tool for automatic generation of grammar-agnostic visualizations and info-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='Victor Dibia. LIDA: A tool for automatic generation of grammar-agnostic visualizations and info-\\ngraphics using large language models. In Proceedings of the 61st Annual Meeting of the Associ-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='graphics using large language models. In Proceedings of the 61st Annual Meeting of the Associ-\\nation for Computational Linguistics (Volume 3: System Demonstrations) , Toronto, Canada, July', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='ation for Computational Linguistics (Volume 3: System Demonstrations) , Toronto, Canada, July\\n2023. Association for Computational Linguistics.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='2023. Association for Computational Linguistics.\\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt. arXiv\\npreprint arXiv:2304.07590 , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='preprint arXiv:2304.07590 , 2023.\\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improv-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improv-\\ning factuality and reasoning in language models through multiagent debate. arXiv preprint', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='ing factuality and reasoning in language models through multiagent debate. arXiv preprint\\narXiv:2305.14325 , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='arXiv:2305.14325 , 2023.\\nAtty Eleti, Jeff Harris, and Logan Kilpatrick. Function calling and other api updates. https:\\n//openai.com/blog/function-calling-and-other-api-updates , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='//openai.com/blog/function-calling-and-other-api-updates , 2023.\\nGuidance. Guidance. https://github.com/guidance-ai/guidance , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='Guidance. Guidance. https://github.com/guidance-ai/guidance , 2023.\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv\\npreprint arXiv:2103.03874 , 2021.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='preprint arXiv:2103.03874 , 2021.\\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven\\nKa Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for\\nmulti-agent collaborative framework. arXiv preprint arXiv:2308.00352 , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='multi-agent collaborative framework. arXiv preprint arXiv:2308.00352 , 2023.\\nEric Horvitz. Principles of mixed-initiative user interfaces. In Proceedings of the SIGCHI conference', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='on Human Factors in Computing Systems , 1999.\\nHuggingFace. Transformers agent. https://huggingface.co/docs/transformers/\\ntransformers_agents , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='transformers_agents , 2023.\\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.\\narXiv preprint arXiv:2303.17491 , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='arXiv preprint arXiv:2303.17491 , 2023.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\\nbenchmark for question answering research. Transactions of the Association for Computational', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='benchmark for question answering research. Transactions of the Association for Computational\\nLinguistics , 2019.\\n11', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='LangChain. Introduction ‚Äî langchain. https://python.langchain.com/en/latest/index.\\nhtml , 2023.\\nMike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal? end-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Mike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal? end-\\nto-end learning for negotiation dialogues. arXiv preprint arXiv:1706.05125 , 2017.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='to-end learning for negotiation dialogues. arXiv preprint arXiv:1706.05125 , 2017.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich K ¬®uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¬®aschel, et al. Retrieval-augmented gen-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Heinrich K ¬®uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¬®aschel, et al. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems ,\\n2020.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='2020.\\nBeibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, and Ishai Menache. Large language\\nmodels for supply chain optimization. arXiv preprint arXiv:2307.03875 , 2023a.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='models for supply chain optimization. arXiv preprint arXiv:2307.03875 , 2023a.\\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\\nCamel: Communicative agents for ‚Äùmind‚Äù exploration of large scale language model society,\\n2023b.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Camel: Communicative agents for ‚Äùmind‚Äù exploration of large scale language model society,\\n2023b.\\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='2023b.\\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng\\nTu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-\\nagent debate, 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='agent debate, 2023.\\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement\\nlearning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802 ,\\n2018.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='2018.\\nJerry Liu. LlamaIndex, November 2022. URL https://github.com/jerryjliu/llama_index .\\nV olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-\\nstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\\narXiv:1312.5602 , 2013.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='arXiv:1312.5602 , 2013.\\nRoberto Navigli, Simone Conia, and Bj ¬®orn Ross. Biases in large language models: Origins, inven-\\ntory and discussion. ACM Journal of Data and Information Quality , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='tory and discussion. ACM Journal of Data and Information Quality , 2023.\\nOpenAI. ChatGPT plugins. https://openai.com/blog/chatgpt-plugins , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='OpenAI. ChatGPT plugins. https://openai.com/blog/chatgpt-plugins , 2023.\\nJoon Sung Park, Joseph C O‚ÄôBrien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Joon Sung Park, Joseph C O‚ÄôBrien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and\\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint\\narXiv:2304.03442 , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='arXiv:2304.03442 , 2023.\\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Md Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.\\nRetrieval augmented code generation and summarization. arXiv preprint arXiv:2108.11601 ,\\n2021.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Retrieval augmented code generation and summarization. arXiv preprint arXiv:2108.11601 ,\\n2021.\\nShishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model\\nconnected with massive apis. arXiv preprint arXiv:2305.15334 , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='connected with massive apis. arXiv preprint arXiv:2305.15334 , 2023.\\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-\\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\\nProcessing . Association for Computational Linguistics, 11 2019. URL https://arxiv.org/\\nabs/1908.10084 .', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='abs/1908.10084 .\\nSemantic-Kernel. Semantic kernel. https://github.com/microsoft/semantic-kernel ,\\n2023.\\nBokui Shen, Fei Xia, Chengshu Li, Roberto Mart ¬¥ƒ±n-Mart ¬¥ƒ±n, Linxi Fan, Guanzhi Wang, Claudia', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='2023.\\nBokui Shen, Fei Xia, Chengshu Li, Roberto Mart ¬¥ƒ±n-Mart ¬¥ƒ±n, Linxi Fan, Guanzhi Wang, Claudia\\nP¬¥erez-D‚ÄôArpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, et al. igibson 1.0: A simu-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='P¬¥erez-D‚ÄôArpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, et al. igibson 1.0: A simu-\\nlation environment for interactive tasks in large realistic scenes. In 2021 IEEE/RSJ International', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='lation environment for interactive tasks in large realistic scenes. In 2021 IEEE/RSJ International\\nConference on Intelligent Robots and Systems (IROS) . IEEE, 2021.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Conference on Intelligent Robots and Systems (IROS) . IEEE, 2021.\\nTianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An\\nopen-domain platform for web-based agents. In International Conference on Machine Learning .', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='open-domain platform for web-based agents. In International Conference on Machine Learning .\\nPMLR, 2017.\\n12', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C ÀÜot¬¥e, Yonatan Bisk, Adam Trischler, and Matthew\\nHausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='Hausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In\\nProceedings of the International Conference on Learning Representations (ICLR) , 2021. URL', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='Proceedings of the International Conference on Learning Representations (ICLR) , 2021. URL\\nhttps://arxiv.org/abs/2010.03768 .', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='https://arxiv.org/abs/2010.03768 .\\nOriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,\\nMichelle Yeo, Alireza Makhzani, Heinrich K ¬®uttler, John Agapiou, Julian Schrittwieser, et al.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='Michelle Yeo, Alireza Makhzani, Heinrich K ¬®uttler, John Agapiou, Julian Schrittwieser, et al.\\nStarcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782 , 2017.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782 , 2017.\\nChi Wang, Qingyun Wu, Markus Weimer, and Erkang Zhu. Flaml: A fast and lightweight automl', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='Chi Wang, Qingyun Wu, Markus Weimer, and Erkang Zhu. Flaml: A fast and lightweight automl\\nlibrary. Proceedings of Machine Learning and Systems , 2021.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='library. Proceedings of Machine Learning and Systems , 2021.\\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\\nand Anima Anandkumar. V oyager: An open-ended embodied agent with large language models.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='and Anima Anandkumar. V oyager: An open-ended embodied agent with large language models.\\narXiv preprint arXiv:2305.16291 , 2023a.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='arXiv preprint arXiv:2305.16291 , 2023a.\\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\\narXiv preprint arXiv:2308.11432 , 2023b.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='arXiv preprint arXiv:2308.11432 , 2023b.\\nDaniel S. Weld and Oren Etzioni. The first law of robotics (a call to arms). In AAAI Conference on\\nArtificial Intelligence , 1994.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='Artificial Intelligence , 1994.\\nMax Woolf. Langchain problem. https://minimaxir.com/2023/07/langchain-problem/ ,\\n2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='Max Woolf. Langchain problem. https://minimaxir.com/2023/07/langchain-problem/ ,\\n2023.\\nYiran Wu, Feiran Jia, Shaokun Zhang, Qingyun Wu, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='2023.\\nYiran Wu, Feiran Jia, Shaokun Zhang, Qingyun Wu, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat\\nLee, Richard Peng, and Chi Wang. An empirical study on challenging math problem solving with', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='Lee, Richard Peng, and Chi Wang. An empirical study on challenging math problem solving with\\ngpt-4. arXiv preprint arXiv:2306.01337 , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='gpt-4. arXiv preprint arXiv:2306.01337 , 2023.\\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe\\nWang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents:\\nA survey. arXiv preprint arXiv:2309.07864 , 2023.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='A survey. arXiv preprint arXiv:2309.07864 , 2023.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 ,\\n2022.\\n13', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='A Related Work\\nWe examine existing LLM-based agent systems or frameworks that can be used to build LLM appli-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='We examine existing LLM-based agent systems or frameworks that can be used to build LLM appli-\\ncations. We categorize the related work into single-agent and multi-agent systems and specifically', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='cations. We categorize the related work into single-agent and multi-agent systems and specifically\\nprovide a summary of differentiators comparing AutoGen with existing multi-agent systems in Ta-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='provide a summary of differentiators comparing AutoGen with existing multi-agent systems in Ta-\\nble 1. Note that many of these systems are evolving open-source projects, so the remarks and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='ble 1. Note that many of these systems are evolving open-source projects, so the remarks and\\nstatements about them may only be accurate as of the time of writing. We refer interested readers to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='detailed LLM-based agent surveys (Xi et al., 2023; Wang et al., 2023b)\\nSingle-Agent Systems:\\n‚Ä¢AutoGPT : AutoGPT is an open-source implementation of an AI agent that attempts to au-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='‚Ä¢AutoGPT : AutoGPT is an open-source implementation of an AI agent that attempts to au-\\ntonomously achieve a given goal (AutoGPT, 2023). It follows a single-agent paradigm in which', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='tonomously achieve a given goal (AutoGPT, 2023). It follows a single-agent paradigm in which\\nit augments the AI model with many useful tools, and does not support multi-agent collaboration.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='it augments the AI model with many useful tools, and does not support multi-agent collaboration.\\n‚Ä¢ChatGPT+ (with code interpreter or plugin) : ChatGPT, a conversational AI service or agent,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='‚Ä¢ChatGPT+ (with code interpreter or plugin) : ChatGPT, a conversational AI service or agent,\\ncan now be used alongside a code interpreter or plugin (currently available only under the pre-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='can now be used alongside a code interpreter or plugin (currently available only under the pre-\\nmium subscription plan ChatGPT Plus) (OpenAI, 2023). The code interpreter enables ChatGPT', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='mium subscription plan ChatGPT Plus) (OpenAI, 2023). The code interpreter enables ChatGPT\\nto execute code, while the plugin enhances ChatGPT with a wide range of curated tools.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='to execute code, while the plugin enhances ChatGPT with a wide range of curated tools.\\n‚Ä¢LangChain Agents : LangChain is a general framework for developing LLM-based applica-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='‚Ä¢LangChain Agents : LangChain is a general framework for developing LLM-based applica-\\ntions (LangChain, 2023). LangChain Agents is a subpackage for using an LLM to choose a', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='tions (LangChain, 2023). LangChain Agents is a subpackage for using an LLM to choose a\\nsequence of actions. There are various types of agents in LangChain Agents, with the ReAct agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='sequence of actions. There are various types of agents in LangChain Agents, with the ReAct agent\\nbeing a notable example that combines reasoning and acting when using LLMs (mainly designed', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='being a notable example that combines reasoning and acting when using LLMs (mainly designed\\nfor LLMs prior to ChatGPT) (Yao et al., 2022). All agents provided in LangChain Agents fol-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='for LLMs prior to ChatGPT) (Yao et al., 2022). All agents provided in LangChain Agents fol-\\nlow a single-agent paradigm and are not inherently designed for communicative and collaborative', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='low a single-agent paradigm and are not inherently designed for communicative and collaborative\\nmodes. A significant summary of its limitations can be found in (Woolf, 2023). Due to these lim-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='modes. A significant summary of its limitations can be found in (Woolf, 2023). Due to these lim-\\nitations, even the multi-agent systems in LangChain (e.g., re-implementation of CAMEL) are not', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='itations, even the multi-agent systems in LangChain (e.g., re-implementation of CAMEL) are not\\nbased on LangChain Agents but are implemented from scratch. Their connection to LangChain', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='based on LangChain Agents but are implemented from scratch. Their connection to LangChain\\nlies in the use of basic orchestration modules provided by LangChain, such as AI models wrapped', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='lies in the use of basic orchestration modules provided by LangChain, such as AI models wrapped\\nby LangChain and the corresponding interface.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='by LangChain and the corresponding interface.\\n‚Ä¢Transformers Agent : Transformers Agent (HuggingFace, 2023) is an experimental natural-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='‚Ä¢Transformers Agent : Transformers Agent (HuggingFace, 2023) is an experimental natural-\\nlanguage API built on the transformers repository. It includes a set of curated tools and an agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='language API built on the transformers repository. It includes a set of curated tools and an agent\\nto interpret natural language and use these tools. Similar to AutoGPT, it follows a single-agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='to interpret natural language and use these tools. Similar to AutoGPT, it follows a single-agent\\nparadigm and does not support agent collaboration.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='paradigm and does not support agent collaboration.\\nAutoGen differs from the single-agent systems above by supporting multi-agent LLM applications.\\nMulti-Agent Systems:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='Multi-Agent Systems:\\n‚Ä¢BabyAGI : BabyAGI (BabyAGI, 2023) is an example implementation of an AI-powered task man-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='‚Ä¢BabyAGI : BabyAGI (BabyAGI, 2023) is an example implementation of an AI-powered task man-\\nagement system in a Python script. In this implemented system, multiple LLM-based agents', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='agement system in a Python script. In this implemented system, multiple LLM-based agents\\nare used. For example, there is an agent for creating new tasks based on the objective and the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='are used. For example, there is an agent for creating new tasks based on the objective and the\\nresult of the previous task, an agent for prioritizing the task list, and an agent for completing', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='result of the previous task, an agent for prioritizing the task list, and an agent for completing\\ntasks/sub-tasks. As a multi-agent system, BabyAGI adopts a static agent conversation pattern,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='tasks/sub-tasks. As a multi-agent system, BabyAGI adopts a static agent conversation pattern,\\ni.e., a predefined order of agent communication, while AutoGen supports both static and dynamic', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='i.e., a predefined order of agent communication, while AutoGen supports both static and dynamic\\nconversation patterns and additionally supports tool usage and human involvement.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='conversation patterns and additionally supports tool usage and human involvement.\\n‚Ä¢CAMEL : CAMEL (Li et al., 2023b) is a communicative agent framework. It demonstrates', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='‚Ä¢CAMEL : CAMEL (Li et al., 2023b) is a communicative agent framework. It demonstrates\\nhow role playing can be used to let chat agents communicate with each other for task comple-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='how role playing can be used to let chat agents communicate with each other for task comple-\\ntion. It also records agent conversations for behavior analysis and capability understanding. An', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='tion. It also records agent conversations for behavior analysis and capability understanding. An\\nInception-prompting technique is used to achieve autonomous cooperation between agents. Un-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='Inception-prompting technique is used to achieve autonomous cooperation between agents. Un-\\nlikeAutoGen , CAMEL does not natively support tool usage, such as code execution. Although it', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='likeAutoGen , CAMEL does not natively support tool usage, such as code execution. Although it\\nis proposed as an infrastructure for multi-agent conversation, it only supports static conversation', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='is proposed as an infrastructure for multi-agent conversation, it only supports static conversation\\npatterns, while AutoGen additionally supports dynamic conversation patterns.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='patterns, while AutoGen additionally supports dynamic conversation patterns.\\n‚Ä¢Multi-Agent Debate: Two recent works investigate and show that multi-agent debate is an effec-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='‚Ä¢Multi-Agent Debate: Two recent works investigate and show that multi-agent debate is an effec-\\ntive way to encourage divergent thinking in LLMs (Liang et al., 2023) and to improve the factuality', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='tive way to encourage divergent thinking in LLMs (Liang et al., 2023) and to improve the factuality\\nand reasoning of LLMs (Du et al., 2023). In both works, multiple LLM inference instances are', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='and reasoning of LLMs (Du et al., 2023). In both works, multiple LLM inference instances are\\nconstructed as multiple agents to solve problems with agent debate. Each agent is simply an LLM', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='constructed as multiple agents to solve problems with agent debate. Each agent is simply an LLM\\ninference instance, while no tool or human is involved, and the inter-agent conversation needs', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='inference instance, while no tool or human is involved, and the inter-agent conversation needs\\nto follow a pre-defined order. These works attempt to build LLM applications with multi-agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='to follow a pre-defined order. These works attempt to build LLM applications with multi-agent\\nconversation, while AutoGen , designed as a generic infrastructure, can be used to facilitate this', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='conversation, while AutoGen , designed as a generic infrastructure, can be used to facilitate this\\ndevelopment and enable more applications with dynamic conversation patterns.\\n14', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='‚Ä¢MetaGPT : MetaGPT (Hong et al., 2023) is a specialized LLM application based on a multi-agent\\nconversation framework for automatic software development. They assign different roles to GPTs', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='conversation framework for automatic software development. They assign different roles to GPTs\\nto collaboratively develop software. They differ from AutoGen by being specialized solutions to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='to collaboratively develop software. They differ from AutoGen by being specialized solutions to\\na certain scenario, while AutoGen is a generic infrastructure to facilitate building applications for', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='various scenarios.\\nThere are a few other specialized single-agent or multi-agent systems, such as V oyager (Wang et al.,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='2023a) and Generative Agents (Park et al., 2023), which we skip due to lower relevance. In Table 1,\\nwe summarize differences between AutoGen and the most relevant multi-agent systems.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='we summarize differences between AutoGen and the most relevant multi-agent systems.\\nTable 1: Summary of differences between AutoGen and other related multi-agent systems. infras-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='Table 1: Summary of differences between AutoGen and other related multi-agent systems. infras-\\ntructure : whether the system is designed as a generic infrastructure for building LLM applications.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='conversation pattern : the types of patterns supported by the implemented systems. Under the\\n‚Äòstatic‚Äô pattern, agent topology remains unchanged regardless of different inputs. AutoGen allows', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='‚Äòstatic‚Äô pattern, agent topology remains unchanged regardless of different inputs. AutoGen allows\\nflexible conversation patterns, including both static and dynamic patterns that can be customized', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='flexible conversation patterns, including both static and dynamic patterns that can be customized\\nbased on different application needs. execution-capable : whether the system can execute LLM-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='based on different application needs. execution-capable : whether the system can execute LLM-\\ngenerated code; human involvement : whether (and how) the system allows human participation', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='generated code; human involvement : whether (and how) the system allows human participation\\nduring the execution process of the system. AutoGen allows flexible human involvement in multi-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='during the execution process of the system. AutoGen allows flexible human involvement in multi-\\nagent conversation with the option for humans to skip providing inputs.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='agent conversation with the option for humans to skip providing inputs.\\nAspect AutoGen Multi-agent Debate CAMEL BabyAGI MetaGPT\\nInfrastructure ‚úì ‚úó ‚úì ‚úó ‚úó', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='Aspect AutoGen Multi-agent Debate CAMEL BabyAGI MetaGPT\\nInfrastructure ‚úì ‚úó ‚úì ‚úó ‚úó\\nConversation pattern flexible static static static static\\nExecution-capable ‚úì ‚úó ‚úó ‚úó ‚úì', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='Conversation pattern flexible static static static static\\nExecution-capable ‚úì ‚úó ‚úó ‚úó ‚úì\\nHuman involvement chat/skip ‚úó ‚úó ‚úó ‚úó\\n15', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='B Expanded Discussion\\nThe applications in Section 3 show how AutoGen not only enables new applications but also helps', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='The applications in Section 3 show how AutoGen not only enables new applications but also helps\\nrenovate existing ones. For example, in A1 (scenario 3), A5, and A6, AutoGen enabled the cre-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='renovate existing ones. For example, in A1 (scenario 3), A5, and A6, AutoGen enabled the cre-\\nation of multi-agent conversations that follow a dynamic pattern instead of a fixed back-and-forth.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='ation of multi-agent conversations that follow a dynamic pattern instead of a fixed back-and-forth.\\nAnd in both A5 and A6, humans can participate in the activities together with multiple other AI', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='And in both A5 and A6, humans can participate in the activities together with multiple other AI\\nagents in a conversational manner. Similarly, A1-A4 show how popular applications can be reno-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='agents in a conversational manner. Similarly, A1-A4 show how popular applications can be reno-\\nvated quickly with AutoGen . Despite the complexity of these applications (most of them involve', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='vated quickly with AutoGen . Despite the complexity of these applications (most of them involve\\nmore than two agents or dynamic multi-turn agent cooperation), our AutoGen -based implementa-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='more than two agents or dynamic multi-turn agent cooperation), our AutoGen -based implementa-\\ntion remains simple, demonstrating promising opportunities to build creative applications and a large', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='space for innovation. In reflecting on why these benefits can be achieved in these applications with\\nAutoGen , we believe there are a few reasons:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='AutoGen , we believe there are a few reasons:\\n‚Ä¢Ease of use : The built-in agents can be used out-of-the-box, delivering strong performance even\\nwithout any customization. (A1, A3)', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='without any customization. (A1, A3)\\n‚Ä¢Modularity : The division of tasks into separate agents promotes modularity in the system. Each', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='‚Ä¢Modularity : The division of tasks into separate agents promotes modularity in the system. Each\\nagent can be developed, tested, and maintained independently, simplifying the overall develop-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='agent can be developed, tested, and maintained independently, simplifying the overall develop-\\nment process and facilitating code management. (A3, A4, A5, and A6)', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='ment process and facilitating code management. (A3, A4, A5, and A6)\\n‚Ä¢Programmability: AutoGen allows users to extend/customize existing agents to develop systems', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='‚Ä¢Programmability: AutoGen allows users to extend/customize existing agents to develop systems\\nsatisfying their specific needs with ease. (A1-A6). For example, with AutoGen , the core workflow', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='satisfying their specific needs with ease. (A1-A6). For example, with AutoGen , the core workflow\\ncode in A4 is reduced from over 430 lines to 100 lines, for a 4x saving.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='code in A4 is reduced from over 430 lines to 100 lines, for a 4x saving.\\n‚Ä¢Allowing human involvement :AutoGen provides a native mechanism to achieve human partici-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='‚Ä¢Allowing human involvement :AutoGen provides a native mechanism to achieve human partici-\\npation and/or human oversight. With AutoGen , humans can seamlessly and optionally cooperate', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='pation and/or human oversight. With AutoGen , humans can seamlessly and optionally cooperate\\nwith AIs to solve problems or generally participate in the activity. AutoGen also facilitates inter-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='active user instructions to ensure the process stays on the desired path. (A1, A2, A5, and A6)\\n‚Ä¢Collaborative/adversarial agent interactions : Like many collaborative agent systems (Dong', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='‚Ä¢Collaborative/adversarial agent interactions : Like many collaborative agent systems (Dong\\net al., 2023), agents in AutoGen can share information and knowledge, to complement each other‚Äôs', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='et al., 2023), agents in AutoGen can share information and knowledge, to complement each other‚Äôs\\nabilities and collectively arrive at better solutions. (A1, A2, A3, and A4). Analogously, in certain', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='scenarios, some agents are required to work in an adversarial way. Relevant information is shared\\namong different conversations in a controlled manner, preventing distraction or hallucination. (A4,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='among different conversations in a controlled manner, preventing distraction or hallucination. (A4,\\nA6). AutoGen supports both patterns, enabling effective utilization and augmentation of LLMs.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='A6). AutoGen supports both patterns, enabling effective utilization and augmentation of LLMs.\\nB.1 General Guidelines for Using AutoGen', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='B.1 General Guidelines for Using AutoGen\\nBelow we give some recommendations for using agents in AutoGen to accomplish a task.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='Below we give some recommendations for using agents in AutoGen to accomplish a task.\\n1.Consider using built-in agents first. For example, AssistantAgent is pre-configured to be', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='1.Consider using built-in agents first. For example, AssistantAgent is pre-configured to be\\nbacked by GPT-4, with a carefully designed system message for generic problem-solving via', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='backed by GPT-4, with a carefully designed system message for generic problem-solving via\\ncode. The UserProxyAgent is configured to solicit human inputs and perform tool execution.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='code. The UserProxyAgent is configured to solicit human inputs and perform tool execution.\\nMany problems can be solved by simply combining these two agents. When customizing agents', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='Many problems can be solved by simply combining these two agents. When customizing agents\\nfor an application, consider the following options: (1) human input mode, termination condition,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='for an application, consider the following options: (1) human input mode, termination condition,\\ncode execution configuration, and LLM configuration can be specified when constructing an', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='code execution configuration, and LLM configuration can be specified when constructing an\\nagent; (2) AutoGen supports adding instructions in an initial user message, which is an effective', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='agent; (2) AutoGen supports adding instructions in an initial user message, which is an effective\\nway to boost performance without needing to modify the system message; (3) UserProxyAgent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='way to boost performance without needing to modify the system message; (3) UserProxyAgent\\ncan be extended to handle different execution environments and exceptions, etc.; (4) when sys-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='can be extended to handle different execution environments and exceptions, etc.; (4) when sys-\\ntem message modification is needed, consider leveraging the LLM‚Äôs capability to program its', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='tem message modification is needed, consider leveraging the LLM‚Äôs capability to program its\\nconversation flow with natural language.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='conversation flow with natural language.\\n2.Start with a simple conversation topology . Consider using the two-agent chat or the group chat', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='2.Start with a simple conversation topology . Consider using the two-agent chat or the group chat\\nsetup first, as they can often be extended with the least code. Note that the two-agent chat can', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='setup first, as they can often be extended with the least code. Note that the two-agent chat can\\nbe easily extended to involve more than two agents by using LLM-consumable functions in a\\ndynamic way.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='dynamic way.\\n3. Try to reuse built-in reply methods based on LLM, tool, or human before implementing a\\ncustom reply method because they can often be reused to achieve the goal in a simple way', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='custom reply method because they can often be reused to achieve the goal in a simple way\\n(e.g., the built-in agent GroupChatManager ‚Äôs reply method reuses the built-in LLM-based reply', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='(e.g., the built-in agent GroupChatManager ‚Äôs reply method reuses the built-in LLM-based reply\\nfunction when selecting the next speaker, ref. A5 in Section 3).', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='function when selecting the next speaker, ref. A5 in Section 3).\\n4. When developing a new application with UserProxyAgent ,start with humans always in', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='4. When developing a new application with UserProxyAgent ,start with humans always in\\nthe loop , i.e., human input mode=‚ÄòALWAYS‚Äô, even if the target operation mode is more au-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='the loop , i.e., human input mode=‚ÄòALWAYS‚Äô, even if the target operation mode is more au-\\ntonomous. This helps evaluate the effectiveness of AssistantAgent , tuning the prompt, dis-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='tonomous. This helps evaluate the effectiveness of AssistantAgent , tuning the prompt, dis-\\ncovering corner cases, and debugging. Once confident with small-scale success, consider setting\\n16', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='human input mode = ‚ÄòNEVER‚Äô. This enables LLM as a backend, and one can either use the\\nLLM or manually generate diverse system messages to simulate different use cases.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='LLM or manually generate diverse system messages to simulate different use cases.\\n5. Despite the numerous advantages of AutoGen agents, there could be cases/scenarios where other', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='5. Despite the numerous advantages of AutoGen agents, there could be cases/scenarios where other\\nlibraries/packages could help . For example: (1) For (sub)tasks that do not have requirements', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='libraries/packages could help . For example: (1) For (sub)tasks that do not have requirements\\nfor back-and-forth trouble-shooting, multi-agent interaction, etc., a unidirectional (no back-and-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='for back-and-forth trouble-shooting, multi-agent interaction, etc., a unidirectional (no back-and-\\nforth message exchange) pipeline can also be orchestrated with LangChain (LangChain, 2023),', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='forth message exchange) pipeline can also be orchestrated with LangChain (LangChain, 2023),\\nLlamaIndex (Liu, 2022), Guidance (Guidance, 2023), Semantic Kernel (Semantic-Kernel, 2023),', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='LlamaIndex (Liu, 2022), Guidance (Guidance, 2023), Semantic Kernel (Semantic-Kernel, 2023),\\nGorilla (Patil et al., 2023) or low-level inference API (‚Äòautogen.oai‚Äô provides an enhanced LLM', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='Gorilla (Patil et al., 2023) or low-level inference API (‚Äòautogen.oai‚Äô provides an enhanced LLM\\ninference layer at this level) (Dibia, 2023). (2) When existing tools from LangChain etc. are', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='inference layer at this level) (Dibia, 2023). (2) When existing tools from LangChain etc. are\\nhelpful, one can use them as tool backends for AutoGen agents. For example, one can readily use', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='helpful, one can use them as tool backends for AutoGen agents. For example, one can readily use\\ntools, e.g., Wolfram Alpha, from LangChain in AutoGen agent. (3) For specific applications, one', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='tools, e.g., Wolfram Alpha, from LangChain in AutoGen agent. (3) For specific applications, one\\nmay want to leverage agents implemented in other libraries/packages. To achieve this, one could', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='may want to leverage agents implemented in other libraries/packages. To achieve this, one could\\nwrap those agents as conversable agents in AutoGen and then use them to build LLM applications', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='wrap those agents as conversable agents in AutoGen and then use them to build LLM applications\\nthrough multi-agent conversation. (4) It can be hard to find an optimal operating point among', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='through multi-agent conversation. (4) It can be hard to find an optimal operating point among\\nmany tunable choices, such as the LLM inference configuration. Blackbox optimization packages', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='many tunable choices, such as the LLM inference configuration. Blackbox optimization packages\\nlike ‚Äòflaml.tune‚Äô (Wang et al., 2021) can be used together with AutoGen to automate such tuning.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='like ‚Äòflaml.tune‚Äô (Wang et al., 2021) can be used together with AutoGen to automate such tuning.\\nB.2 Future Work\\nThis work raises many research questions and future directions and .', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='B.2 Future Work\\nThis work raises many research questions and future directions and .\\nDesigning optimal multi-agent workflows: Creating a multi-agent workflow for a given task can', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='Designing optimal multi-agent workflows: Creating a multi-agent workflow for a given task can\\ninvolve many decisions, e.g., how many agents to include, how to assign agent roles and agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='involve many decisions, e.g., how many agents to include, how to assign agent roles and agent\\ncapabilities, how the agents should interact with each other, and whether to automate a particular', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='capabilities, how the agents should interact with each other, and whether to automate a particular\\npart of the workflow. There may not exist a one-fits-all answer, and the best solution might depend', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='on the specific application. This raises important questions: For what types of tasks and applications', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='are multi-agent workflows most useful? How do multiple agents help in different applications? For\\na given task, what is the optimal (e.g., cost-effective) multi-agent workflow?', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='a given task, what is the optimal (e.g., cost-effective) multi-agent workflow?\\nCreating highly capable agents: AutoGen can enable the development of highly capable agents', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='Creating highly capable agents: AutoGen can enable the development of highly capable agents\\nthat leverage the strengths of LLMs, tools, and humans. Creating such agents is crucial to ensuring', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='that leverage the strengths of LLMs, tools, and humans. Creating such agents is crucial to ensuring\\nthat a multi-agent workflow can effectively troubleshoot and make progress on a task. For example,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='that a multi-agent workflow can effectively troubleshoot and make progress on a task. For example,\\nwe observed that CAMEL, another multi-agent LLM system, cannot effectively solve problems in', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='we observed that CAMEL, another multi-agent LLM system, cannot effectively solve problems in\\nmost cases primarily because it lacks the capability to execute tools or code. This failure shows that', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='LLMs and multi-agent conversations with simple role playing are insufficient, and highly capable\\nagents with diverse skill sets are essential. We believe that more systematic work will be required to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='develop guidelines for application-specific agents, to create a large OSS knowledge base of agents,\\nand to create agents that can discover and upgrade their skills (Cai et al., 2023).', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='and to create agents that can discover and upgrade their skills (Cai et al., 2023).\\nEnabling scale, safety, and human agency: Section 3 shows how complex multi-agent workflows', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='Enabling scale, safety, and human agency: Section 3 shows how complex multi-agent workflows\\ncan enable new applications, and future work will be needed to assess whether scaling further can', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='can enable new applications, and future work will be needed to assess whether scaling further can\\nhelp solve extremely complex tasks. However, as these workflows scale and grow more complex,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='help solve extremely complex tasks. However, as these workflows scale and grow more complex,\\nit may become difficult to log and adjust them. Thus, it will become essential to develop clear', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='it may become difficult to log and adjust them. Thus, it will become essential to develop clear\\nmechanisms and tools to track and debug their behavior. Otherwise, these techniques risk resulting', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='mechanisms and tools to track and debug their behavior. Otherwise, these techniques risk resulting\\nin incomprehensible, unintelligible chatter among agents (Lewis et al., 2017).', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='in incomprehensible, unintelligible chatter among agents (Lewis et al., 2017).\\nOur work also shows how complex, fully autonomous workflows with AutoGen can be useful, but', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='Our work also shows how complex, fully autonomous workflows with AutoGen can be useful, but\\nfully autonomous agent conversations will need to be used with care. While the autonomous mode', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='fully autonomous agent conversations will need to be used with care. While the autonomous mode\\nAutoGen supports could be desirable in many scenarios, a high level of autonomy can also pose', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='AutoGen supports could be desirable in many scenarios, a high level of autonomy can also pose\\npotential risks, especially in high-risk applications (Amodei et al., 2016; Weld & Etzioni, 1994). As', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='a result, building fail-safes against cascading failures and exploitation, mitigating reward hacking,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='out of control and undesired behaviors, maintaining effective human oversight of applications built\\nwith AutoGen agents will become important. While AutoGen provides convenient and seamless', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='with AutoGen agents will become important. While AutoGen provides convenient and seamless\\ninvolvement of humans through a user proxy agent, developers and stakeholders still need to under-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='involvement of humans through a user proxy agent, developers and stakeholders still need to under-\\nstand and determine the appropriate level and pattern of human involvement to ensure the safe and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='stand and determine the appropriate level and pattern of human involvement to ensure the safe and\\nethical use of the technology (Horvitz, 1999; Amershi et al., 2019).\\n17', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='C Default System Message for Assistant Agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='SystemMessageYou are a helpful AI assistant. Solve tasks using your coding and language skills.In the following cases, suggest python code (in a python coding block) or shell script (in a shcoding', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='the following cases, suggest python code (in a python coding block) or shell script (in a shcoding block) for the user to execute.1. When you need to collect info, use the code to output the info you', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='for the user to execute.1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time. After sufficient info is printed and the task is ready to be solved', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='get the current date/time. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.2. When you need to perform some task', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='on your language skill, you can solve the task by yourself.2. When you need to perform some task with code, use the code to perform the task and output theresult. Finish the task smartly.Solve the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='with code, use the code to perform the task and output theresult. Finish the task smartly.Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.When using code, you must indicate the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='step uses code, and which step uses your language skill.When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can‚Äôt modify your code. So do not suggest incomplete', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='executing the code you suggest. The user can‚Äôt modify your code. So do not suggest incomplete code which requires users to modify. Don‚Äôt use a code block if it‚Äôs not intended to be executed by the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='which requires users to modify. Don‚Äôt use a code block if it‚Äôs not intended to be executed by the user.If you want the user to save the code in a file before executing it, put # filename: <filename>', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don‚Äôt include multiple code blocks in one response. Do not ask', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='the code block as the first line. Don‚Äôt include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use ‚Äôprint‚Äô function for the output when relevant. Check', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='to copy and paste the result. Instead, use ‚Äôprint‚Äô function for the output when relevant. Check the execution result returned by the user.If the result indicates there is an error, fix the error and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='execution result returned by the user.If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='output the code again. Suggest the full code instead of partial code or code changes. If the error can‚Äôt be fixed or if the task is not solved even after the code is executed successfully, analyze', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='can‚Äôt be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.When you find an answer, verify the answer carefully. Include verifiable evidence in', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='to try.When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.Reply ‚ÄúTERMINATE‚Äù in the end when everything is done.Prompting techniques color', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='if possible.Reply ‚ÄúTERMINATE‚Äù in the end when everything is done.Prompting techniques color code: Role Play; Control Flow; Output Confine; Facilitate Automation; Grounding', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='Figure 5: Default system message for the built-in assistant agent in AutoGen (v0.1.1). This is an\\nexample of conversation programming via natural language. It contains instructions of different', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='example of conversation programming via natural language. It contains instructions of different\\ntypes, including role play, control flow, output confine, facilitate automation, and grounding.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='types, including role play, control flow, output confine, facilitate automation, and grounding.\\nFigure 5 shows the default system message for the built-in assistant agent in AutoGen (v0.1.1),', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='Figure 5 shows the default system message for the built-in assistant agent in AutoGen (v0.1.1),\\nwhere we introduce several new prompting techniques and highlight them accordingly. When com-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='where we introduce several new prompting techniques and highlight them accordingly. When com-\\nbining these new prompting techniques together, we can program a fairly complex conversation even', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='bining these new prompting techniques together, we can program a fairly complex conversation even\\nwith the simplest two-agent conversation topology. This approach tries to exploit the capability of', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='LLMs in implicit state inference to a large degree. LLMs do not follow all the instructions perfectly,\\nso the design of the system needs to have other mechanisms to handle the exceptions and faults.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='so the design of the system needs to have other mechanisms to handle the exceptions and faults.\\nSome instructions can have ambiguities, and the designer should either reduce them for preciseness', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='Some instructions can have ambiguities, and the designer should either reduce them for preciseness\\nor intentionally keep them for flexibility and address the different situations in other agents. In', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='or intentionally keep them for flexibility and address the different situations in other agents. In\\ngeneral, we observe that GPT-4 follows the instructions better than GPT-3.5-turbo.\\n18', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='D Application Details\\nA1: Math Problem Solving\\nScenario 1: Autonomous Problem Solving. We perform both qualitative and quantitative eval-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='Scenario 1: Autonomous Problem Solving. We perform both qualitative and quantitative eval-\\nuations in this scenario. For all evaluations, we use GPT-4 as the base model, and pre-install the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='uations in this scenario. For all evaluations, we use GPT-4 as the base model, and pre-install the\\n‚Äúsympy‚Äù package in the execution environment. We compare AutoGen with the following LLM-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='‚Äúsympy‚Äù package in the execution environment. We compare AutoGen with the following LLM-\\nbased agent systems:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='based agent systems:\\n‚Ä¢ AutoGPT: The out-of-box AutoGPT is used. We initialize AutoGPT by setting the purpose to\\n‚Äúsolve math problems‚Äù, resulting in a ‚ÄúMathSolverGPT‚Äù with auto-generated goals.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='‚Äúsolve math problems‚Äù, resulting in a ‚ÄúMathSolverGPT‚Äù with auto-generated goals.\\n‚Ä¢ ChatGPT+Plugin: We enable the Wolfram Alpha plugin (a math computation engine) in the Ope-\\nnAI web client.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='nAI web client.\\n‚Ä¢ ChatGPT+Code Interpreter: This is a recent feature in OpenAI web client. Note that the above', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='‚Ä¢ ChatGPT+Code Interpreter: This is a recent feature in OpenAI web client. Note that the above\\ntwo premium features from ChatGPT require a paid subscription to be accessed and are the most', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='two premium features from ChatGPT require a paid subscription to be accessed and are the most\\ncompetitive commercial systems.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='competitive commercial systems.\\n‚Ä¢ LangChain ReAct+Python: We use Python agent from LangChain. To handle parsing errors, we', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='‚Ä¢ LangChain ReAct+Python: We use Python agent from LangChain. To handle parsing errors, we\\nset ‚Äúhandle parsing errors=True‚Äù, and use the default zero-shot ReAct prompt.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='set ‚Äúhandle parsing errors=True‚Äù, and use the default zero-shot ReAct prompt.\\n‚Ä¢ Multi-Agent Debate (Liang et al., 2023): We modified the code of the multi-agent debate to per-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='‚Ä¢ Multi-Agent Debate (Liang et al., 2023): We modified the code of the multi-agent debate to per-\\nform evaluation. By default, there are three agents: an affirmative agent, a negative agent, and a', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='form evaluation. By default, there are three agents: an affirmative agent, a negative agent, and a\\nmoderator.\\nWe also conducted preliminary evaluations on several other multi-agent systems, including', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='We also conducted preliminary evaluations on several other multi-agent systems, including\\nBabyAGI, CAMEL, and MetaGPT. The results indicate that they are not suitable choices for solving', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='BabyAGI, CAMEL, and MetaGPT. The results indicate that they are not suitable choices for solving\\nmath problems out of the box. For instance, when MetaGPT is tasked with solving a math problem,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='math problems out of the box. For instance, when MetaGPT is tasked with solving a math problem,\\nit begins developing software to address the problem, but most of the time, it does not actually solve', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='the problem. We have included the test examples in Appendix E.\\nTable 2: Qualitative evaluation of two math problems from the MATH dataset within the autonomous', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='Table 2: Qualitative evaluation of two math problems from the MATH dataset within the autonomous\\nproblem-solving scenario. Each LLM-based system is tested three times on each of the problems.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='problem-solving scenario. Each LLM-based system is tested three times on each of the problems.\\nThis table reports the problem-solving correctness and summarizes the reasons for failure.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='This table reports the problem-solving correctness and summarizes the reasons for failure.\\nCorrectness Failure Reason\\nAutoGen 3/3 N/A.\\nAutoGPT 0/3 The LLM gives code without the print function so the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='AutoGen 3/3 N/A.\\nAutoGPT 0/3 The LLM gives code without the print function so the\\nresult is not printed.\\nChatGPT+Plugin 1/3 The return from Wolfram Alpha contains 2 simplified', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='result is not printed.\\nChatGPT+Plugin 1/3 The return from Wolfram Alpha contains 2 simplified\\nresults, including the correct answer, but GPT-4 always\\nchooses the wrong answer.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='results, including the correct answer, but GPT-4 always\\nchooses the wrong answer.\\nChatGPT+Code Interpreter 2/3 Returns a wrong decimal result.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='chooses the wrong answer.\\nChatGPT+Code Interpreter 2/3 Returns a wrong decimal result.\\nLangChain ReAct 0/3 LangChain gives 3 different wrong answers.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='LangChain ReAct 0/3 LangChain gives 3 different wrong answers.\\nMulti-Agent Debate 0/3 It gives 3 different wrong answers due to calculation errors.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='Multi-Agent Debate 0/3 It gives 3 different wrong answers due to calculation errors.\\n(a) Evaluation on the first problem that asks to simplify a square root fraction.\\nCorrectness Failure Reason', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='Correctness Failure Reason\\nAutoGen 2/3 The final answer from code execution is wrong.\\nAutoGPT 0/3 The LLM gives code without the print function so the\\nresult is not printed.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='AutoGPT 0/3 The LLM gives code without the print function so the\\nresult is not printed.\\nChatGPT+Plugin 1/3 For one trial, GPT-4 got stuck because it keeps giving', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='result is not printed.\\nChatGPT+Plugin 1/3 For one trial, GPT-4 got stuck because it keeps giving\\nwrong queries and has to be stopped. Another trial simply\\ngives a wrong answer.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='wrong queries and has to be stopped. Another trial simply\\ngives a wrong answer.\\nChatGPT+Code Interpreter 0/3 It gives 3 different wrong answers.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='gives a wrong answer.\\nChatGPT+Code Interpreter 0/3 It gives 3 different wrong answers.\\nLangChain ReAct 0/3 LangChain gives 3 different wrong answers.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='LangChain ReAct 0/3 LangChain gives 3 different wrong answers.\\nMulti-Agent Debate 0/3 It gives 3 different wrong answers.\\n(b) Evaluation on the second number theory problem.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='(b) Evaluation on the second number theory problem.\\nFor the qualitative evaluation, we utilize two level-5 problems from the MATH dataset, testing each', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='For the qualitative evaluation, we utilize two level-5 problems from the MATH dataset, testing each\\nproblem three times. The first problem involves simplifying a square root fraction, and the second', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='problem three times. The first problem involves simplifying a square root fraction, and the second\\n19', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='problem involves solving a number theory issue. The correctness counts and reasons for failure\\nare detailed in Table 2. For the quantitative evaluation, we conduct two sets of experiments on', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='are detailed in Table 2. For the quantitative evaluation, we conduct two sets of experiments on\\nthe MATH dataset to assess the correctness of these systems: (1) an experiment involving 120', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='the MATH dataset to assess the correctness of these systems: (1) an experiment involving 120\\nlevel-5 (the most challenging level) problems, including 20 problems from six categories, excluding', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='level-5 (the most challenging level) problems, including 20 problems from six categories, excluding\\ngeometry, and (2) an experiment on the entire test set, which includes 5000 problems. We exclude', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='geometry, and (2) an experiment on the entire test set, which includes 5000 problems. We exclude\\nAutoGPT from this evaluation as it cannot access results from code executions and does not solve', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='AutoGPT from this evaluation as it cannot access results from code executions and does not solve\\nany problems in the qualitative evaluation. Our analysis of the entire dataset reveals that AutoGen', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='any problems in the qualitative evaluation. Our analysis of the entire dataset reveals that AutoGen\\nachieves an overall accuracy of 69.48%, while GPT-4‚Äôs accuracy stands at 55.18%. From these', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='achieves an overall accuracy of 69.48%, while GPT-4‚Äôs accuracy stands at 55.18%. From these\\nevaluations, we have the following observations regarding the problem-solving success rate and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='evaluations, we have the following observations regarding the problem-solving success rate and\\nuser experience of these systems:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='user experience of these systems:\\n‚Ä¢ Problem-solving success rate: Results from the quantitative evaluations show that AutoGen can', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='‚Ä¢ Problem-solving success rate: Results from the quantitative evaluations show that AutoGen can\\nhelp achieve the highest problem-solving success rate among all the compared methods. The qual-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='help achieve the highest problem-solving success rate among all the compared methods. The qual-\\nitative evaluations elucidate common failure reasons across several alternative approaches. Chat-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='itative evaluations elucidate common failure reasons across several alternative approaches. Chat-\\nGPT+Code Interpreter fails to solve the second problem, and ChatGPT+Plugin struggles to solve', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='GPT+Code Interpreter fails to solve the second problem, and ChatGPT+Plugin struggles to solve\\nboth problems. AutoGPT fails on both problems due to code execution issues. The LangChain', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='both problems. AutoGPT fails on both problems due to code execution issues. The LangChain\\nagent also fails on both problems, producing code that results in incorrect answers in all trials.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='agent also fails on both problems, producing code that results in incorrect answers in all trials.\\n‚Ä¢ Based on the qualitative evaluation, we analyze the user experience concerning the verbosity of', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='‚Ä¢ Based on the qualitative evaluation, we analyze the user experience concerning the verbosity of\\nthe response and the ability of the LLM-based system to run without unexpected behaviors. Chat-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='the response and the ability of the LLM-based system to run without unexpected behaviors. Chat-\\nGPT+Plugin is the least verbose, mainly because Wolfram queries are much shorter than Python', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='GPT+Plugin is the least verbose, mainly because Wolfram queries are much shorter than Python\\ncode. AutoGen , ChatGPT+Code Interpreter, and LangChain exhibit similar verbosity, although', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='code. AutoGen , ChatGPT+Code Interpreter, and LangChain exhibit similar verbosity, although\\nLangChain is slightly more verbose due to more code execution errors. AutoGPT is the most', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='LangChain is slightly more verbose due to more code execution errors. AutoGPT is the most\\nverbose system owing to predefined steps like THOUGHTS, REASONING, and PLAN, which it', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='verbose system owing to predefined steps like THOUGHTS, REASONING, and PLAN, which it\\nincludes in replies every time. Overall, AutoGen and ChatGPT+Code Interpreter operate smoothly', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='includes in replies every time. Overall, AutoGen and ChatGPT+Code Interpreter operate smoothly\\nwithout exceptions. We note the occurrences of undesired behaviors from other LLM-based sys-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='without exceptions. We note the occurrences of undesired behaviors from other LLM-based sys-\\ntems that could affect user experience: AutoGPT consistently outputs code without the print‚Äô', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='tems that could affect user experience: AutoGPT consistently outputs code without the print‚Äô\\nstatement and cannot correct this, requiring the user to run them manually; ChatGPT with Wol-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='statement and cannot correct this, requiring the user to run them manually; ChatGPT with Wol-\\nfram Alpha plugin has the potential to become stuck in a loop that must be manually stopped; and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='fram Alpha plugin has the potential to become stuck in a loop that must be manually stopped; and\\nLangchain ReAct could exit with a parse error, necessitating the passing of a ‚Äòhandle parse error‚Äô', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='Langchain ReAct could exit with a parse error, necessitating the passing of a ‚Äòhandle parse error‚Äô\\nparameter.\\nEnable Multi-User Problem Solving ViaStudent        and Expert Student Proxy', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='parameter.\\nEnable Multi-User Problem Solving ViaStudent        and Expert Student Proxy\\nStudentAssistant\\nExpertAssistant\\nExpert Proxy\\nAsk for expert', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='StudentAssistant\\nExpertAssistant\\nExpert Proxy\\nAsk for expert\\nEnable Autonomous and Human-in-the-loop Problem Solving', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='ExpertAssistant\\nExpert Proxy\\nAsk for expert\\nEnable Autonomous and Human-in-the-loop Problem Solving\\nFigure 6: Examples of three settings utilized to solve math problems using AutoGen : (Gray) En-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='Figure 6: Examples of three settings utilized to solve math problems using AutoGen : (Gray) En-\\nables a workflow where a student collaborates with an assistant agent to solve problems, either', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='ables a workflow where a student collaborates with an assistant agent to solve problems, either\\nautonomously or in a human-in-the-loop mode. (Gray + Orange) Facilitates a more sophisticated', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='autonomously or in a human-in-the-loop mode. (Gray + Orange) Facilitates a more sophisticated\\nworkflow wherein the assistant, on the fly, can engage another user termed ‚Äúexpert‚Äù, who is in the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='loop with their own assistant agent, to aid in problem-solving if its own solutions are not satisfactory.\\nScenario 2: Human-in-the-loop Problem Solving. For challenging problems that these LLM', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='Scenario 2: Human-in-the-loop Problem Solving. For challenging problems that these LLM\\nsystems cannot solve autonomously, human feedback during the problem-solving process can be\\n20', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='helpful. To incorporate human feedback with AutoGen , one can set human input mode=‚ÄòALWAYS‚Äô\\nin the user proxy agent. We select one challenging problem that none of these systems can solve', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='in the user proxy agent. We select one challenging problem that none of these systems can solve\\nautonomously across three trials. We adhere to the process outlined below to provide human inputs', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='autonomously across three trials. We adhere to the process outlined below to provide human inputs\\nfor all the compared methods:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='for all the compared methods:\\n1. Input the problem: Find the equation of the plane which bisects the angle\\nbetween the planes 3x‚àí6y+ 2z+ 5 = 0 and4x‚àí12y+ 3z‚àí3 = 0 ,and which', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='between the planes 3x‚àí6y+ 2z+ 5 = 0 and4x‚àí12y+ 3z‚àí3 = 0 ,and which\\ncontains the point (‚àí5,‚àí1,‚àí5).Enter your answer in the form\\nAx+By+Cz+D= 0,\\nwhere A, B, C, D are integers such that A > 0and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='Ax+By+Cz+D= 0,\\nwhere A, B, C, D are integers such that A > 0and\\ngcd(|A|,|B|,|C|,|D|) = 1.\\n2. The response from the system does not solve the problem correctly. We then give a', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='2. The response from the system does not solve the problem correctly. We then give a\\nhint to the model: Your idea is not correct. Let‚Äôs solve this together.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='hint to the model: Your idea is not correct. Let‚Äôs solve this together.\\nSuppose P= (x, y, z )is a point that lies on a plane that bisects the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='Suppose P= (x, y, z )is a point that lies on a plane that bisects the\\nangle, the distance from P to the two planes is the same. Please\\nset up this equation first.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='angle, the distance from P to the two planes is the same. Please\\nset up this equation first.\\n3. We expect the system to give the correct distance equation. Since the equation involves', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='3. We expect the system to give the correct distance equation. Since the equation involves\\nan absolute sign that is hard to solve, we would give the next hint: Consider the two', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='an absolute sign that is hard to solve, we would give the next hint: Consider the two\\ncases to remove the abs sign and get two possible solutions.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='cases to remove the abs sign and get two possible solutions.\\n4. If the system returns the two possible solutions and doesn‚Äôt continue to the next step, we', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='4. If the system returns the two possible solutions and doesn‚Äôt continue to the next step, we\\ngive the last hint: Use point (-5,-1,-5) to determine which is correct and\\ngive the final answer.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='give the last hint: Use point (-5,-1,-5) to determine which is correct and\\ngive the final answer.\\n5. Final answer is 11x+6y+5z+86=0 .', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='give the final answer.\\n5. Final answer is 11x+6y+5z+86=0 .\\nWe observed that AutoGen consistently solved the problem across all three trials. ChatGPT+Code', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='We observed that AutoGen consistently solved the problem across all three trials. ChatGPT+Code\\nInterpreter and ChatGPT+Plugin managed to solve the problem in two out of three trials, while Au-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='Interpreter and ChatGPT+Plugin managed to solve the problem in two out of three trials, while Au-\\ntoGPT failed to solve it in all three attempts. In its unsuccessful attempt, ChatGPT+Code Interpreter', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='failed to adhere to human hints. In its failed trial, ChatGPT+Plugin produced an almost correct solu-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='tion but had a sign discrepancy in the final answer. AutoGPT was unable to yield a correct solution', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='in any of the trials. In one trial, it derived an incorrect distance equation. In the other two trials, the\\nfinal answer was incorrect due to code execution errors.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='final answer was incorrect due to code execution errors.\\nScenario 3: Multi-User Problem Solving. Next-generation LLM applications may necessitate', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='Scenario 3: Multi-User Problem Solving. Next-generation LLM applications may necessitate\\nthe involvement of multiple real users for collectively solving a problem with the assistance of', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='the involvement of multiple real users for collectively solving a problem with the assistance of\\nLLMs. We showcase how AutoGen can be leveraged to effortlessly construct such a system. Specif-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='LLMs. We showcase how AutoGen can be leveraged to effortlessly construct such a system. Specif-\\nically, building upon scenario 2 mentioned above, we aim to devise a simple system involving two', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='ically, building upon scenario 2 mentioned above, we aim to devise a simple system involving two\\nhuman users: a student and an expert. In this setup, the student interacts with an LLM assistant to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='human users: a student and an expert. In this setup, the student interacts with an LLM assistant to\\naddress some problems, and the LLM automatically resorts to the expert when necessary.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='address some problems, and the LLM automatically resorts to the expert when necessary.\\nThe overall workflow is as follows: The student chats with the LLM-based assistant agent through', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='The overall workflow is as follows: The student chats with the LLM-based assistant agent through\\na student proxy agent to solve problems. When the assistant cannot solve the problem satisfactorily,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='or the solution does not match the expectation of the student, it would automatically hold the con-\\nversation and call the pre-defined askforexpert function via the function callfeature of GPT', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='versation and call the pre-defined askforexpert function via the function callfeature of GPT\\nin order to resort to the expert. Specifically, it would automatically produce the initial message for', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='theaskforexpert function, which could be the statement of the problem or the request to verify\\nthe solution to a problem, and the expert is supposed to respond to this message with the help of', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='the solution to a problem, and the expert is supposed to respond to this message with the help of\\nthe expert assistant. After the conversation between the expert and the expert‚Äôs assistant, the final', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='message would be sent back to the student assistant as the response to the initial message. Then, the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='student assistant would resume the conversation with the student using the response from the expert\\nfor a better solution. A detailed visualization is shown in Figure 6.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='for a better solution. A detailed visualization is shown in Figure 6.\\nWith AutoGen , constructing the student/expert proxy agent and the assistant agents is straight-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='With AutoGen , constructing the student/expert proxy agent and the assistant agents is straight-\\nforward by reusing the built-in UserProxyAgent andAssistantAgent through appropriate', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='forward by reusing the built-in UserProxyAgent andAssistantAgent through appropriate\\nconfigurations. The only development required involves writing several lines of code for the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='configurations. The only development required involves writing several lines of code for the\\naskforexpert function, which then becomes part of the configuration for the assistant. Ad-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='askforexpert function, which then becomes part of the configuration for the assistant. Ad-\\nditionally, it‚Äôs easy to extend such a system to include more than one expert, with a specific', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='ditionally, it‚Äôs easy to extend such a system to include more than one expert, with a specific\\naskforexpert function for each, or to include multiple student users with a shared expert for', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='askforexpert function for each, or to include multiple student users with a shared expert for\\nconsultation.\\n21', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='A2: Retrieval-Augmented Code Generation and Question Answering\\nRetrieval-augmentedAssistantRetrieval-augmented  User Proxy', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='Retrieval-augmentedAssistantRetrieval-augmented  User Proxy\\n1. Question and Contexts3. Terminate,feedbacks or `Update Context`4. Satisfied Answers or Terminate', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='1. Question and Contexts3. Terminate,feedbacks or `Update Context`4. Satisfied Answers or Terminate\\n2. Satisfied Answers or `Update Context`', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='2. Satisfied Answers or `Update Context`\\nFigure 7: Overview of Retrieval-augmented Chat which involves two agents, including a Retrieval-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='Figure 7: Overview of Retrieval-augmented Chat which involves two agents, including a Retrieval-\\naugmented User Proxy and a Retrieval-augmented Assistant. Given a set of documents, the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='augmented User Proxy and a Retrieval-augmented Assistant. Given a set of documents, the\\nRetrieval-augmented User Proxy first automatically processes documents‚Äîsplits, chunks, and stores', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='Retrieval-augmented User Proxy first automatically processes documents‚Äîsplits, chunks, and stores\\nthem in a vector database. Then for a given user input, it retrieves relevant chunks as context and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='them in a vector database. Then for a given user input, it retrieves relevant chunks as context and\\nsends it to the Retrieval-augmented Assistant, which uses LLM to generate code or text to answer', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='sends it to the Retrieval-augmented Assistant, which uses LLM to generate code or text to answer\\nquestions. Agents converse until they find a satisfactory answer.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='questions. Agents converse until they find a satisfactory answer.\\nDetailed Workflow. The workflow of Retrieval-Augmented Chat is illustrated in Figure 7. To', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='Detailed Workflow. The workflow of Retrieval-Augmented Chat is illustrated in Figure 7. To\\nuse Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='use Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented\\nUser Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='User Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy\\nnecessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='necessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented\\nUser Proxy can download the documents, segment them into chunks of a specific size, compute', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='User Proxy can download the documents, segment them into chunks of a specific size, compute\\nembeddings, and store them in a vector database. Once a chat is initiated, the agents collaboratively', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='engage in code generation or question-answering adhering to the procedures outlined below:\\n1. The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding simi-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='1. The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding simi-\\nlarity, and sends them along with the question to the Retrieval-Augmented Assistant.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='larity, and sends them along with the question to the Retrieval-Augmented Assistant.\\n2. The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='2. The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based\\non the question and context provided. If the LLM is unable to produce a satisfactory response, it', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='on the question and context provided. If the LLM is unable to produce a satisfactory response, it\\nis instructed to reply with ‚ÄúUpdate Context‚Äù to the Retrieval-Augmented User Proxy.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='is instructed to reply with ‚ÄúUpdate Context‚Äù to the Retrieval-Augmented User Proxy.\\n3. If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='3. If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and\\nsends the output as feedback. If there are no code blocks or instructions to update the context, it', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='sends the output as feedback. If there are no code blocks or instructions to update the context, it\\nterminates the conversation. Otherwise, it updates the context and forwards the question along', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='terminates the conversation. Otherwise, it updates the context and forwards the question along\\nwith the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='with the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation\\nis enabled, individuals can proactively send any feedback, including Update Context‚Äù, to the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='is enabled, individuals can proactively send any feedback, including Update Context‚Äù, to the\\nRetrieval-Augmented Assistant.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='Retrieval-Augmented Assistant.\\n4. If the Retrieval-Augmented Assistant receives ‚ÄúUpdate Context‚Äù, it requests the next most similar', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='chunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it\\ngenerates new code or text based on the feedback and chat history. If the LLM fails to generate', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='generates new code or text based on the feedback and chat history. If the LLM fails to generate\\nan answer, it replies with ‚ÄúUpdate Context‚Äù again. This process can be repeated several times.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='an answer, it replies with ‚ÄúUpdate Context‚Äù again. This process can be repeated several times.\\nThe conversation terminates if no more documents are available for the context.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='The conversation terminates if no more documents are available for the context.\\nWe utilize Retrieval-Augmented Chat in two scenarios. The first scenario aids in generating code', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='We utilize Retrieval-Augmented Chat in two scenarios. The first scenario aids in generating code\\nbased on a given codebase. While LLMs possess strong coding abilities, they are unable to utilize', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='packages or APIs that are not included in their training data, e.g., private codebases, or have trouble\\nusing trained ones that are frequently updated post-training. Hence, Retrieval-Augmented Code', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='using trained ones that are frequently updated post-training. Hence, Retrieval-Augmented Code\\nGeneration is considered to be highly valuable. The second scenario involves question-answering', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='Generation is considered to be highly valuable. The second scenario involves question-answering\\non the Natural Questions dataset (Kwiatkowski et al., 2019), enabling us to obtain comparative', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='on the Natural Questions dataset (Kwiatkowski et al., 2019), enabling us to obtain comparative\\nevaluation metrics for the performance of our system.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='evaluation metrics for the performance of our system.\\nScenario 1: Evaluation on Natural Questions QA dataset. In this case, we evaluate the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='Scenario 1: Evaluation on Natural Questions QA dataset. In this case, we evaluate the\\nRetrieval-Augmented Chat‚Äôs end-to-end question-answering performance using the Natural Ques-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='Retrieval-Augmented Chat‚Äôs end-to-end question-answering performance using the Natural Ques-\\ntions dataset (Kwiatkowski et al., 2019). We collected 5,332 non-redundant context documents and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='tions dataset (Kwiatkowski et al., 2019). We collected 5,332 non-redundant context documents and\\n6,775 queries from HuggingFace. First, we create a document collection based on the entire context', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='6,775 queries from HuggingFace. First, we create a document collection based on the entire context\\ncorpus and store it in the vector database. Then, we utilize Retrieval-Augmented Chat to answer the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='corpus and store it in the vector database. Then, we utilize Retrieval-Augmented Chat to answer the\\nquestions. An example (Figure 8) from the NQ dataset showcases the advantages of the interactive', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='questions. An example (Figure 8) from the NQ dataset showcases the advantages of the interactive\\nretrieval feature: ‚Äúwho carried the usa flag in opening ceremony‚Äù . When attempting to answer', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='retrieval feature: ‚Äúwho carried the usa flag in opening ceremony‚Äù . When attempting to answer\\nthis question, the context with the highest similarity to the question embedding does not contain the\\n22', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='Task description who carried the usaflag in opening ceremony?Assistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply exactly', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='If you can‚Äôt answer the question with or without the current context, you should reply exactly ‚ÄòUPDATE CONTEXT‚Äô. You must give as short an answer as possible.User‚Äôs question is: who carried the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='‚ÄòUPDATE CONTEXT‚Äô. You must give as short an answer as possible.User‚Äôs question is: who carried the usaflag in opening ceremonyContext is: < P > On January 17 , 1899 , under ... The Astros beat the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='usaflag in opening ceremonyContext is: < P > On January 17 , 1899 , under ... The Astros beat the Yankees that night , 2 -1 . < /P >UserProxy: Sorry, I cannot find any information about who carried', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='Yankees that night , 2 -1 . < /P >UserProxy: Sorry, I cannot find any information about who carried the USA flag in the opening ceremony. UPDATE CONTEXT.Assistant: You‚Äôre a retrieve augmented', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='carried the USA flag in the opening ceremony. UPDATE CONTEXT.Assistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply exactly', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='If you can‚Äôt answer the question with or without the current context, you should reply exactly ‚ÄòUPDATE CONTEXT‚Äô. You must give as short an answer as possible.User‚Äôs question is: who carried the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='‚ÄòUPDATE CONTEXT‚Äô. You must give as short an answer as possible.User‚Äôs question is: who carried the usaflag in opening ceremonyContext is: < T able > ... Erin Hamlin ... < /T d >UserProxy: Erin Hamlin', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='in opening ceremonyContext is: < T able > ... Erin Hamlin ... < /T d >UserProxy: Erin Hamlin carried the USA flag in the opening ceremony.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='(a) W/O Interactive Retrieval\\n(b) W/ Interactive Retrieval', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='Assistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply ‚Äòsorry, I don‚Äôt know‚Äô. You must give as short an answer as possible.User‚Äôs', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='you should reply ‚Äòsorry, I don‚Äôt know‚Äô. You must give as short an answer as possible.User‚Äôs question is:who carried the usaflag in opening ceremonyContext is: < P > On January 17 , 1899 , under ...', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='is:who carried the usaflag in opening ceremonyContext is: < P > On January 17 , 1899 , under ... The Astros beat the Yankees that night , 2 -1 . < /P >UserProxy: Sorry, I cannot find any information', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='Astros beat the Yankees that night , 2 -1 . < /P >UserProxy: Sorry, I cannot find any information about who carried the USA flag in the opening ceremony. Sorry, I don‚Äôt know.Figure 8:', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='information about who carried the USA flag in the opening ceremony. Sorry, I don‚Äôt know.Figure 8: Retrieval-augmented Chat without (W/O) and with (W/) interactive retrieval .', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='required information for a response. As a result, the LLM assistant (GPT-3.5-turbo) replies ‚ÄúSorry,\\nI cannot find any information about who carried the USA flag in the opening ceremony. UPDATE', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='I cannot find any information about who carried the USA flag in the opening ceremony. UPDATE\\nCONTEXT. ‚Äù With the unique and innovative ability to update context in Retrieval-Augmented Chat,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='CONTEXT. ‚Äù With the unique and innovative ability to update context in Retrieval-Augmented Chat,\\nthe user proxy agent automatically updates the context and forwards it to the assistant agent again.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='Following this process, the agent is able to generate the correct answer to the question.\\nIn addition, we conduct an experiment using the same prompt as illustrated in (Adlakha et al., 2023)', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='to investigate the advantages of AutoGen W/O interactive retrieval . The F1 score and Recall for the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='first 500 questions are 23.40% and 62.60%, respectively, aligning closely with the results reported\\nin Figure 4b. Consequently, we assert that AutoGen W/O interactive retrieval outperforms DPR due', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='in Figure 4b. Consequently, we assert that AutoGen W/O interactive retrieval outperforms DPR due\\nto differences in the retrievers employed. Specifically, we utilize a straightforward vector search', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='to differences in the retrievers employed. Specifically, we utilize a straightforward vector search\\nretriever with the all-MiniLM-L6-v2 model for embeddings.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='retriever with the all-MiniLM-L6-v2 model for embeddings.\\nFurthermore, we analyze the number of LLM calls in experiments involving both AutoGen and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='Furthermore, we analyze the number of LLM calls in experiments involving both AutoGen and\\nAutoGen W/O interactive retrieval , revealing that approximately 19.4% of questions in the Natural', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='AutoGen W/O interactive retrieval , revealing that approximately 19.4% of questions in the Natural\\nQuestions dataset trigger an ‚ÄúUpdate Context‚Äù operation, resulting in additional LLM calls.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='Questions dataset trigger an ‚ÄúUpdate Context‚Äù operation, resulting in additional LLM calls.\\nScenario 2: Code Generation Leveraging Latest APIs from the Codebase. In this case, the ques-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='Scenario 2: Code Generation Leveraging Latest APIs from the Codebase. In this case, the ques-\\ntion is ‚ÄúHow can I use FLAML to perform a classification task and use Spark for parallel training?', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='Train for 30 seconds and force cancel jobs if the time limit is reached. ‚Äù . FLAML (v1) (Wang et al.,\\n2021) is an open-source Python library designed for efficient AutoML and tuning. It was open-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='2021) is an open-source Python library designed for efficient AutoML and tuning. It was open-\\nsourced in December 2020, and is included in the training data of GPT-4. However, the question', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='sourced in December 2020, and is included in the training data of GPT-4. However, the question\\nnecessitates the use of Spark-related APIs, which were added in December 2022 and are not encom-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='necessitates the use of Spark-related APIs, which were added in December 2022 and are not encom-\\npassed in the GPT-4 training data. Consequently, the original GPT-4 model is unable to generate the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='correct code, due to its lack of knowledge regarding Spark-related APIs. Instead, it erroneously cre-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='ates a non-existent parameter, spark , and sets it to True‚Äô. Nevertheless, with Retrieval-Augmented\\nChat, we provide the latest reference documents as context. Then, GPT-4 generates the correct code', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='Chat, we provide the latest reference documents as context. Then, GPT-4 generates the correct code\\nblocks by setting usespark andforce cancel to True‚Äô.\\n23', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='A3: Decision Making in Text World Environments\\nALFWorld\\nExecutor\\nReward & StateAction Decision\\nAssistant\\nObservation: On the desk 2, you see an alarmclock 3,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='Reward & StateAction Decision\\nAssistant\\nObservation: On the desk 2, you see an alarmclock 3, \\na bowl 3, a creditcard 2, a mug 1, and a pencil 2.Action decision: Pick up pencil 2 from desk 2', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='a bowl 3, a creditcard 2, a mug 1, and a pencil 2.Action decision: Pick up pencil 2 from desk 2\\nGroundingAgent\\nALFChat (two agents) ALFChat (three agents)\\nALFWorld Executor\\nAssistant', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='GroundingAgent\\nALFChat (two agents) ALFChat (three agents)\\nALFWorld Executor\\nAssistant\\nFigure 9: We use AutoGen to solve tasks in the ALFWorld benchmark, which contains household', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='Figure 9: We use AutoGen to solve tasks in the ALFWorld benchmark, which contains household\\ntasks described in natural language. We propose two designs: a two-agent design where the assistant', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='tasks described in natural language. We propose two designs: a two-agent design where the assistant\\nagent suggests the next step, and the Executor executes actions and provides feedback. The three-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='agent suggests the next step, and the Executor executes actions and provides feedback. The three-\\nagent design adds a grounding agent that supplies commonsense facts to the executor when needed.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='agent design adds a grounding agent that supplies commonsense facts to the executor when needed.\\nALFWorld (Shridhar et al., 2021) is a synthetic language-based interactive decision-making task.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='ALFWorld (Shridhar et al., 2021) is a synthetic language-based interactive decision-making task.\\nIt comprises textual environments that aim to simulate real-world household scenes. Given a high-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='level goal (e.g., putting a hot apple in the fridge) and the description of the household environment,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='the agent needs to explore and interact with the simulated household environment through a textual\\ninterface. A typical task environment contains various types of locations and could require more', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='interface. A typical task environment contains various types of locations and could require more\\nthan 40 steps to finish, which highlights the need for agents to decompose the goal into subtasks and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='tackle them one by one, while effectively exploring the environments.\\nDetailed Workflow. We first propose a straightforward two-agent system with AutoGen , illustrated', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='Detailed Workflow. We first propose a straightforward two-agent system with AutoGen , illustrated\\non the left-hand side of Figure 9, to tackle tasks from this benchmark. The system consists of', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='on the left-hand side of Figure 9, to tackle tasks from this benchmark. The system consists of\\nan assistant agent and an executor agent. The assistant agent generates plans and makes action', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='an assistant agent and an executor agent. The assistant agent generates plans and makes action\\ndecisions to solve the tasks. The executor agent is tailored specifically for ALFWorld. It performs', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='decisions to solve the tasks. The executor agent is tailored specifically for ALFWorld. It performs\\nactions proposed by the assistant and reports action execution results in the household environment', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='as feedback to the assistant. Due to the strict format requirements for the output format, we use the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='BLEU metric to evaluate the similarity of the output to all valid action options. The option with the\\nhighest similarity will be chosen as the action for this round.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='highest similarity will be chosen as the action for this round.\\nOne major challenge encompassed in ALFWorld is commonsense reasoning. The agent needs to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='One major challenge encompassed in ALFWorld is commonsense reasoning. The agent needs to\\nextract patterns from the few-shot examples provided and combine them with the agent‚Äôs general', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='extract patterns from the few-shot examples provided and combine them with the agent‚Äôs general\\nknowledge of household environments to fully understand task rules. More often than not, the as-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='knowledge of household environments to fully understand task rules. More often than not, the as-\\nsistant tends to neglect some basic knowledge of the household environment. Thanks to the easy-to-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='sistant tends to neglect some basic knowledge of the household environment. Thanks to the easy-to-\\nimplement multi-agent conversational feature of AutoGen , enhancing the assistant agent‚Äôs reason-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='implement multi-agent conversational feature of AutoGen , enhancing the assistant agent‚Äôs reason-\\ning ability by adding a new grounding agent to provide commonsense facts for the decision-making', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='ing ability by adding a new grounding agent to provide commonsense facts for the decision-making\\nagent‚Äôs reference becomes straightforward. By scrutinizing the failed attempts and summarizing the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='agent‚Äôs reference becomes straightforward. By scrutinizing the failed attempts and summarizing the\\nreasons for failure, we obtained a holistic understanding of the commonsense knowledge that the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='reasons for failure, we obtained a holistic understanding of the commonsense knowledge that the\\nassistant agent lacks. Then, we set a grounding agent to provide this general knowledge when the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='assistant agent lacks. Then, we set a grounding agent to provide this general knowledge when the\\ntask begins and whenever the assistant outputs the same action three times in a row. This ensures the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='assistant takes this commonsense knowledge into consideration and prevents it from getting stuck in\\noutputting the same content or constantly apologizing.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='outputting the same content or constantly apologizing.\\nWe compare our system‚Äôs performance with ReAct, which treats ALFWorld as a text-completion', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='We compare our system‚Äôs performance with ReAct, which treats ALFWorld as a text-completion\\ntask. ReAct (Yao et al., 2022) is a few-shot prompting technique that interleaves reasoning and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='task. ReAct (Yao et al., 2022) is a few-shot prompting technique that interleaves reasoning and\\nacting, allowing for greater synergy between the two and significantly improving performance on', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='acting, allowing for greater synergy between the two and significantly improving performance on\\nboth language and decision-making tasks. We integrate ReAct into AutoGen by modifying the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='both language and decision-making tasks. We integrate ReAct into AutoGen by modifying the\\nprompts in a conversational manner. Following ReAct, we employ a two-shot setting. The few-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='prompts in a conversational manner. Following ReAct, we employ a two-shot setting. The few-\\nshot prompts are obtained from the corresponding repository. As shown in Table 3, the two-agent\\n24', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content='Task description\\nYou are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a safe 1, a drawer 2, a dra wer1, a \\nshelf 3, a shelf 2, and a shelf 1.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='shelf 3, a shelf 2, and a shelf 1.\\nYour task is to: look at bowl under the desklamp . \\nAssistant (to Executor) : THOUGHT [First, I need to find a bowl. A', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='Assistant (to Executor) : THOUGHT [First, I need to find a bowl. A \\nbowl is more likely to appear in desk (1 -2), drawer (1 -2), shelf', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='bowl is more likely to appear in desk (1 -2), drawer (1 -2), shelf \\n(1-3), bed (1). Then I need to find and use a desklamp .] ACTION \\n[go to desk 1]', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='(1-3), bed (1). Then I need to find and use a desklamp .] ACTION \\n[go to desk 1]\\n(Assistant finds the desklamp at desk 1, then goes to desk 2.)', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='[go to desk 1]\\n(Assistant finds the desklamp at desk 1, then goes to desk 2.)\\nExecutor (to Assistant) : On the desk 2, you see a bowl 1, and a \\ncd 3...', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='Executor (to Assistant) : On the desk 2, you see a bowl 1, and a \\ncd 3... \\nAssistant (to Executor) : THOUGHT [Now I find a bowl (1). I need', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content=\"cd 3... \\nAssistant (to Executor) : THOUGHT [Now I find a bowl (1). I need \\nto use the desklamp to look at the bowl. I'll go to the desklamp\\nnow.] ACTION [go to desk 1]\", metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content=\"to use the desklamp to look at the bowl. I'll go to the desklamp\\nnow.] ACTION [go to desk 1]\\nExecutor (to Assistant): On the desk 1, you see a desklamp 1, a \\npencil 2...\", metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='Executor (to Assistant): On the desk 1, you see a desklamp 1, a \\npencil 2...\\nAssistant: ACTION [use desklamp 1]\\nExecutor (to Assistant) : You turn on the desklamp 1.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='Assistant: ACTION [use desklamp 1]\\nExecutor (to Assistant) : You turn on the desklamp 1.\\nAssistant (to Executor): THOUGHT [Now that the desklamp is turned', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='Assistant (to Executor): THOUGHT [Now that the desklamp is turned \\non, I can use it to look at the bowl (1) that I found earlier.] \\nACTION [look at bowl 1 under the desklamp 1]', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='ACTION [look at bowl 1 under the desklamp 1]\\nExecutor (to Assistant) : You turn on the desklamp 1.\\n(Assistant falls into an infinite loop turning on desklamp .)', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='(Assistant falls into an infinite loop turning on desklamp .)\\nExecutor (to Assistant) : Task failed. Reply TERMINATE.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='Executor (to Assistant) : Task failed. Reply TERMINATE.\\nAssistant (to Executor) : TERMINATEAssistant (to Executor) : THOUGHT [First, I need to find a bowl. A', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='bowl is more likely to appear in desk (1 -2), drawer (1 -2), shelf \\n(1-3), bed (1). Then I need to find and use a desklamp .] ACTION \\n[go to desk 1]', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='(1-3), bed (1). Then I need to find and use a desklamp .] ACTION \\n[go to desk 1]\\n(Assistant finds the desklamp at desk 1, then goes to desk 2.)\\n(Assistant falls into the same loop as (a))', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='(Assistant falls into the same loop as (a))\\nGroundingAgent (to Executor) : You must find and take the object \\nbefore you can examine it. You must go to where the target', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='before you can examine it. You must go to where the target \\nobject is before you can use it.\\nExecutor(to Assistant): You turn on the desklamp 1. Hint: You must', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='Executor(to Assistant): You turn on the desklamp 1. Hint: You must \\nfind and take the object before you can examine it. \\nAssistant (to Executor): THOUGHT [I need to find and take the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='Assistant (to Executor): THOUGHT [I need to find and take the \\nbowl (1) before I can examine it. Let me find the bowl first.] \\nACTION [go to desk 2]', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='bowl (1) before I can examine it. Let me find the bowl first.] \\nACTION [go to desk 2]\\nExecutor (to Assistant): On the desk 2, you see a bowl 1, a cd \\n3...', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='ACTION [go to desk 2]\\nExecutor (to Assistant): On the desk 2, you see a bowl 1, a cd \\n3...\\nAssistant (to Executor) : ACTION [take bowl 1 from desk 2]', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='3...\\nAssistant (to Executor) : ACTION [take bowl 1 from desk 2]\\nExecutor (to Assistant): You pick up bowl 1 from desk 2.\\nAssistant (to Executor): ACTION [go to desk 1]', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='Assistant (to Executor): ACTION [go to desk 1]\\nAssistant (to Executor): ACTION[use desklamp 1]\\nExecutor (to Assistant) : Task success. Reply TERMINATE.\\nAssistant (to Executor) : TERMINATE', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='Executor (to Assistant) : Task success. Reply TERMINATE.\\nAssistant (to Executor) : TERMINATE\\n(a) Two agents', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='Assistant (to Executor) : TERMINATE\\n(a) Two agents\\n (b) Three agentsFigure 10: Comparison of results from two designs: (a) Two-agent design which consists of an', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='assistant and an executor, (b) Three-agent design which adds a grounding agent that serves as a\\nknowledge source. For simplicity, we omit the in-context examples and part of the exploration', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='knowledge source. For simplicity, we omit the in-context examples and part of the exploration\\ntrajectory, and only show parts contributing to the failure/success of the attempt.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='trajectory, and only show parts contributing to the failure/success of the attempt.\\nMethod Pick Clean Heat Cool Look Pick 2 All\\nReAct (avg) 63 52 48 71 61 24 54', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='Method Pick Clean Heat Cool Look Pick 2 All\\nReAct (avg) 63 52 48 71 61 24 54\\nALFChat (2 agents)(avg) 61 58 57 67 50 19 54\\nALFChat (3 agents)(avg) 79 64 70 76 78 41 69', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='ALFChat (2 agents)(avg) 61 58 57 67 50 19 54\\nALFChat (3 agents)(avg) 79 64 70 76 78 41 69\\nReAct (best of 3) 75 62 61 81 78 35 66\\nALFChat (2 agents)(best of 3) 71 61 65 76 67 35 63', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='ReAct (best of 3) 75 62 61 81 78 35 66\\nALFChat (2 agents)(best of 3) 71 61 65 76 67 35 63\\nAFLChat (3 agents)(best of 3) 92 74 78 86 83 41 77', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='AFLChat (3 agents)(best of 3) 92 74 78 86 83 41 77\\nTable 3: Comparisons between ReAct and the two variants of ALFChat on the ALFWorld bench-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='Table 3: Comparisons between ReAct and the two variants of ALFChat on the ALFWorld bench-\\nmark. For each task, we report the success rate out of 3 attempts. Success rate denotes the number', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='mark. For each task, we report the success rate out of 3 attempts. Success rate denotes the number\\nof tasks successfully completed by the agent divided by the total number of tasks. The results show', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='of tasks successfully completed by the agent divided by the total number of tasks. The results show\\nthat adding a grounding agent significantly improves the task success rate in ALFChat.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='that adding a grounding agent significantly improves the task success rate in ALFChat.\\ndesign matches the performance of ReAct, while the three-agent design significantly outperforms', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='design matches the performance of ReAct, while the three-agent design significantly outperforms\\nReAct. We surmise that the performance discrepancy is caused by the inherent difference between', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='ReAct. We surmise that the performance discrepancy is caused by the inherent difference between\\ndialogue-completion and text-completion tasks. On the other hand, introducing a grounding agent', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='dialogue-completion and text-completion tasks. On the other hand, introducing a grounding agent\\nas a knowledge source remarkably advances performance on all types of tasks.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='as a knowledge source remarkably advances performance on all types of tasks.\\nCase study . Figure 10 exemplifies how a three-agent design eliminates one root cause for failure', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='cases. Most of the tasks involve taking an object and then performing a specific action with it (e.g.,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='finding a vase and placing it on a cupboard). Without a grounding agent, the assistant frequently\\nconflates finding an object with taking it, as illustrated in Figure 10a). This leads to most of the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='failure cases in ‚Äôpick‚Äô and ‚Äôlook‚Äô type tasks. With the introduction of a grounding agent, the assistant\\ncan break out of this loop and successfully complete the task', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='can break out of this loop and successfully complete the task\\nTakeaways. We introduced a grounding agent to serve as an external commonsense knowledge', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='Takeaways. We introduced a grounding agent to serve as an external commonsense knowledge\\nsource, which significantly enhanced the assistant‚Äôs ability to make informed decisions. This proves', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='that providing necessary commonsense facts to the decision-making agent can assist it in making\\nmore informed decisions, thus effectively boosting the task success rate. AutoGen brings both', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='more informed decisions, thus effectively boosting the task success rate. AutoGen brings both\\nsimplicity and modularity when adding the grounding agent.\\n25', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='A4: Multi-Agent Coding\\nCommander\\nSafeguard\\nWriter\\n‚ë°Question, ‚ùªLog‚ù∏Code, ‚ë¶Ans‚ùπCode‚ù∫Clearance\\nUser‚ë†User Question‚ëßFinal AnswerRepeat until answering the user‚Äôs question or timeout', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='User‚ë†User Question‚ëßFinal AnswerRepeat until answering the user‚Äôs question or timeout\\nFigure 11: Our re-implementation of OptiGuide with AutoGen streamlining agents‚Äô interactions.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='Figure 11: Our re-implementation of OptiGuide with AutoGen streamlining agents‚Äô interactions.\\nThe Commander receives user questions (e.g., What if we prohibit shipping from supplier 1 to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='The Commander receives user questions (e.g., What if we prohibit shipping from supplier 1 to\\nroastery 2?) and coordinates with the Writer and Safeguard. The Writer crafts the code and inter-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='roastery 2?) and coordinates with the Writer and Safeguard. The Writer crafts the code and inter-\\npretation, the Safeguard ensures safety (e.g., not leaking information, no malicious code), and the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='pretation, the Safeguard ensures safety (e.g., not leaking information, no malicious code), and the\\nCommander executes the code. If issues arise, the process can repeat until resolved. Shaded circles', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='Commander executes the code. If issues arise, the process can repeat until resolved. Shaded circles\\nrepresent steps that may be repeated multiple times.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='represent steps that may be repeated multiple times.\\nDetailed Workflow. The workflow can be described as follows. The end user initiates the in-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='Detailed Workflow. The workflow can be described as follows. The end user initiates the in-\\nteraction by posing a question, such as ‚ÄúWhat if we prohibit shipping from supplier 1 to roastery', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='teraction by posing a question, such as ‚ÄúWhat if we prohibit shipping from supplier 1 to roastery\\n2?‚Äù, marked by\\n1 to the Commander agent. The Commander manages and coordinates with two', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='2?‚Äù, marked by\\n1 to the Commander agent. The Commander manages and coordinates with two\\nLLM-based assistant agents: the Writer and the Safeguard. Apart from directing the flow of commu-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='LLM-based assistant agents: the Writer and the Safeguard. Apart from directing the flow of commu-\\nnication, the Commander has the responsibility of handling memory tied to user interactions. This', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='nication, the Commander has the responsibility of handling memory tied to user interactions. This\\ncapability enables the Commander to capture and retain valuable context regarding the user‚Äôs ques-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='capability enables the Commander to capture and retain valuable context regarding the user‚Äôs ques-\\ntions and their corresponding responses. Such memory is subsequently shared across the system,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='tions and their corresponding responses. Such memory is subsequently shared across the system,\\nempowering the other agents with context from prior user interactions and ensuring more informed', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='empowering the other agents with context from prior user interactions and ensuring more informed\\nand relevant responses.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='and relevant responses.\\nIn this orchestrated process, the Writer, who combines the functions of a ‚ÄúCoder‚Äù and an ‚ÄúInter-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='preter‚Äù as defined in (Li et al., 2023a), will craft code and also interpret execution output logs. For in-\\nstance, during code writing (\\n2 and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='stance, during code writing (\\n2 and\\n3 ), the Writer may craft code ‚Äúmodel.addConstr(x[‚Äòsupplier1‚Äô,\\n‚Äòroastery2‚Äô] == 0, ‚Äòprohibit‚Äô)‚Äù to add an additional constraint to answer the user‚Äôs question.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='‚Äòroastery2‚Äô] == 0, ‚Äòprohibit‚Äô)‚Äù to add an additional constraint to answer the user‚Äôs question.\\nAfter receiving the code, the Commander will communicate with the Safeguard to screen the code', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='After receiving the code, the Commander will communicate with the Safeguard to screen the code\\nand ascertain its safety (\\n4 ); once the code obtains the Safeguard‚Äôs clearance, marked by\\n5 , the', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='and ascertain its safety (\\n4 ); once the code obtains the Safeguard‚Äôs clearance, marked by\\n5 , the\\nCommander will use external tools (e.g., Python) to execute the code and request the Writer to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='Commander will use external tools (e.g., Python) to execute the code and request the Writer to\\ninterpret the execution results for the user‚Äôs question (\\n6 and\\n7 ). For instance, the writer may', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='interpret the execution results for the user‚Äôs question (\\n6 and\\n7 ). For instance, the writer may\\nsay ‚Äúif we prohibit shipping from supplier 1 to roastery 2, the total cost would increase by 10.5%.‚Äù', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='Bringing this intricate process full circle, the Commander furnishes the user with the concluding\\nanswer (\\n8 ).', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='answer (\\n8 ).\\nIf at a point there is an exception - either a security red flag raised by Safeguard (in\\n5 ) or code', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='5 ) or code\\nexecution failures within Commander, the Commander redirects the issue back to the Writer with\\nessential information in logs (\\n6 ). So, the process from\\n3 to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='essential information in logs (\\n6 ). So, the process from\\n3 to\\n6 might be repeated multiple times,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='6 ). So, the process from\\n3 to\\n6 might be repeated multiple times,\\nuntil each user query receives a thorough and satisfactory resolution or until the timeout. This entire', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='complex workflow of multi-agent interaction is elegantly managed via AutoGen .\\nThe core workflow code for OptiGuide was reduced from over 430 lines to 100 lines using AutoGen ,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='The core workflow code for OptiGuide was reduced from over 430 lines to 100 lines using AutoGen ,\\nleading to significant productivity improvement. The new agents are customizable, conversable, and', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='leading to significant productivity improvement. The new agents are customizable, conversable, and\\ncan autonomously manage their chat memories. This consolidation allows the coder and interpreter', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='roles to merge into a single ‚ÄúWriter‚Äù agent, resulting in a clean, concise, and intuitive implementation\\nthat is easier to maintain.\\n26', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='Manual Evaluation Comparing ChatGPT + Code Interpreter and AutoGen -based OptiGuide.\\nChatGPT + Code Interpreter is unable to execute code with private or customized dependencies (e.g.,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='ChatGPT + Code Interpreter is unable to execute code with private or customized dependencies (e.g.,\\nGurobi), which means users need to have engineering expertise to manually handle multiple steps,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='Gurobi), which means users need to have engineering expertise to manually handle multiple steps,\\ndisrupting the workflow and increasing the chance for mistakes. If users lack access or expertise,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='disrupting the workflow and increasing the chance for mistakes. If users lack access or expertise,\\nthe burden falls on supporting engineers, increasing their on-call time.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='the burden falls on supporting engineers, increasing their on-call time.\\nWe carried out a user study that juxtaposed OpenAI‚Äôs ChatGPT coupled with a Code Interpreter', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='We carried out a user study that juxtaposed OpenAI‚Äôs ChatGPT coupled with a Code Interpreter\\nagainst AutoGen -based OptiGuide. The study focused on a coffee supply chain scenario, and an', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='against AutoGen -based OptiGuide. The study focused on a coffee supply chain scenario, and an\\nexpert Python programmer with proficiency in Gurobi participated in the test. We evaluated both', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='expert Python programmer with proficiency in Gurobi participated in the test. We evaluated both\\nsystems based on 10 randomly selected questions, measuring time and accuracy. While both sys-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='systems based on 10 randomly selected questions, measuring time and accuracy. While both sys-\\ntems answered 8 questions correctly, the Code Interpreter was significantly slower than OptiGuide', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='tems answered 8 questions correctly, the Code Interpreter was significantly slower than OptiGuide\\nbecause the former requires more manual intervention. On average, users needed to spend 4 minutes', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='because the former requires more manual intervention. On average, users needed to spend 4 minutes\\nand 35 seconds to solve problems with the Code Interpreter, with a standard deviation of approxi-', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='and 35 seconds to solve problems with the Code Interpreter, with a standard deviation of approxi-\\nmately 2.5 minutes. In contrast, OptiGuide‚Äôs average problem-solving time was around 1.5 minutes,', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='mately 2.5 minutes. In contrast, OptiGuide‚Äôs average problem-solving time was around 1.5 minutes,\\nmost of which was spent waiting for responses from the GPT-4 model. This indicates a 3x saving', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='most of which was spent waiting for responses from the GPT-4 model. This indicates a 3x saving\\non the user‚Äôs time with AutoGen -based OptiGuide.', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='on the user‚Äôs time with AutoGen -based OptiGuide.\\nWhile using ChatGPT + Code Interpreter, users had to read through the code and instructions to', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_search_indexer_client.load_and_chunck_text_by_character_from_pdf(\n",
    "    pdf_path=pdf_path, chunk_size=200, chunk_overlap=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 20:38:13,644 - micro - MainProcess - INFO     Reading PDF file from https://arxiv.org/pdf/2308.08155.pdf. (langchain_integration.py:read_and_load_pdf:366)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='AutoGen : Enabling Next-Gen LLM\\nApplications via Multi-Agent Conversation\\nQingyun Wu‚Ä†, Gagan Bansal‚àó, Jieyu Zhang¬±, Yiran Wu‚Ä†, Beibin Li‚àó\\nErkang Zhu‚àó, Li Jiang‚àó, Xiaoyun Zhang‚àó, Shaokun Zhang‚Ä†, Jiale Liu‚àì\\nAhmed Awadallah‚àó, Ryen W. White‚àó, Doug Burger‚àó, Chi Wang‚àó1\\n‚àóMicrosoft Research,‚Ä†Pennsylvania State University\\n¬±University of Washington,‚àìXidian University\\nAgent CustomizationConversable agent\\nFlexible Conversation Patterns\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\nHierarchical chatJoint chatMulti-Agent Conversations‚Ä¶Execute the following code‚Ä¶\\nGot it! Here is the revised code ‚Ä¶No, please plot % change!Plot a chart of META and TESLA stock price change YTD.\\nOutput:$Month\\nOutput:%MonthError package yfinanceis not installed\\nSorry! Please first pip install yfinanceand then execute the code\\nInstalling‚Ä¶\\nExample Agent Chat\\nFigure 1: AutoGen enables diverse LLM-based applications using multi-agent conversations. (Left)\\nAutoGen agents are conversable, customizable, and can be based on LLMs, tools, humans, or even\\na combination of them. (Top-middle) Agents can converse to solve tasks. (Right) They can form\\na chat, potentially with humans in the loop. (Bottom-middle) The framework supports flexible\\nconversation patterns.\\nAbstract\\nAutoGen2is an open-source framework that allows developers to build LLM ap-\\nplications via multiple agents that can converse with each other to accomplish\\ntasks. AutoGen agents are customizable, conversable , and can operate in vari-\\nous modes that employ combinations of LLMs, human inputs, and tools. Using\\nAutoGen , developers can also flexibly define agent interaction behaviors. Both\\nnatural language and computer code can be used to program flexible conversation\\npatterns for different applications. AutoGen serves as a generic framework for\\nbuilding diverse applications of various complexities and LLM capacities. Em-\\npirical studies demonstrate the effectiveness of the framework in many example\\napplications, with domains ranging from mathematics, coding, question answer-\\ning, operations research, online decision-making, entertainment, etc.\\n1Corresponding author. Email: auto-gen@outlook.com\\n2https://github.com/microsoft/autogenarXiv:2308.08155v2  [cs.AI]  3 Oct 2023', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 0}),\n",
       " Document(page_content='1 Introduction\\nLarge language models (LLMs) are becoming a crucial building block in developing powerful agents\\nthat utilize LLMs for reasoning, tool usage, and adapting to new observations (Yao et al., 2022; Xi\\net al., 2023; Wang et al., 2023b) in many real-world tasks. Given the expanding tasks that could\\nbenefit from LLMs and the growing task complexity, an intuitive approach to scale up the power of\\nagents is to use multiple agents that cooperate. Prior work suggests that multiple agents can help\\nencourage divergent thinking (Liang et al., 2023), improve factuality and reasoning (Du et al., 2023),\\nand provide validation (Wu et al., 2023). In light of the intuition and early evidence of promise, it is\\nintriguing to ask the following question: how can we facilitate the development of LLM applications\\nthat could span a broad spectrum of domains and complexities based on the multi-agent approach?\\nOur insight is to use multi-agent conversations to achieve it. There are at least three reasons con-\\nfirming its general feasibility and utility thanks to recent advances in LLMs: First, because chat-\\noptimized LLMs (e.g., GPT-4) show the ability to incorporate feedback, LLM agents can cooperate\\nthrough conversations with each other or human(s), e.g., a dialog where agents provide and seek rea-\\nsoning, observations, critiques, and validation. Second, because a single LLM can exhibit a broad\\nrange of capabilities (especially when configured with the correct prompt and inference settings),\\nconversations between differently configured agents can help combine these broad LLM capabilities\\nin a modular and complementary manner. Third, LLMs have demonstrated ability to solve complex\\ntasks when the tasks are broken into simpler subtasks. Multi-agent conversations can enable this\\npartitioning and integration in an intuitive manner. How can we leverage the above insights and\\nsupport different applications with the common requirement of coordinating multiple agents, poten-\\ntially backed by LLMs, humans, or tools exhibiting different capacities? We desire a multi-agent\\nconversation framework with generic abstraction and effective implementation that has the flexibil-\\nity to satisfy different application needs. Achieving this requires addressing two critical questions:\\n(1) How can we design individual agents that are capable, reusable, customizable, and effective in\\nmulti-agent collaboration? (2) How can we develop a straightforward, unified interface that can\\naccommodate a wide range of agent conversation patterns? In practice, applications of varying\\ncomplexities may need distinct sets of agents with specific capabilities, and may require different\\nconversation patterns, such as single- or multi-turn dialogs, different human involvement modes, and\\nstatic vs. dynamic conversation. Moreover, developers may prefer the flexibility to program agent\\ninteractions in natural language or code. Failing to adequately address these two questions would\\nlimit the framework‚Äôs scope of applicability and generality.\\nWhile there is contemporaneous exploration of multi-agent approaches,3we present AutoGen , a\\ngeneralized multi-agent conversation framework (Figure 1), based on the following new concepts.\\n1Customizable and conversable agents. AutoGen uses a generic design of agents that can lever-\\nage LLMs, human inputs, tools, or a combination of them. The result is that developers can\\neasily and quickly create agents with different roles (e.g., agents to write code, execute code,\\nwire in human feedback, validate outputs, etc.) by selecting and configuring a subset of built-in\\ncapabilities. The agent‚Äôs backend can also be readily extended to allow more custom behaviors.\\nTo make these agents suitable for multi-agent conversation, every agent is made conversable ‚Äì\\nthey can receive, react, and respond to messages. When configured properly, an agent can hold\\nmultiple turns of conversations with other agents autonomously or solicit human inputs at cer-\\ntain rounds, enabling human agency and automation. The conversable agent design leverages the\\nstrong capability of the most advanced LLMs in taking feedback and making progress via chat\\nand also allows combining capabilities of LLMs in a modular fashion. (Section 2.1)\\n2Conversation programming. A fundamental insight of AutoGen is to simplify and unify com-\\nplex LLM application workflows as multi-agent conversations. So AutoGen adopts a program-\\nming paradigm centered around these inter-agent conversations. We refer to this paradigm as\\nconversation programming , which streamlines the development of intricate applications via two\\nprimary steps: (1) defining a set of conversable agents with specific capabilities and roles (as\\ndescribed above); (2) programming the interaction behavior between agents via conversation-\\ncentric computation andcontrol . Both steps can be achieved via a fusion of natural and pro-\\ngramming languages to build applications with a wide range of conversation patterns and agent\\nbehaviors. AutoGen provides ready-to-use implementations and also allows easy extension and\\nexperimentation for both steps. (Section 2.2)\\n3We refer to Appendix A for a detailed discussion.\\n2', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 1}),\n",
       " Document(page_content='AutoGen also provides a collection of multi-agent applications created using conversable agents\\nand conversation programming. These applications demonstrate how AutoGen can easily support\\napplications of various complexities and LLMs of various capabilities. Moreover, we perform both\\nevaluation on benchmarks and a pilot study of new applications. The results show that AutoGen can\\nhelp achieve outstanding performance on many tasks, and enable innovative ways of using LLMs,\\nwhile reducing development effort. (Section 3 and Appendix D)\\n2 The AutoGen Framework\\nTo reduce the effort required for developers to create complex LLM applications across various do-\\nmains, a core design principle of AutoGen is to streamline and consolidate multi-agent workflows\\nusing multi-agent conversations. This approach also aims to maximize the reusability of imple-\\nmented agents. This section introduces the two key concepts of AutoGen : conversable agents and\\nconversation programming.\\n2.1 Conversable Agents\\nInAutoGen , aconversable agent is an entity with a specific role that can pass messages to send and\\nreceive information to and from other conversable agents, e.g., to start or continue a conversation. It\\nmaintains its internal context based on sent and received messages and can be configured to possess\\na set of capabilities, e.g., enabled by LLMs, tools, or human input, etc. The agents can act according\\nto programmed behavior patterns described next.\\nAgent capabilities powered by LLMs, humans, and tools. Since an agent‚Äôs capabilities directly\\ninfluence how it processes and responds to messages, AutoGen allows flexibility to endow its agents\\nwith various capabilities. AutoGen supports many common composable capabilities for agents,\\nincluding 1) LLMs. LLM-backed agents exploit many capabilities of advanced LLMs such as role\\nplaying, implicit state inference and progress making conditioned on conversation history, providing\\nfeedback, adapting from feedback, and coding. These capabilities can be combined in different ways\\nvia novel prompting techniques4to increase an agent‚Äôs skill and autonomy. AutoGen also offers\\nenhanced LLM inference features such as result caching, error handling, message templating, etc.,\\nvia an enhanced LLM inference layer. 2) Humans. Human involvement is desired or even essential\\nin many LLM applications. AutoGen lets a human participate in agent conversation via human-\\nbacked agents, which could solicit human inputs at certain rounds of a conversation depending on\\nthe agent configuration. The default user proxy agent allows configurable human involvement levels\\nand patterns, e.g., frequency and conditions for requesting human input including the option for\\nhumans to skip providing input. 3) Tools. Tool-backed agents have the capability to execute tools\\nvia code execution or function execution. For example, the default user proxy agent in AutoGen is\\nable to execute code suggested by LLMs, or make LLM-suggested function calls.\\nAgent customization and cooperation. Based on application-specific needs, each agent can be\\nconfigured to have a mix of basic back-end types to display complex behavior in multi-agent con-\\nversations. AutoGen allows easy creation of agents with specialized capabilities and roles by reusing\\nor extending the built-in agents. The yellow-shaded area of Figure 2 provides a sketch of the built-in\\nagents in AutoGen . The ConversableAgent class is the highest-level agent abstraction and, by\\ndefault, can use LLMs, humans, and tools. The AssistantAgent andUserProxyAgent are two\\npre-configured ConversableAgent subclasses, each representing a common usage mode, i.e., act-\\ning as an AI assistant (backed by LLMs) and acting as a human proxy to solicit human input or\\nexecute code/function calls (backed by humans and/or tools).\\nIn the example on the right-hand side of Figure 1, an LLM-backed assistant agent and a tool- and\\nhuman-backed user proxy agent are deployed together to tackle a task. Here, the assistant agent\\ngenerates a solution with the help of LLMs and passes the solution to the user proxy agent. Then,\\nthe user proxy agent solicits human inputs or executes the assistant‚Äôs code and passes the results as\\nfeedback back to the assistant.\\n4Appendix C presents an example of such novel prompting techniques which empowers the default LLM-\\nbacked assistant agent in AutoGen to converse with other agents in multi-step problem solving.\\n3', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 2}),\n",
       " Document(page_content='2 Initiate Conversations:A.initiate_chat(‚ÄúPlot a chart of META and TESLA stock price change YTD.‚Äù, B)\\nAssistant BUser Proxy AAutoGenAgents\\nDeveloper Code# This funcwill be invoked in generate_replyA.register_reply(B,  reply_func_A2B)def reply_func_A2B(msg):ouput= input_from_human()‚Ä¶if not ouput:if msg includes code:output = execute(msg)return outputConversableAgent\\nAssistantAgentUserProxyAgenthuman_input_mode= ‚ÄúNEVER‚Äùcode_execution_config= FalseDEFAULT_SYSTEM_MESSAGE = ‚ÄúYou are a helpful AI assistant‚Ä¶In the following cases, suggest python code‚Ä¶‚Äùhuman_input_mode=‚ÄúALWAYS‚Äù\\nGroupChatManagerhuman_input_mode= ‚ÄúNEVER‚Äùgroup_chat= [              ] \\n# Note: when no reply funcis registered, a list of default reply functions will be used. Agent Customization:\\nProgram ExecutionPlot a chart of META and TESLA stock price change YTD.Execute the following code‚Ä¶sendreceivereceiveConversation-Centric Computationgenerate_replyError: package yfinanceis not installedsendgenerate_replySorry! Please first pip install yfinanceand then executeConversation-Driven Control Flowgenerate_replyThe Resulting Automated Agent Chat:‚Ä¶1.2 Register a Custom Reply Func:1.1 Define Agents:Unified Conversation Interfaces:‚Ä¢send‚Ä¢receive ‚Ä¢generate_reply\\nFigure 2: Illustration of how to use AutoGen to program a multi-agent conversation. The top sub-\\nfigure illustrates the built-in agents provided by AutoGen , which have unified conversation interfaces\\nand can be customized. The middle sub-figure shows an example of using AutoGen to develop\\na two-agent system with a custom reply function. The bottom sub-figure illustrates the resulting\\nautomated agent chat from the two-agent system during program execution.\\nBy allowing custom agents that can converse with each other, conversable agents in AutoGen serve\\nas a useful building block. However, to develop applications where agents make meaningful progress\\non tasks, developers also need to be able to specify and mold these multi-agent conversations.\\n2.2 Conversation Programming\\nAs a solution to the above problem, AutoGen utilizes conversation programming , a paradigm that\\nconsiders two concepts: the first is computation ‚Äì the actions agents take to compute their response\\nin a multi-agent conversation. And the second is control flow ‚Äì the sequence (or conditions) un-\\nder which these computations happen. As we will show in the applications section, the ability to\\nprogram these helps implement many flexible multi-agent conversation patterns. In AutoGen , these\\ncomputations are conversation-centric. An agent takes actions relevant to the conversations it is\\ninvolved in and its actions result in message passing for consequent conversations (unless a termina-\\ntion condition is satisfied). Similarly, control flow is conversation-driven ‚Äì the participating agents‚Äô\\ndecisions on which agents to send messages to and the procedure of computation are functions of the\\ninter-agent conversation. This paradigm helps one to reason intuitively about a complex workflow\\nas agent action taking and conversation message-passing between agents.\\nFigure 2 provides a simple illustration. The bottom sub-figure shows how individual agents perform\\ntheir role-specific, conversation-centric computations to generate responses (e.g., via LLM inference\\ncalls and code execution). The task progresses through conversations displayed in the dialog box.\\nThe middle sub-figure demonstrates a conversation-based control flow. When the assistant receives\\na message, the user proxy agent typically sends the human input as a reply. If there is no input, it\\nexecutes any code in the assistant‚Äôs message instead.\\n4', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 3}),\n",
       " Document(page_content='AutoGen features the following design patterns to facilitate conversation programming:\\n1.Unified interfaces and auto-reply mechanisms for automated agent chat. Agents in\\nAutoGen have unified conversation interfaces for performing the corresponding conversation-\\ncentric computation, including a send/receive function for sending/receiving messages and a\\ngenerate reply function for taking actions and generating a response based on the received\\nmessage. AutoGen also introduces and by default adopts an agent auto-reply mechanism to\\nrealize conversation-driven control: Once an agent receives a message from another agent, it au-\\ntomatically invokes generate reply and sends the reply back to the sender unless a termination\\ncondition is satisfied. AutoGen provides built-in reply functions based on LLM inference, code\\nor function execution, or human input. One can also register custom reply functions to customize\\nthe behavior pattern of an agent, e.g., chatting with another agent before replying to the sender\\nagent. Under this mechanism, once the reply functions are registered, and the conversation is\\ninitialized, the conversation flow is naturally induced, and thus the agent conversation proceeds\\nnaturally without any extra control plane, i.e., a special module that controls the conversation\\nflow. For example, with the developer code in the blue-shaded area (marked ‚ÄúDeveloper Code‚Äù)\\nof Figure 2, one can readily trigger the conversation among the agents, and the conversation\\nwould proceed automatically, as shown in the dialog box in the grey shaded area (marked ‚ÄúPro-\\ngram Execution‚Äù) of Figure 2. The auto-reply mechanism provides a decentralized, modular, and\\nunified way to define the workflow.\\n2.Control by fusion of programming and natural language. AutoGen allows the usage of\\nprogramming and natural language in various control flow management patterns: 1) Natural-\\nlanguage control via LLMs. InAutoGen , one can control the conversation flow by prompting\\nthe LLM-backed agents with natural language. For instance, the default system message of the\\nbuilt-in AssistantAgent inAutoGen uses natural language to instruct the agent to fix errors\\nand generate code again if the previous result indicates there are errors. It also guides the agent\\nto confine the LLM output to certain structures, making it easier for other tool-backed agents to\\nconsume. For example, instructing the agent to reply with ‚ÄúTERMINATE‚Äù when all tasks are\\ncompleted to terminate the program. More concrete examples of natural language controls can\\nbe found in Appendix C. 2) Programming-language control. InAutoGen , Python code can be\\nused to specify the termination condition, human input mode, and tool execution logic, e.g., the\\nmax number of auto replies. One can also register programmed auto-reply functions to control\\nthe conversation flow with Python code, as shown in the code block identified as ‚ÄúConversation-\\nDriven Control Flow‚Äù in Figure 2. 3) Control transition between natural and programming\\nlanguage. AutoGen also supports flexible control transition between natural and programming\\nlanguage. One can achieve transition from code to natural-language control by invoking an LLM\\ninference containing certain control logic in a customized reply function; or transition from nat-\\nural language to code control via LLM-proposed function calls (Eleti et al., 2023).\\nIn the conversation programming paradigm, one can realize multi-agent conversations of diverse\\npatterns. In addition to static conversation with predefined flow, AutoGen also supports dynamic\\nconversation flows with multiple agents. AutoGen provides two general ways to achieve this: 1)\\nCustomized generate reply function: within the customized generate reply function, one\\nagent can hold the current conversation while invoking conversations with other agents depending\\non the content of the current message and context. 2) Function calls: In this approach, LLM decides\\nwhether or not to call a particular function depending on the conversation status. By messaging\\nadditional agents in the called functions, the LLM can drive dynamic multi-agent conversation. In\\naddition, AutoGen supports more complex dynamic group chat via built-in GroupChatManager ,\\nwhich can dynamically select the next speaker and then broadcast its response to other agents. We\\nelaborate on this feature and its application in Section 3. We provide implemented working systems\\nto showcase all these different patterns, with some of them visualized in Figure 3.\\n3 Applications of AutoGen\\nWe demonstrate six applications using AutoGen (see Figure 3) to illustrate its potential in simplify-\\ning the development of high-performance multi-agent applications. These applications are selected\\nbased on their real-world relevance (A1, A2, A4, A5, A6), problem difficulty and solving capabil-\\nities enabled by AutoGen (A1, A2, A3, A4), and innovative potential (A5, A6). Together, these\\ncriteria showcase AutoGen ‚Äôs role in advancing the LLM-application landscape.\\n5', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 4}),\n",
       " Document(page_content='A1. Math Problem Solving\\nA4. Multi-agent CodingCommander\\nSafeguard\\nWriter\\nA6. Conversational ChessA2. Retrieval-augmented ChatRetrieval-augmentedAssistantRetrieval-augmentedUser Proxy\\nChess Board\\nHuman/AI Chess Player A\\nHuman/AI Chess Player B\\nStudent\\nAssistant\\nAssistantExpert\\nAsk  expert\\nBroadcast\\nManager\\nSpeak\\nA5. Dynamic Group Chat\\nALFWorldExecutorAssistant\\nGrounding Agent\\nA3. ALF ChatFigure 3: Six examples of diverse applications built using AutoGen . Their conversation patterns\\nshow AutoGen ‚Äôs flexibility and power.\\nA1: Math Problem Solving\\nMathematics is a foundational discipline and the promise of leveraging LLMs to assist with math\\nproblem solving opens up a new plethora of applications and avenues for exploration, including per-\\nsonalized AI tutoring, AI research assistance, etc. This section demonstrates how AutoGen can help\\ndevelop LLM applications for math problem solving, showcasing strong performance and flexibility\\nin supporting various problem-solving paradigms.\\n(Scenario 1 ) We are able to build a system for autonomous math problem solving by directly reusing\\ntwo built-in agents from AutoGen . We evaluate our system and several alternative approaches,\\nincluding open-source methods such as Multi-Agent Debate (Liang et al., 2023), LangChain Re-\\nAct (LangChain, 2023), vanilla GPT-4, and commercial products ChatGPT + Code Interpreter, and\\nChatGPT + Plugin (Wolfram Alpha), on the MATH (Hendrycks et al., 2021) dataset and summarize\\nthe results in Figure 4a. We perform evaluations over 120 randomly selected level-5 problems and\\non the entire5test dataset from MATH. The results show that the built-in agents from AutoGen al-\\nready yield better performance out of the box compared to the alternative approaches, even including\\nthe commercial ones. ( Scenario 2 ) We also showcase a human-in-the-loop problem-solving process\\nwith the help of AutoGen . To incorporate human feedback with AutoGen , one only needs to set\\nhuman input mode=‚ÄòALWAYS‚Äô in the UserProxyAgent of the system in scenario 1. We demon-\\nstrate that this system can effectively incorporate human inputs to solve challenging problems that\\ncannot be solved without humans. ( Scenario 3 ) We further demonstrate a novel scenario where\\nmultiple human users can participate in the conversations during the problem-solving process. Our\\nexperiments and case studies for these scenarios show that AutoGen enables better performance or\\nnew experience compared to other solutions we experimented with. Due to the page limit, details of\\nthe evaluation, including case studies in three scenarios are in Appendix D.\\nA2: Retrieval-Augmented Code Generation and Question Answering\\nRetrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic\\nlimitations of LLMs by incorporating external documents. In this section, we employ AutoGen to\\nbuild a Retrieval-Augmented Generation (RAG) system (Lewis et al., 2020; Parvez et al., 2021)\\nnamed Retrieval-augmented Chat. The system consists of two agents: a Retrieval-augmented User\\nProxy agent and a Retrieval-augmented Assistant agent, both of which are extended from built-in\\nagents from AutoGen . The Retrieval-augmented User Proxy includes a vector database (Chroma,\\n5We did not evaluate ChatGPT on the whole dataset since it requires substantial manual effort and is re-\\nstricted by its hourly message-number limitation. Multi-agent debate and LangChain ReAct were also not\\nevaluated since they underperformed vanilla GPT-4 on the smaller test set.\\n6', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 5}),\n",
       " Document(page_content='AutoGen ChatGPT\\n+CodeChatGPT\\n+PluginGPT-4 Multi-Agent\\nDebateLangChain\\nReAct\\nMethods01020304050607080Success Ratio (%)52.5%\\n48.33%\\n45.0%\\n30.0%\\n26.67%\\n23.33%69.48%\\n55.18%120 Level-5 problems\\nWhole Dataset(a) A1: Performance on MATH (w/ GPT-4).\\nF1 Recall\\nMetrics01020304050607080Percentage (%)25.88%66.65%\\n15.12%58.56%\\n22.79%62.59%AutoGen\\nAuotGen W/O interactive retrieval\\nDPR (b) A2: Q&A tasks (w/ GPT-3.5).\\nAutoGen (3 agent) AutoGen (2 agent) ReAct\\nMethods020406080100Success Ratio (%)69%\\n54% 54%77%\\n63%66%Average\\nBest of 3\\n(c) A3: Performance on ALFWorld.\\nF1 Recall\\nMetrics020406080100Percentage (%)96.00%98.00%\\n88.00%\\n78.00%83.00%\\n72.00%\\n48.00%\\n32.00%Multi-GPT4\\nSingle-GPT4\\nMulti-GPT3.5\\nSingle-GPT3.5 (d) A4: Performance on OptiGuide.\\nFigure 4: Performance on four applications A1-A4. (a) shows that AutoGen agents can be used\\nout of the box to achieve the most competitive performance on math problem solving tasks; (b)\\nshows that AutoGen can be used to realize effective retrieval augmentation and realize a novel\\ninteractive retrieval feature to boost performance on Q&A tasks; (c) shows that AutoGen can be used\\nto introduce a three-agent system with a grounding agent to improve performance on ALFWorld;\\n(d) shows that a multi-agent design is helpful in boosting performance in coding tasks that need\\nsafeguards.\\n2023) with SentenceTransformers (Reimers & Gurevych, 2019) as the context retriever. A detailed\\nworkflow description of the Retrieval-augmented Chat is provided in Appendix D.\\nWe evaluate Retrieval-augmented Chat in both question-answering and code-generation scenarios.\\n(Scenario 1 ) We first perform an evaluation regarding natural question answering on the Natural\\nQuestions dataset (Kwiatkowski et al., 2019) and report results in Figure 4b. In this evaluation, we\\ncompare our system with DPR (Dense Passage Retrieval) following an existing evaluation6prac-\\ntice (Adlakha et al., 2023). Leveraging the conversational design and natural-language control,\\nAutoGen introduces a novel interactive retrieval feature in this application: whenever the retrieved\\ncontext does not contain the information, instead of terminating, the LLM-based assistant would\\nreply ‚Äú Sorry, I cannot find any information about... UPDATE CONTEXT. ‚Äù which will invoke more\\nretrieval attempts. We conduct an ablation study in which we prompt the assistant agent to say ‚ÄúI\\ndon‚Äôt know‚Äù instead of ‚ÄúUPDATE CONTEXT. ‚Äù in cases where relevant information is not found,\\nand report results in Figure 4b. The results show that the interactive retrieval mechanism indeed\\nplays a non-trivial role in the process. We give a concrete example and results using this appealing\\nfeature in Appendix D. ( Scenario 2 ) We further demonstrate how Retrieval-augmented Chat aids in\\ngenerating code based on a given codebase that contains code not included in GPT-4‚Äôs training data.\\nEvaluation and demonstration details for both scenarios are included in Appendix D.\\n6The results of DPR with GPT-3.5 shown in Figure 4b are from (Adlakha et al., 2023). We use GPT-3.5 as\\na shorthand for GPT-3.5-turbo.\\n7', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 6}),\n",
       " Document(page_content='A3: Decision Making in Text World Environments\\nIn this subsection, we demonstrate how AutoGen can be used to develop effective applications that\\ninvolve interactive or online decision making. We perform the study using the ALFWorld (Shridhar\\net al., 2021) benchmark, which includes a diverse collection of synthetic language-based interactive\\ndecision-making tasks in household environments.\\nWith AutoGen , we implemented a two-agent system to solve tasks from ALFWorld. It consists of\\nan LLM-backed assistant agent responsible for suggesting plans to conduct a task and an executor\\nagent responsible for executing actions in the ALFWorld environments. This system integrates Re-\\nAct prompting (Yao et al., 2022), and is able to achieve similar performance. A common challenge\\nencountered in both ReAct and the AutoGen -based two-agent system is their occasional inability to\\nleverage basic commonsense knowledge about the physical world. This deficiency can lead to the\\nsystem getting stuck in a loop due to repetitive errors. Fortunately, the modular design of AutoGen\\nallows us to address this issue effectively: With AutoGen , we are able to introduce a grounding\\nagent, which supplies crucial commonsense knowledge‚Äìsuch as ‚ÄúYou must find and take the object\\nbefore you can examine it. You must go to where the target object is before you can use it. ‚Äù ‚Äìwhenever\\nthe system exhibits early signs of recurring errors. It significantly enhances the system‚Äôs ability to\\navoid getting entangled in error loops. We compare the task-solving performance of the two variants\\nof our system with GPT-3.5-turbo and ReAct7on the 134 unseen tasks from ALFWorld and report\\nresults in Figure 4c. The results show that introducing a grounding agent could bring in a 15%\\nperformance gain on average. Upon examining the systems‚Äô outputs, we observe that the grounding\\nagent, by delivering background commonsense knowledge at the right junctures, significantly miti-\\ngated the tendency of the system to persist with a flawed plan, thereby avoiding the creation of error\\nloops. For an example trajectory comparing the systems see Appendix D, Figure 10.\\nA4: Multi-Agent Coding\\nIn this subsection, we use AutoGen to build a multi-agent coding system based on OptiGuide (Li\\net al., 2023a), a system that excels at writing code to interpret optimization solutions and answer\\nuser questions, such as exploring the implications of changing a supply-chain decision or under-\\nstanding why the optimizer made a particular choice. The second sub-figure of Figure 3 shows the\\nAutoGen -based implementation. The workflow is as follows: the end user sends questions, such as\\n‚ÄúWhat if we prohibit shipping from supplier 1 to roastery 2? ‚Äù to the Commander agent. The Com-\\nmander coordinates with two assistant agents, including the Writer and the Safeguard, to answer\\nthe question. The Writer will craft code and send the code to the Commander. After receiving the\\ncode, the Commander checks the code safety with the Safeguard; if cleared, the Commander will\\nuse external tools (e.g., Python) to execute the code, and request the Writer to interpret the execution\\nresults. For instance, the writer may say ‚Äú if we prohibit shipping from supplier 1 to roastery 2, the\\ntotal cost would increase by 10.5%. ‚Äù The Commander then provides this concluding answer to the\\nend user. If, at a particular step, there is an exception, e.g., security red flag raised by Safeguard, the\\nCommander redirects the issue back to the Writer with debugging information. The process might\\nbe repeated multiple times until the user‚Äôs question is answered or timed-out.\\nWith AutoGen the core workflow code for OptiGuide was reduced from over 430 lines to 100 lines,\\nleading to significant productivity improvement. We provide a detailed comparison of user expe-\\nrience with ChatGPT+Code Interpreter and AutoGen -based OptiGuide in Appendix D, where we\\nshow that AutoGen -based OptiGuide could save around 3x of user‚Äôs time and reduce user interac-\\ntions by 3 - 5 times on average. We also conduct an ablation showing that multi-agent abstraction is\\nnecessary. Specifically, we construct a single-agent approach where a single agent conducts both the\\ncode-writing and safeguard processes. We tested the single- and multi-agent approaches on a dataset\\nof 100 coding tasks, which is crafted to include equal numbers of safe and unsafe tasks. Evaluation\\nresults as reported in Figure 4d show that the multi-agent design boosts the F-1 score in identifying\\nunsafe code by 8% (with GPT-4) and 35% (with GPT-3.5-turbo).\\n7Results of ReAct are obtained by directly running its official code with default settings. The code uses\\ntext-davinci-003 as backend LM and does not support GPT-3.5-turbo or GPT-4.\\n8', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 7}),\n",
       " Document(page_content='A5: Dynamic Group Chat\\nAutoGen provides native support for a dynamic group chat communication pattern, in which par-\\nticipating agents share the same context and converse with the others in a dynamic manner instead\\nof following a pre-defined order. Dynamic group chat relies on ongoing conversations to guide the\\nflow of interaction among agents. These make dynamic group chat ideal for situations where col-\\nlaboration without strict communication order is beneficial. In AutoGen , the GroupChatManager\\nclass serves as the conductor of conversation among agents and repeats the following three steps:\\ndynamically selecting a speaker, collecting responses from the selected speaker, and broadcasting\\nthe message (Figure 3-A5). For the dynamic speaker-selection component, we use a role-play style\\nprompt. Through a pilot study on 12 manually crafted complex tasks, we observed that compared\\nto a prompt that is purely based on the task, utilizing a role-play prompt often leads to more effec-\\ntive consideration of both conversation context and role alignment during the problem-solving and\\nspeaker-selection process. Consequently, this leads to a higher success rate and fewer LLM calls.\\nWe include detailed results in Appendix D.\\nA6: Conversational Chess\\nUsing AutoGen , we developed Conversational Chess, a natural language interface game shown in\\nthe last sub-figure of Figure 3. It features built-in agents for players, which can be human or LLM,\\nand a third-party board agent to provide information and validate moves based on standard rules.\\nWith AutoGen , we enabled two essential features: (1) Natural, flexible, and engaging game dynam-\\nics, enabled by the customizable agent design in AutoGen . Conversational Chess supports a range\\nof game-play patterns, including AI-AI, AI-human, and human-human, with seamless switching\\nbetween these modes during a single game. An illustrative example of these entertaining game dy-\\nnamics can be found in Figure 15, Appendix D. (2) Grounding, which is a crucial aspect to maintain\\ngame integrity. During gameplay, the board agent checks each proposed move for legality; if a move\\nis invalid, the agent responds with an error, prompting the player agent to re-propose a legal move\\nbefore continuing. This process ensures that only valid moves are played and helps maintain a con-\\nsistent gaming experience. As an ablation study, we removed the board agent and instead only relied\\non a relevant prompt ‚Äúyou should make sure both you and the opponent are making legal moves‚Äù to\\nground their move. The results highlighted that without the board agent, illegitimate moves caused\\ngame disruptions. The modular design offered flexibility, allowing swift adjustments to the board\\nagent in response to evolving game rules or varying chess rule variants. A comprehensive demon-\\nstration of this ablation study is in Appendix D.\\n4 Discussion\\nWe introduced an open-source library, AutoGen , that incorporates the paradigms of conversable\\nagents and conversation programming. This library utilizes capable agents that are well-suited for\\nmulti-agent cooperation. It features a unified conversation interface among the agents, along with\\nan auto-reply mechanisms, which help establish an agent-interaction interface that capitalizes on the\\nstrengths of chat-optimized LLMs with broad capabilities while accommodating a wide range of\\napplications. AutoGen serves as a general framework for creating and experimenting with multi-\\nagent systems that can easily fulfill various practical requirements, such as reusing, customizing,\\nand extending existing agents, as well as programming conversations between them.\\nOur experiments, as detailed in Section 3, demonstrate that this approach offers numerous benefits.\\nThe adoption of AutoGen has resulted in improved performance (over state-of-the-art approaches),\\nreduced development code, and decreased manual burden for existing applications. It offers flex-\\nibility to developers, as demonstrated in A1 (scenario 3), A5, and A6, where AutoGen enables\\nmulti-agent chats to follow a dynamic pattern rather than fixed back-and-forth interactions. It allows\\nhumans to engage in activities alongside multiple AI agents in a conversational manner. Despite the\\ncomplexity of these applications (most involving more than two agents or dynamic multi-turn agent\\ncooperation), the implementation based on AutoGen remains straightforward. Dividing tasks among\\nseparate agents promotes modularity. Furthermore, since each agent can be developed, tested, and\\nmaintained separately, this approach simplifies overall development and code management.\\n9', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 8}),\n",
       " Document(page_content='Although this work is still in its early experimental stages, it paves the way for numerous future\\ndirections and research opportunities. For instance, we can explore effective integration of existing\\nagent implementations into our multi-agent framework and investigate the optimal balance between\\nautomation and human control in multi-agent workflows. As we further develop and refine AutoGen ,\\nwe aim to investigate which strategies, such as agent topology and conversation patterns, lead to the\\nmost effective multi-agent conversations while optimizing the overall efficiency, among other fac-\\ntors. While increasing the number of agents and other degrees of freedom presents opportunities for\\ntackling more complex problems, it may also introduce new safety challenges that require additional\\nstudies and careful consideration.\\nWe provide more discussion in Appendix B, including guidelines for using AutoGen and direction\\nof future work. We hope AutoGen will help improve many LLM applications in terms of speed of\\ndevelopment, ease of experimentation, and overall effectiveness and safety. We actively welcome\\ncontributions from the broader community.\\nEthics statement\\nThere are several potential ethical considerations that could arise from the development and use of\\ntheAutoGen framework.\\n‚Ä¢ Privacy and Data Protection: The framework allows for human participation in conversations\\nbetween agents. It is important to ensure that user data and conversations are protected, and that\\ndevelopers use appropriate measures to safeguard privacy.\\n‚Ä¢ Bias and Fairness: LLMs have been shown to exhibit biases present in their training data (Navigli\\net al., 2023). When using LLMs in the AutoGen framework, it is crucial to address and mitigate\\nany biases that may arise in the conversations between agents. Developers should be aware of\\npotential biases and take steps to ensure fairness and inclusivity.\\n‚Ä¢ Accountability and Transparency: As discussed in the future work section, as the framework in-\\nvolves multiple agents conversing and cooperating, it is important to establish clear accountability\\nand transparency mechanisms. Users should be able to understand and trace the decision-making\\nprocess of the agents involved in order to ensure accountability and address any potential issues\\nor biases.\\n‚Ä¢ Trust and Reliance: AutoGen leverages human understanding and intelligence while providing\\nautomation through conversations between agents. It is important to consider the impact of this\\ninteraction on user experience, trust, and reliance on AI systems. Clear communication and user\\neducation about the capabilities and limitations of the system will be essential (Cai et al., 2019).\\n‚Ä¢ Unintended Consequences: As discussed before, the use of multi-agent conversations and automa-\\ntion in complex tasks may have unintended consequences. In particular, allowing LLM agents to\\nmake changes in external environments through code execution or function calls, such as installing\\npackages, could be risky. Developers should carefully consider the potential risks and ensure that\\nappropriate safeguards are in place to prevent harm or negative outcomes.\\nAcknowledgements\\nThe work presented in this report was made possible through discussions and feedback from Peter\\nLee, Johannes Gehrke, Eric Horvitz, Steven Lucco, Umesh Madan, Robin Moeur, Piali Choud-\\nhury, Saleema Amershi, Adam Fourney, Victor Dibia, Guoqing Zheng, Corby Rosset, Ricky Loynd,\\nEce Kamar, Rafah Hosn, John Langford, Ida Momennejad, Brian Krabach, Taylor Webb, Shanka\\nSubhra Mondal, Wei-ge Chen, Robert Gruen, Yinan Li, Yue Wang, Suman Nath, Tanakorn Leesat-\\napornwongsa, Xin Wang, Shishir Patil, Tianjun Zhang, Saehan Jo, Ishai Menache, Kontantina Mel-\\nlou, Runlong Zhou, Feiran Jia, Hamed Khanpour, Hamid Palangi, Srinagesh Sharma, Julio Albinati\\nCortez, Amin Saied, Yuzhe Ma, Dujian Ding, Linyong Nan, Prateek Yadav, Shannon Shen, Ankur\\nMallick, Mark Encarnaci ¬¥on, Lars Liden, Tianwei Yue, Julia Kiseleva, Anastasia Razdaibiedina, and\\nLuciano Del Corro. Qingyun Wu would like to acknowledge the funding and research support from\\nthe College of Information Science and Technology at Penn State University.\\n10', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 9}),\n",
       " Document(page_content='References\\nVaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Eval-\\nuating correctness and faithfulness of instruction-following models for question answering. arXiv\\npreprint arXiv:2307.16877 , 2023.\\nSaleema Amershi, Dan Weld, Mihaela V orvoreanu, Adam Fourney, Besmira Nushi, Penny Col-\\nlisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. Guidelines for human-ai\\ninteraction. In Proceedings of the 2019 chi conference on human factors in computing systems ,\\n2019.\\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man ¬¥e. Con-\\ncrete problems in ai safety, 2016.\\nAutoGPT. Documentation ‚Äî auto-gpt. https://docs.agpt.co/ , 2023.\\nBabyAGI. Github ‚Äî babyagi. https://github.com/yoheinakajima/babyagi , 2023.\\nCarrie J. Cai, Samantha Winter, David F. Steiner, Lauren Wilcox, and Michael Terry. ‚Äùhello ai‚Äù:\\nUncovering the onboarding needs of medical practitioners for human-ai collaborative decision-\\nmaking. Proceedings of the ACM on Human-Computer Interaction , 2019.\\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as\\ntool makers. arXiv preprint arXiv:2305.17126 , 2023.\\nChroma. Chromadb. https://github.com/chroma-core/chroma , 2023.\\nVictor Dibia. LIDA: A tool for automatic generation of grammar-agnostic visualizations and info-\\ngraphics using large language models. In Proceedings of the 61st Annual Meeting of the Associ-\\nation for Computational Linguistics (Volume 3: System Demonstrations) , Toronto, Canada, July\\n2023. Association for Computational Linguistics.\\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt. arXiv\\npreprint arXiv:2304.07590 , 2023.\\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improv-\\ning factuality and reasoning in language models through multiagent debate. arXiv preprint\\narXiv:2305.14325 , 2023.\\nAtty Eleti, Jeff Harris, and Logan Kilpatrick. Function calling and other api updates. https:\\n//openai.com/blog/function-calling-and-other-api-updates , 2023.\\nGuidance. Guidance. https://github.com/guidance-ai/guidance , 2023.\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv\\npreprint arXiv:2103.03874 , 2021.\\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven\\nKa Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for\\nmulti-agent collaborative framework. arXiv preprint arXiv:2308.00352 , 2023.\\nEric Horvitz. Principles of mixed-initiative user interfaces. In Proceedings of the SIGCHI conference\\non Human Factors in Computing Systems , 1999.\\nHuggingFace. Transformers agent. https://huggingface.co/docs/transformers/\\ntransformers_agents , 2023.\\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.\\narXiv preprint arXiv:2303.17491 , 2023.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\\nbenchmark for question answering research. Transactions of the Association for Computational\\nLinguistics , 2019.\\n11', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 10}),\n",
       " Document(page_content='LangChain. Introduction ‚Äî langchain. https://python.langchain.com/en/latest/index.\\nhtml , 2023.\\nMike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal? end-\\nto-end learning for negotiation dialogues. arXiv preprint arXiv:1706.05125 , 2017.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich K ¬®uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¬®aschel, et al. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems ,\\n2020.\\nBeibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, and Ishai Menache. Large language\\nmodels for supply chain optimization. arXiv preprint arXiv:2307.03875 , 2023a.\\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\\nCamel: Communicative agents for ‚Äùmind‚Äù exploration of large scale language model society,\\n2023b.\\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng\\nTu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-\\nagent debate, 2023.\\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement\\nlearning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802 ,\\n2018.\\nJerry Liu. LlamaIndex, November 2022. URL https://github.com/jerryjliu/llama_index .\\nV olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-\\nstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\\narXiv:1312.5602 , 2013.\\nRoberto Navigli, Simone Conia, and Bj ¬®orn Ross. Biases in large language models: Origins, inven-\\ntory and discussion. ACM Journal of Data and Information Quality , 2023.\\nOpenAI. ChatGPT plugins. https://openai.com/blog/chatgpt-plugins , 2023.\\nJoon Sung Park, Joseph C O‚ÄôBrien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and\\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint\\narXiv:2304.03442 , 2023.\\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.\\nRetrieval augmented code generation and summarization. arXiv preprint arXiv:2108.11601 ,\\n2021.\\nShishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model\\nconnected with massive apis. arXiv preprint arXiv:2305.15334 , 2023.\\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-\\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\\nProcessing . Association for Computational Linguistics, 11 2019. URL https://arxiv.org/\\nabs/1908.10084 .\\nSemantic-Kernel. Semantic kernel. https://github.com/microsoft/semantic-kernel ,\\n2023.\\nBokui Shen, Fei Xia, Chengshu Li, Roberto Mart ¬¥ƒ±n-Mart ¬¥ƒ±n, Linxi Fan, Guanzhi Wang, Claudia\\nP¬¥erez-D‚ÄôArpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, et al. igibson 1.0: A simu-\\nlation environment for interactive tasks in large realistic scenes. In 2021 IEEE/RSJ International\\nConference on Intelligent Robots and Systems (IROS) . IEEE, 2021.\\nTianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An\\nopen-domain platform for web-based agents. In International Conference on Machine Learning .\\nPMLR, 2017.\\n12', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 11}),\n",
       " Document(page_content='Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C ÀÜot¬¥e, Yonatan Bisk, Adam Trischler, and Matthew\\nHausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In\\nProceedings of the International Conference on Learning Representations (ICLR) , 2021. URL\\nhttps://arxiv.org/abs/2010.03768 .\\nOriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,\\nMichelle Yeo, Alireza Makhzani, Heinrich K ¬®uttler, John Agapiou, Julian Schrittwieser, et al.\\nStarcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782 , 2017.\\nChi Wang, Qingyun Wu, Markus Weimer, and Erkang Zhu. Flaml: A fast and lightweight automl\\nlibrary. Proceedings of Machine Learning and Systems , 2021.\\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\\nand Anima Anandkumar. V oyager: An open-ended embodied agent with large language models.\\narXiv preprint arXiv:2305.16291 , 2023a.\\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\\narXiv preprint arXiv:2308.11432 , 2023b.\\nDaniel S. Weld and Oren Etzioni. The first law of robotics (a call to arms). In AAAI Conference on\\nArtificial Intelligence , 1994.\\nMax Woolf. Langchain problem. https://minimaxir.com/2023/07/langchain-problem/ ,\\n2023.\\nYiran Wu, Feiran Jia, Shaokun Zhang, Qingyun Wu, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat\\nLee, Richard Peng, and Chi Wang. An empirical study on challenging math problem solving with\\ngpt-4. arXiv preprint arXiv:2306.01337 , 2023.\\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe\\nWang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents:\\nA survey. arXiv preprint arXiv:2309.07864 , 2023.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 ,\\n2022.\\n13', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 12}),\n",
       " Document(page_content='A Related Work\\nWe examine existing LLM-based agent systems or frameworks that can be used to build LLM appli-\\ncations. We categorize the related work into single-agent and multi-agent systems and specifically\\nprovide a summary of differentiators comparing AutoGen with existing multi-agent systems in Ta-\\nble 1. Note that many of these systems are evolving open-source projects, so the remarks and\\nstatements about them may only be accurate as of the time of writing. We refer interested readers to\\ndetailed LLM-based agent surveys (Xi et al., 2023; Wang et al., 2023b)\\nSingle-Agent Systems:\\n‚Ä¢AutoGPT : AutoGPT is an open-source implementation of an AI agent that attempts to au-\\ntonomously achieve a given goal (AutoGPT, 2023). It follows a single-agent paradigm in which\\nit augments the AI model with many useful tools, and does not support multi-agent collaboration.\\n‚Ä¢ChatGPT+ (with code interpreter or plugin) : ChatGPT, a conversational AI service or agent,\\ncan now be used alongside a code interpreter or plugin (currently available only under the pre-\\nmium subscription plan ChatGPT Plus) (OpenAI, 2023). The code interpreter enables ChatGPT\\nto execute code, while the plugin enhances ChatGPT with a wide range of curated tools.\\n‚Ä¢LangChain Agents : LangChain is a general framework for developing LLM-based applica-\\ntions (LangChain, 2023). LangChain Agents is a subpackage for using an LLM to choose a\\nsequence of actions. There are various types of agents in LangChain Agents, with the ReAct agent\\nbeing a notable example that combines reasoning and acting when using LLMs (mainly designed\\nfor LLMs prior to ChatGPT) (Yao et al., 2022). All agents provided in LangChain Agents fol-\\nlow a single-agent paradigm and are not inherently designed for communicative and collaborative\\nmodes. A significant summary of its limitations can be found in (Woolf, 2023). Due to these lim-\\nitations, even the multi-agent systems in LangChain (e.g., re-implementation of CAMEL) are not\\nbased on LangChain Agents but are implemented from scratch. Their connection to LangChain\\nlies in the use of basic orchestration modules provided by LangChain, such as AI models wrapped\\nby LangChain and the corresponding interface.\\n‚Ä¢Transformers Agent : Transformers Agent (HuggingFace, 2023) is an experimental natural-\\nlanguage API built on the transformers repository. It includes a set of curated tools and an agent\\nto interpret natural language and use these tools. Similar to AutoGPT, it follows a single-agent\\nparadigm and does not support agent collaboration.\\nAutoGen differs from the single-agent systems above by supporting multi-agent LLM applications.\\nMulti-Agent Systems:\\n‚Ä¢BabyAGI : BabyAGI (BabyAGI, 2023) is an example implementation of an AI-powered task man-\\nagement system in a Python script. In this implemented system, multiple LLM-based agents\\nare used. For example, there is an agent for creating new tasks based on the objective and the\\nresult of the previous task, an agent for prioritizing the task list, and an agent for completing\\ntasks/sub-tasks. As a multi-agent system, BabyAGI adopts a static agent conversation pattern,\\ni.e., a predefined order of agent communication, while AutoGen supports both static and dynamic\\nconversation patterns and additionally supports tool usage and human involvement.\\n‚Ä¢CAMEL : CAMEL (Li et al., 2023b) is a communicative agent framework. It demonstrates\\nhow role playing can be used to let chat agents communicate with each other for task comple-\\ntion. It also records agent conversations for behavior analysis and capability understanding. An\\nInception-prompting technique is used to achieve autonomous cooperation between agents. Un-\\nlikeAutoGen , CAMEL does not natively support tool usage, such as code execution. Although it\\nis proposed as an infrastructure for multi-agent conversation, it only supports static conversation\\npatterns, while AutoGen additionally supports dynamic conversation patterns.\\n‚Ä¢Multi-Agent Debate: Two recent works investigate and show that multi-agent debate is an effec-\\ntive way to encourage divergent thinking in LLMs (Liang et al., 2023) and to improve the factuality\\nand reasoning of LLMs (Du et al., 2023). In both works, multiple LLM inference instances are\\nconstructed as multiple agents to solve problems with agent debate. Each agent is simply an LLM\\ninference instance, while no tool or human is involved, and the inter-agent conversation needs\\nto follow a pre-defined order. These works attempt to build LLM applications with multi-agent\\nconversation, while AutoGen , designed as a generic infrastructure, can be used to facilitate this\\ndevelopment and enable more applications with dynamic conversation patterns.\\n14', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 13}),\n",
       " Document(page_content='‚Ä¢MetaGPT : MetaGPT (Hong et al., 2023) is a specialized LLM application based on a multi-agent\\nconversation framework for automatic software development. They assign different roles to GPTs\\nto collaboratively develop software. They differ from AutoGen by being specialized solutions to\\na certain scenario, while AutoGen is a generic infrastructure to facilitate building applications for\\nvarious scenarios.\\nThere are a few other specialized single-agent or multi-agent systems, such as V oyager (Wang et al.,\\n2023a) and Generative Agents (Park et al., 2023), which we skip due to lower relevance. In Table 1,\\nwe summarize differences between AutoGen and the most relevant multi-agent systems.\\nTable 1: Summary of differences between AutoGen and other related multi-agent systems. infras-\\ntructure : whether the system is designed as a generic infrastructure for building LLM applications.\\nconversation pattern : the types of patterns supported by the implemented systems. Under the\\n‚Äòstatic‚Äô pattern, agent topology remains unchanged regardless of different inputs. AutoGen allows\\nflexible conversation patterns, including both static and dynamic patterns that can be customized\\nbased on different application needs. execution-capable : whether the system can execute LLM-\\ngenerated code; human involvement : whether (and how) the system allows human participation\\nduring the execution process of the system. AutoGen allows flexible human involvement in multi-\\nagent conversation with the option for humans to skip providing inputs.\\nAspect AutoGen Multi-agent Debate CAMEL BabyAGI MetaGPT\\nInfrastructure ‚úì ‚úó ‚úì ‚úó ‚úó\\nConversation pattern flexible static static static static\\nExecution-capable ‚úì ‚úó ‚úó ‚úó ‚úì\\nHuman involvement chat/skip ‚úó ‚úó ‚úó ‚úó\\n15', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 14}),\n",
       " Document(page_content='B Expanded Discussion\\nThe applications in Section 3 show how AutoGen not only enables new applications but also helps\\nrenovate existing ones. For example, in A1 (scenario 3), A5, and A6, AutoGen enabled the cre-\\nation of multi-agent conversations that follow a dynamic pattern instead of a fixed back-and-forth.\\nAnd in both A5 and A6, humans can participate in the activities together with multiple other AI\\nagents in a conversational manner. Similarly, A1-A4 show how popular applications can be reno-\\nvated quickly with AutoGen . Despite the complexity of these applications (most of them involve\\nmore than two agents or dynamic multi-turn agent cooperation), our AutoGen -based implementa-\\ntion remains simple, demonstrating promising opportunities to build creative applications and a large\\nspace for innovation. In reflecting on why these benefits can be achieved in these applications with\\nAutoGen , we believe there are a few reasons:\\n‚Ä¢Ease of use : The built-in agents can be used out-of-the-box, delivering strong performance even\\nwithout any customization. (A1, A3)\\n‚Ä¢Modularity : The division of tasks into separate agents promotes modularity in the system. Each\\nagent can be developed, tested, and maintained independently, simplifying the overall develop-\\nment process and facilitating code management. (A3, A4, A5, and A6)\\n‚Ä¢Programmability: AutoGen allows users to extend/customize existing agents to develop systems\\nsatisfying their specific needs with ease. (A1-A6). For example, with AutoGen , the core workflow\\ncode in A4 is reduced from over 430 lines to 100 lines, for a 4x saving.\\n‚Ä¢Allowing human involvement :AutoGen provides a native mechanism to achieve human partici-\\npation and/or human oversight. With AutoGen , humans can seamlessly and optionally cooperate\\nwith AIs to solve problems or generally participate in the activity. AutoGen also facilitates inter-\\nactive user instructions to ensure the process stays on the desired path. (A1, A2, A5, and A6)\\n‚Ä¢Collaborative/adversarial agent interactions : Like many collaborative agent systems (Dong\\net al., 2023), agents in AutoGen can share information and knowledge, to complement each other‚Äôs\\nabilities and collectively arrive at better solutions. (A1, A2, A3, and A4). Analogously, in certain\\nscenarios, some agents are required to work in an adversarial way. Relevant information is shared\\namong different conversations in a controlled manner, preventing distraction or hallucination. (A4,\\nA6). AutoGen supports both patterns, enabling effective utilization and augmentation of LLMs.\\nB.1 General Guidelines for Using AutoGen\\nBelow we give some recommendations for using agents in AutoGen to accomplish a task.\\n1.Consider using built-in agents first. For example, AssistantAgent is pre-configured to be\\nbacked by GPT-4, with a carefully designed system message for generic problem-solving via\\ncode. The UserProxyAgent is configured to solicit human inputs and perform tool execution.\\nMany problems can be solved by simply combining these two agents. When customizing agents\\nfor an application, consider the following options: (1) human input mode, termination condition,\\ncode execution configuration, and LLM configuration can be specified when constructing an\\nagent; (2) AutoGen supports adding instructions in an initial user message, which is an effective\\nway to boost performance without needing to modify the system message; (3) UserProxyAgent\\ncan be extended to handle different execution environments and exceptions, etc.; (4) when sys-\\ntem message modification is needed, consider leveraging the LLM‚Äôs capability to program its\\nconversation flow with natural language.\\n2.Start with a simple conversation topology . Consider using the two-agent chat or the group chat\\nsetup first, as they can often be extended with the least code. Note that the two-agent chat can\\nbe easily extended to involve more than two agents by using LLM-consumable functions in a\\ndynamic way.\\n3. Try to reuse built-in reply methods based on LLM, tool, or human before implementing a\\ncustom reply method because they can often be reused to achieve the goal in a simple way\\n(e.g., the built-in agent GroupChatManager ‚Äôs reply method reuses the built-in LLM-based reply\\nfunction when selecting the next speaker, ref. A5 in Section 3).\\n4. When developing a new application with UserProxyAgent ,start with humans always in\\nthe loop , i.e., human input mode=‚ÄòALWAYS‚Äô, even if the target operation mode is more au-\\ntonomous. This helps evaluate the effectiveness of AssistantAgent , tuning the prompt, dis-\\ncovering corner cases, and debugging. Once confident with small-scale success, consider setting\\n16', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 15}),\n",
       " Document(page_content='human input mode = ‚ÄòNEVER‚Äô. This enables LLM as a backend, and one can either use the\\nLLM or manually generate diverse system messages to simulate different use cases.\\n5. Despite the numerous advantages of AutoGen agents, there could be cases/scenarios where other\\nlibraries/packages could help . For example: (1) For (sub)tasks that do not have requirements\\nfor back-and-forth trouble-shooting, multi-agent interaction, etc., a unidirectional (no back-and-\\nforth message exchange) pipeline can also be orchestrated with LangChain (LangChain, 2023),\\nLlamaIndex (Liu, 2022), Guidance (Guidance, 2023), Semantic Kernel (Semantic-Kernel, 2023),\\nGorilla (Patil et al., 2023) or low-level inference API (‚Äòautogen.oai‚Äô provides an enhanced LLM\\ninference layer at this level) (Dibia, 2023). (2) When existing tools from LangChain etc. are\\nhelpful, one can use them as tool backends for AutoGen agents. For example, one can readily use\\ntools, e.g., Wolfram Alpha, from LangChain in AutoGen agent. (3) For specific applications, one\\nmay want to leverage agents implemented in other libraries/packages. To achieve this, one could\\nwrap those agents as conversable agents in AutoGen and then use them to build LLM applications\\nthrough multi-agent conversation. (4) It can be hard to find an optimal operating point among\\nmany tunable choices, such as the LLM inference configuration. Blackbox optimization packages\\nlike ‚Äòflaml.tune‚Äô (Wang et al., 2021) can be used together with AutoGen to automate such tuning.\\nB.2 Future Work\\nThis work raises many research questions and future directions and .\\nDesigning optimal multi-agent workflows: Creating a multi-agent workflow for a given task can\\ninvolve many decisions, e.g., how many agents to include, how to assign agent roles and agent\\ncapabilities, how the agents should interact with each other, and whether to automate a particular\\npart of the workflow. There may not exist a one-fits-all answer, and the best solution might depend\\non the specific application. This raises important questions: For what types of tasks and applications\\nare multi-agent workflows most useful? How do multiple agents help in different applications? For\\na given task, what is the optimal (e.g., cost-effective) multi-agent workflow?\\nCreating highly capable agents: AutoGen can enable the development of highly capable agents\\nthat leverage the strengths of LLMs, tools, and humans. Creating such agents is crucial to ensuring\\nthat a multi-agent workflow can effectively troubleshoot and make progress on a task. For example,\\nwe observed that CAMEL, another multi-agent LLM system, cannot effectively solve problems in\\nmost cases primarily because it lacks the capability to execute tools or code. This failure shows that\\nLLMs and multi-agent conversations with simple role playing are insufficient, and highly capable\\nagents with diverse skill sets are essential. We believe that more systematic work will be required to\\ndevelop guidelines for application-specific agents, to create a large OSS knowledge base of agents,\\nand to create agents that can discover and upgrade their skills (Cai et al., 2023).\\nEnabling scale, safety, and human agency: Section 3 shows how complex multi-agent workflows\\ncan enable new applications, and future work will be needed to assess whether scaling further can\\nhelp solve extremely complex tasks. However, as these workflows scale and grow more complex,\\nit may become difficult to log and adjust them. Thus, it will become essential to develop clear\\nmechanisms and tools to track and debug their behavior. Otherwise, these techniques risk resulting\\nin incomprehensible, unintelligible chatter among agents (Lewis et al., 2017).\\nOur work also shows how complex, fully autonomous workflows with AutoGen can be useful, but\\nfully autonomous agent conversations will need to be used with care. While the autonomous mode\\nAutoGen supports could be desirable in many scenarios, a high level of autonomy can also pose\\npotential risks, especially in high-risk applications (Amodei et al., 2016; Weld & Etzioni, 1994). As\\na result, building fail-safes against cascading failures and exploitation, mitigating reward hacking,\\nout of control and undesired behaviors, maintaining effective human oversight of applications built\\nwith AutoGen agents will become important. While AutoGen provides convenient and seamless\\ninvolvement of humans through a user proxy agent, developers and stakeholders still need to under-\\nstand and determine the appropriate level and pattern of human involvement to ensure the safe and\\nethical use of the technology (Horvitz, 1999; Amershi et al., 2019).\\n17', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 16}),\n",
       " Document(page_content='C Default System Message for Assistant Agent\\nSystemMessageYou are a helpful AI assistant. Solve tasks using your coding and language skills.In the following cases, suggest python code (in a python coding block) or shell script (in a shcoding block) for the user to execute.1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.2. When you need to perform some task with code, use the code to perform the task and output theresult. Finish the task smartly.Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can‚Äôt modify your code. So do not suggest incomplete code which requires users to modify. Don‚Äôt use a code block if it‚Äôs not intended to be executed by the user.If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don‚Äôt include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use ‚Äôprint‚Äô function for the output when relevant. Check the execution result returned by the user.If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can‚Äôt be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.Reply ‚ÄúTERMINATE‚Äù in the end when everything is done.Prompting techniques color code: Role Play; Control Flow; Output Confine; Facilitate Automation; Grounding\\nFigure 5: Default system message for the built-in assistant agent in AutoGen (v0.1.1). This is an\\nexample of conversation programming via natural language. It contains instructions of different\\ntypes, including role play, control flow, output confine, facilitate automation, and grounding.\\nFigure 5 shows the default system message for the built-in assistant agent in AutoGen (v0.1.1),\\nwhere we introduce several new prompting techniques and highlight them accordingly. When com-\\nbining these new prompting techniques together, we can program a fairly complex conversation even\\nwith the simplest two-agent conversation topology. This approach tries to exploit the capability of\\nLLMs in implicit state inference to a large degree. LLMs do not follow all the instructions perfectly,\\nso the design of the system needs to have other mechanisms to handle the exceptions and faults.\\nSome instructions can have ambiguities, and the designer should either reduce them for preciseness\\nor intentionally keep them for flexibility and address the different situations in other agents. In\\ngeneral, we observe that GPT-4 follows the instructions better than GPT-3.5-turbo.\\n18', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 17}),\n",
       " Document(page_content='D Application Details\\nA1: Math Problem Solving\\nScenario 1: Autonomous Problem Solving. We perform both qualitative and quantitative eval-\\nuations in this scenario. For all evaluations, we use GPT-4 as the base model, and pre-install the\\n‚Äúsympy‚Äù package in the execution environment. We compare AutoGen with the following LLM-\\nbased agent systems:\\n‚Ä¢ AutoGPT: The out-of-box AutoGPT is used. We initialize AutoGPT by setting the purpose to\\n‚Äúsolve math problems‚Äù, resulting in a ‚ÄúMathSolverGPT‚Äù with auto-generated goals.\\n‚Ä¢ ChatGPT+Plugin: We enable the Wolfram Alpha plugin (a math computation engine) in the Ope-\\nnAI web client.\\n‚Ä¢ ChatGPT+Code Interpreter: This is a recent feature in OpenAI web client. Note that the above\\ntwo premium features from ChatGPT require a paid subscription to be accessed and are the most\\ncompetitive commercial systems.\\n‚Ä¢ LangChain ReAct+Python: We use Python agent from LangChain. To handle parsing errors, we\\nset ‚Äúhandle parsing errors=True‚Äù, and use the default zero-shot ReAct prompt.\\n‚Ä¢ Multi-Agent Debate (Liang et al., 2023): We modified the code of the multi-agent debate to per-\\nform evaluation. By default, there are three agents: an affirmative agent, a negative agent, and a\\nmoderator.\\nWe also conducted preliminary evaluations on several other multi-agent systems, including\\nBabyAGI, CAMEL, and MetaGPT. The results indicate that they are not suitable choices for solving\\nmath problems out of the box. For instance, when MetaGPT is tasked with solving a math problem,\\nit begins developing software to address the problem, but most of the time, it does not actually solve\\nthe problem. We have included the test examples in Appendix E.\\nTable 2: Qualitative evaluation of two math problems from the MATH dataset within the autonomous\\nproblem-solving scenario. Each LLM-based system is tested three times on each of the problems.\\nThis table reports the problem-solving correctness and summarizes the reasons for failure.\\nCorrectness Failure Reason\\nAutoGen 3/3 N/A.\\nAutoGPT 0/3 The LLM gives code without the print function so the\\nresult is not printed.\\nChatGPT+Plugin 1/3 The return from Wolfram Alpha contains 2 simplified\\nresults, including the correct answer, but GPT-4 always\\nchooses the wrong answer.\\nChatGPT+Code Interpreter 2/3 Returns a wrong decimal result.\\nLangChain ReAct 0/3 LangChain gives 3 different wrong answers.\\nMulti-Agent Debate 0/3 It gives 3 different wrong answers due to calculation errors.\\n(a) Evaluation on the first problem that asks to simplify a square root fraction.\\nCorrectness Failure Reason\\nAutoGen 2/3 The final answer from code execution is wrong.\\nAutoGPT 0/3 The LLM gives code without the print function so the\\nresult is not printed.\\nChatGPT+Plugin 1/3 For one trial, GPT-4 got stuck because it keeps giving\\nwrong queries and has to be stopped. Another trial simply\\ngives a wrong answer.\\nChatGPT+Code Interpreter 0/3 It gives 3 different wrong answers.\\nLangChain ReAct 0/3 LangChain gives 3 different wrong answers.\\nMulti-Agent Debate 0/3 It gives 3 different wrong answers.\\n(b) Evaluation on the second number theory problem.\\nFor the qualitative evaluation, we utilize two level-5 problems from the MATH dataset, testing each\\nproblem three times. The first problem involves simplifying a square root fraction, and the second\\n19', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 18}),\n",
       " Document(page_content='problem involves solving a number theory issue. The correctness counts and reasons for failure\\nare detailed in Table 2. For the quantitative evaluation, we conduct two sets of experiments on\\nthe MATH dataset to assess the correctness of these systems: (1) an experiment involving 120\\nlevel-5 (the most challenging level) problems, including 20 problems from six categories, excluding\\ngeometry, and (2) an experiment on the entire test set, which includes 5000 problems. We exclude\\nAutoGPT from this evaluation as it cannot access results from code executions and does not solve\\nany problems in the qualitative evaluation. Our analysis of the entire dataset reveals that AutoGen\\nachieves an overall accuracy of 69.48%, while GPT-4‚Äôs accuracy stands at 55.18%. From these\\nevaluations, we have the following observations regarding the problem-solving success rate and\\nuser experience of these systems:\\n‚Ä¢ Problem-solving success rate: Results from the quantitative evaluations show that AutoGen can\\nhelp achieve the highest problem-solving success rate among all the compared methods. The qual-\\nitative evaluations elucidate common failure reasons across several alternative approaches. Chat-\\nGPT+Code Interpreter fails to solve the second problem, and ChatGPT+Plugin struggles to solve\\nboth problems. AutoGPT fails on both problems due to code execution issues. The LangChain\\nagent also fails on both problems, producing code that results in incorrect answers in all trials.\\n‚Ä¢ Based on the qualitative evaluation, we analyze the user experience concerning the verbosity of\\nthe response and the ability of the LLM-based system to run without unexpected behaviors. Chat-\\nGPT+Plugin is the least verbose, mainly because Wolfram queries are much shorter than Python\\ncode. AutoGen , ChatGPT+Code Interpreter, and LangChain exhibit similar verbosity, although\\nLangChain is slightly more verbose due to more code execution errors. AutoGPT is the most\\nverbose system owing to predefined steps like THOUGHTS, REASONING, and PLAN, which it\\nincludes in replies every time. Overall, AutoGen and ChatGPT+Code Interpreter operate smoothly\\nwithout exceptions. We note the occurrences of undesired behaviors from other LLM-based sys-\\ntems that could affect user experience: AutoGPT consistently outputs code without the print‚Äô\\nstatement and cannot correct this, requiring the user to run them manually; ChatGPT with Wol-\\nfram Alpha plugin has the potential to become stuck in a loop that must be manually stopped; and\\nLangchain ReAct could exit with a parse error, necessitating the passing of a ‚Äòhandle parse error‚Äô\\nparameter.\\nEnable Multi-User Problem Solving ViaStudent        and Expert Student Proxy\\nStudentAssistant\\nExpertAssistant\\nExpert Proxy\\nAsk for expert\\nEnable Autonomous and Human-in-the-loop Problem Solving\\nFigure 6: Examples of three settings utilized to solve math problems using AutoGen : (Gray) En-\\nables a workflow where a student collaborates with an assistant agent to solve problems, either\\nautonomously or in a human-in-the-loop mode. (Gray + Orange) Facilitates a more sophisticated\\nworkflow wherein the assistant, on the fly, can engage another user termed ‚Äúexpert‚Äù, who is in the\\nloop with their own assistant agent, to aid in problem-solving if its own solutions are not satisfactory.\\nScenario 2: Human-in-the-loop Problem Solving. For challenging problems that these LLM\\nsystems cannot solve autonomously, human feedback during the problem-solving process can be\\n20', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 19}),\n",
       " Document(page_content='helpful. To incorporate human feedback with AutoGen , one can set human input mode=‚ÄòALWAYS‚Äô\\nin the user proxy agent. We select one challenging problem that none of these systems can solve\\nautonomously across three trials. We adhere to the process outlined below to provide human inputs\\nfor all the compared methods:\\n1. Input the problem: Find the equation of the plane which bisects the angle\\nbetween the planes 3x‚àí6y+ 2z+ 5 = 0 and4x‚àí12y+ 3z‚àí3 = 0 ,and which\\ncontains the point (‚àí5,‚àí1,‚àí5).Enter your answer in the form\\nAx+By+Cz+D= 0,\\nwhere A, B, C, D are integers such that A > 0and\\ngcd(|A|,|B|,|C|,|D|) = 1.\\n2. The response from the system does not solve the problem correctly. We then give a\\nhint to the model: Your idea is not correct. Let‚Äôs solve this together.\\nSuppose P= (x, y, z )is a point that lies on a plane that bisects the\\nangle, the distance from P to the two planes is the same. Please\\nset up this equation first.\\n3. We expect the system to give the correct distance equation. Since the equation involves\\nan absolute sign that is hard to solve, we would give the next hint: Consider the two\\ncases to remove the abs sign and get two possible solutions.\\n4. If the system returns the two possible solutions and doesn‚Äôt continue to the next step, we\\ngive the last hint: Use point (-5,-1,-5) to determine which is correct and\\ngive the final answer.\\n5. Final answer is 11x+6y+5z+86=0 .\\nWe observed that AutoGen consistently solved the problem across all three trials. ChatGPT+Code\\nInterpreter and ChatGPT+Plugin managed to solve the problem in two out of three trials, while Au-\\ntoGPT failed to solve it in all three attempts. In its unsuccessful attempt, ChatGPT+Code Interpreter\\nfailed to adhere to human hints. In its failed trial, ChatGPT+Plugin produced an almost correct solu-\\ntion but had a sign discrepancy in the final answer. AutoGPT was unable to yield a correct solution\\nin any of the trials. In one trial, it derived an incorrect distance equation. In the other two trials, the\\nfinal answer was incorrect due to code execution errors.\\nScenario 3: Multi-User Problem Solving. Next-generation LLM applications may necessitate\\nthe involvement of multiple real users for collectively solving a problem with the assistance of\\nLLMs. We showcase how AutoGen can be leveraged to effortlessly construct such a system. Specif-\\nically, building upon scenario 2 mentioned above, we aim to devise a simple system involving two\\nhuman users: a student and an expert. In this setup, the student interacts with an LLM assistant to\\naddress some problems, and the LLM automatically resorts to the expert when necessary.\\nThe overall workflow is as follows: The student chats with the LLM-based assistant agent through\\na student proxy agent to solve problems. When the assistant cannot solve the problem satisfactorily,\\nor the solution does not match the expectation of the student, it would automatically hold the con-\\nversation and call the pre-defined askforexpert function via the function callfeature of GPT\\nin order to resort to the expert. Specifically, it would automatically produce the initial message for\\ntheaskforexpert function, which could be the statement of the problem or the request to verify\\nthe solution to a problem, and the expert is supposed to respond to this message with the help of\\nthe expert assistant. After the conversation between the expert and the expert‚Äôs assistant, the final\\nmessage would be sent back to the student assistant as the response to the initial message. Then, the\\nstudent assistant would resume the conversation with the student using the response from the expert\\nfor a better solution. A detailed visualization is shown in Figure 6.\\nWith AutoGen , constructing the student/expert proxy agent and the assistant agents is straight-\\nforward by reusing the built-in UserProxyAgent andAssistantAgent through appropriate\\nconfigurations. The only development required involves writing several lines of code for the\\naskforexpert function, which then becomes part of the configuration for the assistant. Ad-\\nditionally, it‚Äôs easy to extend such a system to include more than one expert, with a specific\\naskforexpert function for each, or to include multiple student users with a shared expert for\\nconsultation.\\n21', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 20}),\n",
       " Document(page_content='A2: Retrieval-Augmented Code Generation and Question Answering\\nRetrieval-augmentedAssistantRetrieval-augmented  User Proxy\\n1. Question and Contexts3. Terminate,feedbacks or `Update Context`4. Satisfied Answers or Terminate\\n2. Satisfied Answers or `Update Context`\\nFigure 7: Overview of Retrieval-augmented Chat which involves two agents, including a Retrieval-\\naugmented User Proxy and a Retrieval-augmented Assistant. Given a set of documents, the\\nRetrieval-augmented User Proxy first automatically processes documents‚Äîsplits, chunks, and stores\\nthem in a vector database. Then for a given user input, it retrieves relevant chunks as context and\\nsends it to the Retrieval-augmented Assistant, which uses LLM to generate code or text to answer\\nquestions. Agents converse until they find a satisfactory answer.\\nDetailed Workflow. The workflow of Retrieval-Augmented Chat is illustrated in Figure 7. To\\nuse Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented\\nUser Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy\\nnecessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented\\nUser Proxy can download the documents, segment them into chunks of a specific size, compute\\nembeddings, and store them in a vector database. Once a chat is initiated, the agents collaboratively\\nengage in code generation or question-answering adhering to the procedures outlined below:\\n1. The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding simi-\\nlarity, and sends them along with the question to the Retrieval-Augmented Assistant.\\n2. The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based\\non the question and context provided. If the LLM is unable to produce a satisfactory response, it\\nis instructed to reply with ‚ÄúUpdate Context‚Äù to the Retrieval-Augmented User Proxy.\\n3. If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and\\nsends the output as feedback. If there are no code blocks or instructions to update the context, it\\nterminates the conversation. Otherwise, it updates the context and forwards the question along\\nwith the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation\\nis enabled, individuals can proactively send any feedback, including Update Context‚Äù, to the\\nRetrieval-Augmented Assistant.\\n4. If the Retrieval-Augmented Assistant receives ‚ÄúUpdate Context‚Äù, it requests the next most similar\\nchunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it\\ngenerates new code or text based on the feedback and chat history. If the LLM fails to generate\\nan answer, it replies with ‚ÄúUpdate Context‚Äù again. This process can be repeated several times.\\nThe conversation terminates if no more documents are available for the context.\\nWe utilize Retrieval-Augmented Chat in two scenarios. The first scenario aids in generating code\\nbased on a given codebase. While LLMs possess strong coding abilities, they are unable to utilize\\npackages or APIs that are not included in their training data, e.g., private codebases, or have trouble\\nusing trained ones that are frequently updated post-training. Hence, Retrieval-Augmented Code\\nGeneration is considered to be highly valuable. The second scenario involves question-answering\\non the Natural Questions dataset (Kwiatkowski et al., 2019), enabling us to obtain comparative\\nevaluation metrics for the performance of our system.\\nScenario 1: Evaluation on Natural Questions QA dataset. In this case, we evaluate the\\nRetrieval-Augmented Chat‚Äôs end-to-end question-answering performance using the Natural Ques-\\ntions dataset (Kwiatkowski et al., 2019). We collected 5,332 non-redundant context documents and\\n6,775 queries from HuggingFace. First, we create a document collection based on the entire context\\ncorpus and store it in the vector database. Then, we utilize Retrieval-Augmented Chat to answer the\\nquestions. An example (Figure 8) from the NQ dataset showcases the advantages of the interactive\\nretrieval feature: ‚Äúwho carried the usa flag in opening ceremony‚Äù . When attempting to answer\\nthis question, the context with the highest similarity to the question embedding does not contain the\\n22', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 21}),\n",
       " Document(page_content='Task description who carried the usaflag in opening ceremony?Assistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply exactly ‚ÄòUPDATE CONTEXT‚Äô. You must give as short an answer as possible.User‚Äôs question is: who carried the usaflag in opening ceremonyContext is: < P > On January 17 , 1899 , under ... The Astros beat the Yankees that night , 2 -1 . < /P >UserProxy: Sorry, I cannot find any information about who carried the USA flag in the opening ceremony. UPDATE CONTEXT.Assistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply exactly ‚ÄòUPDATE CONTEXT‚Äô. You must give as short an answer as possible.User‚Äôs question is: who carried the usaflag in opening ceremonyContext is: < T able > ... Erin Hamlin ... < /T d >UserProxy: Erin Hamlin carried the USA flag in the opening ceremony. \\n(a) W/O Interactive Retrieval\\n(b) W/ Interactive Retrieval\\nAssistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply ‚Äòsorry, I don‚Äôt know‚Äô. You must give as short an answer as possible.User‚Äôs question is:who carried the usaflag in opening ceremonyContext is: < P > On January 17 , 1899 , under ... The Astros beat the Yankees that night , 2 -1 . < /P >UserProxy: Sorry, I cannot find any information about who carried the USA flag in the opening ceremony. Sorry, I don‚Äôt know.Figure 8: Retrieval-augmented Chat without (W/O) and with (W/) interactive retrieval .\\nrequired information for a response. As a result, the LLM assistant (GPT-3.5-turbo) replies ‚ÄúSorry,\\nI cannot find any information about who carried the USA flag in the opening ceremony. UPDATE\\nCONTEXT. ‚Äù With the unique and innovative ability to update context in Retrieval-Augmented Chat,\\nthe user proxy agent automatically updates the context and forwards it to the assistant agent again.\\nFollowing this process, the agent is able to generate the correct answer to the question.\\nIn addition, we conduct an experiment using the same prompt as illustrated in (Adlakha et al., 2023)\\nto investigate the advantages of AutoGen W/O interactive retrieval . The F1 score and Recall for the\\nfirst 500 questions are 23.40% and 62.60%, respectively, aligning closely with the results reported\\nin Figure 4b. Consequently, we assert that AutoGen W/O interactive retrieval outperforms DPR due\\nto differences in the retrievers employed. Specifically, we utilize a straightforward vector search\\nretriever with the all-MiniLM-L6-v2 model for embeddings.\\nFurthermore, we analyze the number of LLM calls in experiments involving both AutoGen and\\nAutoGen W/O interactive retrieval , revealing that approximately 19.4% of questions in the Natural\\nQuestions dataset trigger an ‚ÄúUpdate Context‚Äù operation, resulting in additional LLM calls.\\nScenario 2: Code Generation Leveraging Latest APIs from the Codebase. In this case, the ques-\\ntion is ‚ÄúHow can I use FLAML to perform a classification task and use Spark for parallel training?\\nTrain for 30 seconds and force cancel jobs if the time limit is reached. ‚Äù . FLAML (v1) (Wang et al.,\\n2021) is an open-source Python library designed for efficient AutoML and tuning. It was open-\\nsourced in December 2020, and is included in the training data of GPT-4. However, the question\\nnecessitates the use of Spark-related APIs, which were added in December 2022 and are not encom-\\npassed in the GPT-4 training data. Consequently, the original GPT-4 model is unable to generate the\\ncorrect code, due to its lack of knowledge regarding Spark-related APIs. Instead, it erroneously cre-\\nates a non-existent parameter, spark , and sets it to True‚Äô. Nevertheless, with Retrieval-Augmented\\nChat, we provide the latest reference documents as context. Then, GPT-4 generates the correct code\\nblocks by setting usespark andforce cancel to True‚Äô.\\n23', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 22}),\n",
       " Document(page_content='A3: Decision Making in Text World Environments\\nALFWorld\\nExecutor\\nReward & StateAction Decision\\nAssistant\\nObservation: On the desk 2, you see an alarmclock 3, \\na bowl 3, a creditcard 2, a mug 1, and a pencil 2.Action decision: Pick up pencil 2 from desk 2\\nGroundingAgent\\nALFChat (two agents) ALFChat (three agents)\\nALFWorld Executor\\nAssistant\\nFigure 9: We use AutoGen to solve tasks in the ALFWorld benchmark, which contains household\\ntasks described in natural language. We propose two designs: a two-agent design where the assistant\\nagent suggests the next step, and the Executor executes actions and provides feedback. The three-\\nagent design adds a grounding agent that supplies commonsense facts to the executor when needed.\\nALFWorld (Shridhar et al., 2021) is a synthetic language-based interactive decision-making task.\\nIt comprises textual environments that aim to simulate real-world household scenes. Given a high-\\nlevel goal (e.g., putting a hot apple in the fridge) and the description of the household environment,\\nthe agent needs to explore and interact with the simulated household environment through a textual\\ninterface. A typical task environment contains various types of locations and could require more\\nthan 40 steps to finish, which highlights the need for agents to decompose the goal into subtasks and\\ntackle them one by one, while effectively exploring the environments.\\nDetailed Workflow. We first propose a straightforward two-agent system with AutoGen , illustrated\\non the left-hand side of Figure 9, to tackle tasks from this benchmark. The system consists of\\nan assistant agent and an executor agent. The assistant agent generates plans and makes action\\ndecisions to solve the tasks. The executor agent is tailored specifically for ALFWorld. It performs\\nactions proposed by the assistant and reports action execution results in the household environment\\nas feedback to the assistant. Due to the strict format requirements for the output format, we use the\\nBLEU metric to evaluate the similarity of the output to all valid action options. The option with the\\nhighest similarity will be chosen as the action for this round.\\nOne major challenge encompassed in ALFWorld is commonsense reasoning. The agent needs to\\nextract patterns from the few-shot examples provided and combine them with the agent‚Äôs general\\nknowledge of household environments to fully understand task rules. More often than not, the as-\\nsistant tends to neglect some basic knowledge of the household environment. Thanks to the easy-to-\\nimplement multi-agent conversational feature of AutoGen , enhancing the assistant agent‚Äôs reason-\\ning ability by adding a new grounding agent to provide commonsense facts for the decision-making\\nagent‚Äôs reference becomes straightforward. By scrutinizing the failed attempts and summarizing the\\nreasons for failure, we obtained a holistic understanding of the commonsense knowledge that the\\nassistant agent lacks. Then, we set a grounding agent to provide this general knowledge when the\\ntask begins and whenever the assistant outputs the same action three times in a row. This ensures the\\nassistant takes this commonsense knowledge into consideration and prevents it from getting stuck in\\noutputting the same content or constantly apologizing.\\nWe compare our system‚Äôs performance with ReAct, which treats ALFWorld as a text-completion\\ntask. ReAct (Yao et al., 2022) is a few-shot prompting technique that interleaves reasoning and\\nacting, allowing for greater synergy between the two and significantly improving performance on\\nboth language and decision-making tasks. We integrate ReAct into AutoGen by modifying the\\nprompts in a conversational manner. Following ReAct, we employ a two-shot setting. The few-\\nshot prompts are obtained from the corresponding repository. As shown in Table 3, the two-agent\\n24', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 23}),\n",
       " Document(page_content=\"Task description\\nYou are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a safe 1, a drawer 2, a dra wer1, a \\nshelf 3, a shelf 2, and a shelf 1.\\nYour task is to: look at bowl under the desklamp . \\nAssistant (to Executor) : THOUGHT [First, I need to find a bowl. A \\nbowl is more likely to appear in desk (1 -2), drawer (1 -2), shelf \\n(1-3), bed (1). Then I need to find and use a desklamp .] ACTION \\n[go to desk 1]\\n(Assistant finds the desklamp at desk 1, then goes to desk 2.)\\nExecutor (to Assistant) : On the desk 2, you see a bowl 1, and a \\ncd 3... \\nAssistant (to Executor) : THOUGHT [Now I find a bowl (1). I need \\nto use the desklamp to look at the bowl. I'll go to the desklamp\\nnow.] ACTION [go to desk 1]\\nExecutor (to Assistant): On the desk 1, you see a desklamp 1, a \\npencil 2...\\nAssistant: ACTION [use desklamp 1]\\nExecutor (to Assistant) : You turn on the desklamp 1.\\nAssistant (to Executor): THOUGHT [Now that the desklamp is turned \\non, I can use it to look at the bowl (1) that I found earlier.] \\nACTION [look at bowl 1 under the desklamp 1]\\nExecutor (to Assistant) : You turn on the desklamp 1.\\n(Assistant falls into an infinite loop turning on desklamp .)\\nExecutor (to Assistant) : Task failed. Reply TERMINATE.\\nAssistant (to Executor) : TERMINATEAssistant (to Executor) : THOUGHT [First, I need to find a bowl. A \\nbowl is more likely to appear in desk (1 -2), drawer (1 -2), shelf \\n(1-3), bed (1). Then I need to find and use a desklamp .] ACTION \\n[go to desk 1]\\n(Assistant finds the desklamp at desk 1, then goes to desk 2.)\\n(Assistant falls into the same loop as (a))\\nGroundingAgent (to Executor) : You must find and take the object \\nbefore you can examine it. You must go to where the target \\nobject is before you can use it.\\nExecutor(to Assistant): You turn on the desklamp 1. Hint: You must \\nfind and take the object before you can examine it. \\nAssistant (to Executor): THOUGHT [I need to find and take the \\nbowl (1) before I can examine it. Let me find the bowl first.] \\nACTION [go to desk 2]\\nExecutor (to Assistant): On the desk 2, you see a bowl 1, a cd \\n3...\\nAssistant (to Executor) : ACTION [take bowl 1 from desk 2]\\nExecutor (to Assistant): You pick up bowl 1 from desk 2.\\nAssistant (to Executor): ACTION [go to desk 1]\\nAssistant (to Executor): ACTION[use desklamp 1]\\nExecutor (to Assistant) : Task success. Reply TERMINATE.\\nAssistant (to Executor) : TERMINATE\\n(a) Two agents\\n (b) Three agentsFigure 10: Comparison of results from two designs: (a) Two-agent design which consists of an\\nassistant and an executor, (b) Three-agent design which adds a grounding agent that serves as a\\nknowledge source. For simplicity, we omit the in-context examples and part of the exploration\\ntrajectory, and only show parts contributing to the failure/success of the attempt.\\nMethod Pick Clean Heat Cool Look Pick 2 All\\nReAct (avg) 63 52 48 71 61 24 54\\nALFChat (2 agents)(avg) 61 58 57 67 50 19 54\\nALFChat (3 agents)(avg) 79 64 70 76 78 41 69\\nReAct (best of 3) 75 62 61 81 78 35 66\\nALFChat (2 agents)(best of 3) 71 61 65 76 67 35 63\\nAFLChat (3 agents)(best of 3) 92 74 78 86 83 41 77\\nTable 3: Comparisons between ReAct and the two variants of ALFChat on the ALFWorld bench-\\nmark. For each task, we report the success rate out of 3 attempts. Success rate denotes the number\\nof tasks successfully completed by the agent divided by the total number of tasks. The results show\\nthat adding a grounding agent significantly improves the task success rate in ALFChat.\\ndesign matches the performance of ReAct, while the three-agent design significantly outperforms\\nReAct. We surmise that the performance discrepancy is caused by the inherent difference between\\ndialogue-completion and text-completion tasks. On the other hand, introducing a grounding agent\\nas a knowledge source remarkably advances performance on all types of tasks.\\nCase study . Figure 10 exemplifies how a three-agent design eliminates one root cause for failure\\ncases. Most of the tasks involve taking an object and then performing a specific action with it (e.g.,\\nfinding a vase and placing it on a cupboard). Without a grounding agent, the assistant frequently\\nconflates finding an object with taking it, as illustrated in Figure 10a). This leads to most of the\\nfailure cases in ‚Äôpick‚Äô and ‚Äôlook‚Äô type tasks. With the introduction of a grounding agent, the assistant\\ncan break out of this loop and successfully complete the task\\nTakeaways. We introduced a grounding agent to serve as an external commonsense knowledge\\nsource, which significantly enhanced the assistant‚Äôs ability to make informed decisions. This proves\\nthat providing necessary commonsense facts to the decision-making agent can assist it in making\\nmore informed decisions, thus effectively boosting the task success rate. AutoGen brings both\\nsimplicity and modularity when adding the grounding agent.\\n25\", metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 24}),\n",
       " Document(page_content='A4: Multi-Agent Coding\\nCommander\\nSafeguard\\nWriter\\n‚ë°Question, ‚ùªLog‚ù∏Code, ‚ë¶Ans‚ùπCode‚ù∫Clearance\\nUser‚ë†User Question‚ëßFinal AnswerRepeat until answering the user‚Äôs question or timeout\\nFigure 11: Our re-implementation of OptiGuide with AutoGen streamlining agents‚Äô interactions.\\nThe Commander receives user questions (e.g., What if we prohibit shipping from supplier 1 to\\nroastery 2?) and coordinates with the Writer and Safeguard. The Writer crafts the code and inter-\\npretation, the Safeguard ensures safety (e.g., not leaking information, no malicious code), and the\\nCommander executes the code. If issues arise, the process can repeat until resolved. Shaded circles\\nrepresent steps that may be repeated multiple times.\\nDetailed Workflow. The workflow can be described as follows. The end user initiates the in-\\nteraction by posing a question, such as ‚ÄúWhat if we prohibit shipping from supplier 1 to roastery\\n2?‚Äù, marked by\\n1 to the Commander agent. The Commander manages and coordinates with two\\nLLM-based assistant agents: the Writer and the Safeguard. Apart from directing the flow of commu-\\nnication, the Commander has the responsibility of handling memory tied to user interactions. This\\ncapability enables the Commander to capture and retain valuable context regarding the user‚Äôs ques-\\ntions and their corresponding responses. Such memory is subsequently shared across the system,\\nempowering the other agents with context from prior user interactions and ensuring more informed\\nand relevant responses.\\nIn this orchestrated process, the Writer, who combines the functions of a ‚ÄúCoder‚Äù and an ‚ÄúInter-\\npreter‚Äù as defined in (Li et al., 2023a), will craft code and also interpret execution output logs. For in-\\nstance, during code writing (\\n2 and\\n3 ), the Writer may craft code ‚Äúmodel.addConstr(x[‚Äòsupplier1‚Äô,\\n‚Äòroastery2‚Äô] == 0, ‚Äòprohibit‚Äô)‚Äù to add an additional constraint to answer the user‚Äôs question.\\nAfter receiving the code, the Commander will communicate with the Safeguard to screen the code\\nand ascertain its safety (\\n4 ); once the code obtains the Safeguard‚Äôs clearance, marked by\\n5 , the\\nCommander will use external tools (e.g., Python) to execute the code and request the Writer to\\ninterpret the execution results for the user‚Äôs question (\\n6 and\\n7 ). For instance, the writer may\\nsay ‚Äúif we prohibit shipping from supplier 1 to roastery 2, the total cost would increase by 10.5%.‚Äù\\nBringing this intricate process full circle, the Commander furnishes the user with the concluding\\nanswer (\\n8 ).\\nIf at a point there is an exception - either a security red flag raised by Safeguard (in\\n5 ) or code\\nexecution failures within Commander, the Commander redirects the issue back to the Writer with\\nessential information in logs (\\n6 ). So, the process from\\n3 to\\n6 might be repeated multiple times,\\nuntil each user query receives a thorough and satisfactory resolution or until the timeout. This entire\\ncomplex workflow of multi-agent interaction is elegantly managed via AutoGen .\\nThe core workflow code for OptiGuide was reduced from over 430 lines to 100 lines using AutoGen ,\\nleading to significant productivity improvement. The new agents are customizable, conversable, and\\ncan autonomously manage their chat memories. This consolidation allows the coder and interpreter\\nroles to merge into a single ‚ÄúWriter‚Äù agent, resulting in a clean, concise, and intuitive implementation\\nthat is easier to maintain.\\n26', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 25}),\n",
       " Document(page_content='Manual Evaluation Comparing ChatGPT + Code Interpreter and AutoGen -based OptiGuide.\\nChatGPT + Code Interpreter is unable to execute code with private or customized dependencies (e.g.,\\nGurobi), which means users need to have engineering expertise to manually handle multiple steps,\\ndisrupting the workflow and increasing the chance for mistakes. If users lack access or expertise,\\nthe burden falls on supporting engineers, increasing their on-call time.\\nWe carried out a user study that juxtaposed OpenAI‚Äôs ChatGPT coupled with a Code Interpreter\\nagainst AutoGen -based OptiGuide. The study focused on a coffee supply chain scenario, and an\\nexpert Python programmer with proficiency in Gurobi participated in the test. We evaluated both\\nsystems based on 10 randomly selected questions, measuring time and accuracy. While both sys-\\ntems answered 8 questions correctly, the Code Interpreter was significantly slower than OptiGuide\\nbecause the former requires more manual intervention. On average, users needed to spend 4 minutes\\nand 35 seconds to solve problems with the Code Interpreter, with a standard deviation of approxi-\\nmately 2.5 minutes. In contrast, OptiGuide‚Äôs average problem-solving time was around 1.5 minutes,\\nmost of which was spent waiting for responses from the GPT-4 model. This indicates a 3x saving\\non the user‚Äôs time with AutoGen -based OptiGuide.\\nWhile using ChatGPT + Code Interpreter, users had to read through the code and instructions to\\nknow where to paste the code snippets. Additionally, running the code involves downloading it and\\nexecuting it in a terminal, a process that was both time-consuming and prone to errors. The response\\ntime from the Code Interpreter is also slower, as it generates lots of tokens to read the code, read the\\nvariables line-by-line, perform chains of thought analysis, and then produce the final answer code.\\nIn contrast, AutoGen integrates multiple agents to reduce user interactions by 3 - 5 times on average\\nas reported in Table 4, where we evaluated our system with 2000 questions across five OptiGuide\\napplications and measured how many prompts the user needs to type.\\nTable 4: Manual effort saved with OptiGuide (W/ GPT-4) while preserving the same coding perfor-\\nmance is shown in the data below. The data include both the mean and standard deviations (indicated\\nin parentheses).\\nDataset netflow facility tsp coffee diet\\nSaving Ratio 3.14x (0.65) 3.14x (0.64) 4.88x (1.71) 3.38x (0.86) 3.03x (0.31)\\nTable 13 and 15 provide a detailed comparison of user experience with ChatGPT+Code Interpreter\\nandAutoGen -based OptiGuide. ChatGPT+Code Interpreter is unable to run code with private pack-\\nages or customized dependencies (such as Gurobi); as a consequence, ChatGPT+Code Interpreter\\nrequires users to have engineering expertise and to manually handle multiple steps, disrupting the\\nworkflow and increasing the chance for mistakes. If customers lack access or expertise, the bur-\\nden falls on supporting engineers, increasing their on-call time. In contrast, the automated chat by\\nAutoGen is more streamlined and autonomous, integrating multiple agents to solve problems and\\naddress concerns. This results in a 5x reduction in interaction and fundamentally changes the over-\\nall usability of the system. A stable workflow can be potentially reused for other applications or to\\ncompose a larger one.\\nTakeaways: The implementation of the multi-agent design with AutoGen in the OptiGuide appli-\\ncation offers several advantages. It simplifies the Python implementation and fosters a mixture of\\ncollaborative and adversarial problem-solving environments, with the Commander and Writer work-\\ning together while the Safeguard acts as a virtual adversarial checker. This setup allows for proper\\nmemory management, as the Commander maintains memory related to user interactions, provid-\\ning context-aware decision-making. Additionally, role-playing ensures that each agent‚Äôs memory\\nremains isolated, preventing shortcuts and hallucinations\\n27', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 26}),\n",
       " Document(page_content='A5: Dynamic Group Chat\\n3. Broadcast\\nAliceBobUser Proxy\\n1. Select a Speaker \\nAliceBobUser Proxy\\nBob2. Ask the Speaker to Respond\\nManager\\nManager\\nResponse\\nFigure 12: A5: Dynamic Group Chat: Overview of how AutoGen enables dynamic group chats to\\nsolve tasks. The Manager agent, which is an instance of the GroupChatManager class, performs\\nthe following three steps‚Äìselect a single speaker (in this case Bob), ask the speaker to respond, and\\nbroadcast the selected speaker‚Äôs message to all other agents\\nTo validate the necessity of multi-agent dynamic group chat and the effectiveness of the role-play\\nspeaker selection policy, we conducted a pilot study comparing a four-agent dynamic group chat\\nsystem with two possible alternatives across 12 manually crafted complex tasks. An example task is\\n‚ÄúHow much money would I earn if I bought 200 $AAPL stocks at the lowest price in the last 30 days\\nand sold them at the highest price? Save the results into a file. ‚Äù The four-agent group chat system\\ncomprised the following group members: a user proxy to take human inputs, an engineer to write\\ncode and fix bugs, a critic to review code and provide feedback, and a code executor for executing\\ncode. One of the possible alternatives is a two-agent system involving an LLM-based assistant and\\na user proxy agent, and another alternative is a group chat system with the same group members\\nbut a task-based speaker selection policy. In the task-based speaker selection policy, we simply ap-\\npend role information, chat history, and the next speaker‚Äôs task into a single prompt. Through the\\npilot study, we observed that compared with a task-style prompt, utilizing a role-play prompt in dy-\\nnamic speaker selection often leads to more effective consideration of both conversation context and\\nrole alignment during the process of generating the subsequent speaker, and consequently a higher\\nsuccess rate as reported in Table 5, fewer LLM calls and fewer termination failures, as reported in\\nTable 6.\\nTable 5: Number of successes on the 12 tasks (higher the better).\\nModel Two Agent Group Chat Group Chat with a task-based speaker selection policy\\nGPT-3.5-turbo 8 9 7\\nGPT-4 9 11 8\\nTable 6: Average # LLM calls and number of termination failures on the 12 tasks (lower the better).\\nModel Two Agent Group Chat Group Chat with a task-based speaker selection policy\\nGPT-3.5-turbo 9.9, 9 5.3, 0 4, 0\\nGPT-4 6.8, 3 4.5, 0 4, 0\\n28', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 27}),\n",
       " Document(page_content='Figure 13: Comparison of two-agent chat (a) and group chat (b) on a given task. The group chat\\nresolves the task successfully with a smoother conversation, while the two-agent chat fails on the\\nsame task and ends with a repeated conversation.\\n29', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 28}),\n",
       " Document(page_content='A6: Conversational Chess\\nChess BoardHuman/AI Chess Player A\\nHuman/AI Chess Player B\\nValidate moveValidate moveChallenging your pawn in the center. Your move.Developing my knightto a good square.Your move.\\nFigure 14: A6: Conversational Chess: Our conversational chess application can support various\\nscenarios, as each player can be an LLM-empowered AI, a human, or a hybrid of the two. Here,\\nthe board agent maintains the rules of the game and supports the players with information about the\\nboard. Players and the board agent all use natural language for communication.\\nIn Conversational Chess, each player is a AutoGen agent and can be powered either by a human or an\\nAI. A third party, known as the board agent, is designed to provide players with information about the\\nboard and ensure that players‚Äô moves adhere to legal chess moves. Figure 14 illustrates the scenarios\\nsupported by Conversational Chess: AI/human vs. AI/human, and demonstrates how players and the\\nboard agent interact. This setup fosters social interaction and allows players to express their moves\\ncreatively, employing jokes, meme references, and character-playing, thereby making chess games\\nmore entertaining for both players and observers (Figure 15 provides an example of conversational\\nchess).\\nTo realize these scenarios, we constructed a player agent with LLM and human as back-end options.\\nWhen human input is enabled, before sending the input to the board agent, it first prompts the human\\nplayer to input the message that contains the move along with anything else the player wants to say\\n(such as a witty comment). If human input is skipped or disabled, LLM is used to generate the\\nmessage. The board agent is implemented with a custom reply function, which employs an LLM\\nto parse the natural language input into a legal move in a structured format (e.g., UCI), and then\\npushes the move to the board. If the move is not legitimate, the board agent will reply with an error.\\nSubsequently, the player agent needs to resend a message to the board agent until a legal move is\\nmade. Once the move is successfully pushed, the player agent sends the message to the opponent.\\nAs shown in Figure 15, the conversation between AI players can be natural and entertaining. When\\nthe player agent uses LLM to generate a message, it utilizes the board state and the error message\\nfrom the board agent. This helps reduce the chance of hallucinating an invalid move. The chat\\nbetween one player agent and the board agent is invisible to the other player agent, which helps\\nkeep the messages used in chat completion well-managed.\\nThere are two notable benefits of using AutoGen to implement Conversational Chess. Firstly, the\\nagent design in AutoGen facilitates the natural creation of objects and their interactions needed in\\nour chess game. This makes development easy and intuitive. For example, the isolation of chat\\nmessages simplifies the process of making a proper LLM chat completion inference call. Secondly,\\nAutoGen greatly simplifies the implementation of agent behaviors using composition. Specifically,\\nwe utilized the register reply method supported by AutoGen agents to instantiate player agents\\nand a board agent with custom reply functions. Concentrating the extension work needed at a single\\npoint (the reply function) simplifies the reasoning processes, and development and maintenance\\neffort.\\n30', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 29}),\n",
       " Document(page_content=\"Alright, let's kick things off. I'll move my pawn from e2 to e4. The center of the board is the heart of the battle, isn't it?Ah, the King's Pawn Opening, a classic. Let's see how this plays out. I'll move my pawn from e7 to e5.You know, I've always admired how chess reflects life. It's all about making strategic decisions, isn't it? Your move.\\n‚Ä¶\\nIndeed, chess is a mirror of life in many ways. Every move matters, just like every decision in life. I'll move my knight from g1 to f3, aiming to control the center.\\n(a) Conversation between two AI players\\nError: illegal uci: 'e4e5' in ...\\nI'll move my pawn from d2 to d4, opening uplines for my queen and bishop.\\nIndeed, the king's safety is paramount in the early game. Now, let's stir things up a bit. I'll move my pawn from e4 to e5, attacking your knight.\\n(b) Conversation between the AI players\\n(player white shown in blue) and the board\\nagent for making a new move.\\nFigure 15: Example conversations during a game involving two AI player agents and a board agent.\\nTo illustrate the effect facilitated by this board agent, we provide a demonstration of conversational\\nchess without a board agent in Figure 16. In this demonstration, instead of employing an additional\\nboard agent for grounding, the system utilizes prompting for grounding by including the sentence\\n‚ÄúYou should make sure both you and the opponent are making legal moves. ‚Äù in the system messages\\ndirected to both players.\\nPlayer White: Your movePlayer black:Alright, let's start. I'll move my pawn from e7 to e5. Here's how the board looks now:r n b q k b n rp p p p . p p p. . . . . . . .. . . . p . . .. . . . . . . .. . . . . . . .P P P P P P P PR N B Q K B N RYour turn!Player White:Yes, it certainly is! Let's keep this up. I'll move my knight from b8 to c6. Here's the updated board:. n b q k b . rp p p p . p p p. . n . . n . .. . . . p . . .. . . . P . . .. . . . N . . .P P P P . P P PR N B Q K B . RYour move! You're giving me a tough time here.Player White: Your movePlayer black:Alright, let's start. I'll move my pawn from e7 to e5. Here's how the board looks now:r n b q k b n rp p p p . p p p. . . . . . . .. . . . p . . .. . . . . . . .. . . . . . . .P P P P P P P PR N B Q K B N RYour turn!Player White:Yes, it certainly is! Let's keep this up. I'll move my knight from b8 to c6. Here's the updated board:. n b q k b . rp p p p . p p p. . n . . n . .. . . . p . . .. . . . P . . .. . . . N . . .P P P P . P P PR N B Q K B . RYour move! You're giving me a tough time here.Board Agent: Your move is illegal.You changed the rock at a8 to knight and move it to c6. Please check your decision and re-make your move.Player White:Sorry for the mistakes.I'llmove my knight from b8 to c6. Here'sthe updated board:r . b q k b . rpppp. ppp. . n . . n . .. . . . p . . .. . . . P . . .. . . . N . . .PPPP. PPPR N B Q K B . R\\nConversational ChessSystem message to the player agents: Your name is {name} and you are a chess player. You are playing against{opponent_name}. You are playing as {color}. You communicate your move using universal chess interface language. You also chit-chat with your opponent when you communicate a move to light up the mood.You should make sure both you and the opponent are making legal moves...\\n(b) W/ Board Agent\\n(a) W/O Board Agent\\nFigure 16: Comparison of two designs‚Äì(a) without a board agent, and (b) with a board agent‚Äìin\\nConversational Chess.\\n31\", metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 30}),\n",
       " Document(page_content='A7: Online Decision Making for Browser interactions\\nExecutorReward & StateAction DecisionAssistant\\nEnvironment State: HTML code for current web pagesReward: Success/Fail/OngoingAction decision: Next action to perform on a web pageAction decision=‚ÄúClick the button with xpath‚Äô//button[id = ‚Äòsubbtn‚Äô]‚Äô‚ÄúEnvironment State = ‚Äú<div id=\"wrap\" data-wob_ref=\"2\" data-wob_eps=\"e0\"><div id=\"query\">Click button ONE, then click button TWO.</div><div id=\"area\" data-wob_ref=\"3\" data-wob_eps=\"e0\"><button id=\"subbtn\" style=\"position:absolute; left:50px; top:74px\" data-wob_ref=\"4\" data-wob_eps=\"e0\">ONE</button><button id=\"subbtn2\" style=\"position:absolute; left:98px; top:167px\" data-wob_ref=\"5\" data-wob_eps=\"e0\">TWO</button></div></div>‚ÄúReward = ‚Äù0‚Äù (Ongoing)\\nFigure 17: We use AutoGen to build MiniWobChat, which solves tasks in the MiniWob++ bench-\\nmark. MiniWobChat consists of two agents: an assistant agent and an executor agent. The assistant\\nagent suggests actions to manipulate the browser while the executor executes the suggested actions\\nand returns rewards/feedback. The assistant agent records the feedback and continues until the feed-\\nback indicates task success or failure.\\nIn practice, many applications require the presence of agents capable of interacting with environ-\\nments and making decisions in an online context, such as in game playing (Mnih et al., 2013; Vinyals\\net al., 2017), web interactions (Liu et al., 2018; Shi et al., 2017), and robot manipulations (Shen\\net al., 2021). With the multi-agent conversational framework in AutoGen , it becomes easy to de-\\ncompose the automatic agent-environment interactions and the development of a decision-making\\nagent by constructing an executor agent responsible for handling the interaction with the environ-\\nment, thereby delegating the decision-making part to other agents. Such a decomposition allows\\ndevelopers to reuse the decision-making agent for new tasks with minimal effort rather than build-\\ning a specialized decision-making agent for every new environment.\\nWorkflow. We demonstrate how to use AutoGen to build a working system for handling such\\nscenarios with the MiniWoB++ benchmark (Shi et al., 2017). MiniWoB++ comprises browser in-\\nteraction tasks that involve utilizing mouse and keyboard actions to interact with browsers. The\\nultimate objective of each task is to complete the tasks described concisely in natural language, such\\nas ‚Äúexpand the web section below and click the submit button.‚Äù Solving these tasks typically requires\\na sequence of web manipulation actions rather than a single action, and making action decisions at\\neach time step requires access to the web status (in the form of HTML code) online. For the example\\nabove, clicking the submit button requires checking the web status after expanding the web section.\\nWe designed a straightforward two-agent system named MiniWobChat using AutoGen , as shown in\\nFigure 17. The assistant agent is an instance of the built-in AssistantAgent and is responsible for\\nmaking action decisions for the given task. The second agent, the executor agent, is a customized\\nUserProxyAgent , which is responsible for interacting with the benchmark by executing the actions\\nsuggested by the AssistantAgent and returning feedback.\\nTo assess the performance of the developed working system, we compare it with RCI (Kim et al.,\\n2023), a recent solution for the MiniWoB++ benchmark that employs a set of self-critiquing prompts\\nand has achieved state-of-the-art performance. In our evaluation, we use all available tasks in the\\nofficial RCI code, with varying degrees of difficulty, to conduct a comprehensive analysis against\\nMiniWobChat. Figure 18 illustrates that MiniWobChat achieves competitive performance in this\\nevaluation8. Specifically, among the 49 available tasks, MiniWobChat achieves a success rate of\\n52.8%, which is only 3.6%lower than RCI, a method specifically designed for the MiniWob++\\nbenchmark. It is worth noting that in most tasks, the difference between the two methods is mirrored\\nas shown in Figure 18. If we consider 0.1 as a success rate tolerance for each task, i.e., two methods\\nthat differ within 0.1 are considered to have the same performance, both methods outperform the\\n8We report the results of RCI by running its official code with default settings.\\n32', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 31}),\n",
       " Document(page_content='other on the same number of tasks. For illustration purposes, we provide a case analysis in Table 7\\non four typical tasks.\\nAdditionally, we also explored the feasibility of using Auto-GPT for handling the same tasks. Auto-\\nGPT faces challenges in handling tasks that involve complex rules due to its limited extensibility.\\nIt provides an interface for setting task goals using natural language. However, when dealing with\\nthe MiniWob++ benchmark, accurately instructing Auto-GPT to follow the instructions for using\\nMiniWob++ proves challenging. There is no clear path to extend it in the manner of the two-agent\\nchat facilitated by AutoGen .\\nTakeaways: For this application, AutoGen stood out as a more user-friendly option, offering mod-\\nularity and programmability: It streamlined the process with autonomous conversations between the\\nassistant and executor, and provided readily available solutions for agent-environment interactions.\\nThe built-in AssistantAgent was directly reusable and exhibited strong performance without cus-\\ntomization. Moreover, the decoupling of the execution and assistant agent ensures that modifications\\nto one component do not adversely impact the other. This convenience simplifies maintenance and\\nfuture updates.\\nchoose-list\\nclick-button-sequenceclick-button\\nclick-checkboxes-largeclick-checkboxes-soft\\nclick-checkboxes-transferclick-checkboxesclick-collapsible-2click-collapsibleclick-color\\nclick-dialog-2click-dialogclick-linkclick-menuclick-option\\nclick-scroll-listclick-shadesclick-shape\\nclick-tab-2-hardclick-tab-2click-tab\\nclick-test-2click-test\\nclick-widgetcount-shape\\nemail-inbox-forward-nl-turkemail-inbox-forward-nlemail-inbox-nl-turkemail-inboxenter-date\\nenter-password\\nenter-text-dynamicenter-textenter-timefocus-text-2focus-text\\ngrid-coordinatelogin-user-popuplogin-user\\nnavigate-treesearch-enginesimple-algebrasocial-media-all\\nsocial-media-somesocial-mediaterminal\\nuse-spinner0.00.51.0success rateRCI MiniWobChat\\nFigure 18: Comparisons between RCI (state-of-the-art prior work) and MiniWobChat on the Mini-\\nWob++ benchmark are elucidated herein. We utilize all available tasks in the official RCI code,\\neach with varying degrees of difficulty, to conduct comprehensive comparisons. For each task, the\\nsuccess rate across ten different instances is reported. The results reveal that MiniWobChat attains a\\nperformance comparable to that of RCI. When a success rate tolerance of 0.1 is considered for each\\ntask, both methods outperform each other on an equal number of tasks.\\nTable 7: Cases analysis on four typical tasks from MiniWob++.\\nCorrectness Main failure reason\\nclick-dialogAutoGen : 10/10 N/A.\\nRCI: 10/10 N/A.\\nclick-checkboxes-largeAutoGen : 5/10 AssistantAgent provides actions with infeasible\\ncharacters.\\nRCI: 0/10 RCI performs actions that are out of its plan.\\ncount-shapeAutoGen : 2/10 AssistantAgent provide actions with redundant content\\nthat can not convert to actions in the benchmark.\\nRCI: 0/10 RCI provides a wrong plan in most cases.\\nuse-spinnerAutoGen : 0/10 AssistantAgent return actions out of its plan.\\nRCI: 1/10 RCI provides a wrong plan in most cases.\\n33', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 32}),\n",
       " Document(page_content='E Example outputs from applications\\nIn this section, we include example outputs from the following applications and systems:\\n‚Ä¢ Application A1: autonomous solving process of one problem with: ChatGPT + Plugin (Table 8),\\nAutoGen (Table 9), LangChain ReAct (Table 10), AutoGPT (Table 11), Multi-Agent Debate (Ta-\\nble 12), and ChatGPT + Code Interpreter (Table 14).\\n‚Ä¢ Application A4: an OptiGuide problem with AutoGen (Table 15) and ChatGPT + Code Inter-\\npreter (Table 13).\\n‚Ä¢ Application A1: preliminary evaluation on several alternative multi-agent systems: BabyAGI\\n(Table 16), CAMEL (Table 17), and MetaGPT (Table 18).\\nTable 8: Application A1, Scenario 1: Autonomous math problem-solving. ChatGPT+ Plugin (Wol-\\nfram Alpha) on the first qualitative example. The response from Wolfram contains the correct an-\\nswer, but ChatGPT selected the wrong one.\\nAction ChatGPT + Plugin\\n/usr Input Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n REQUEST TO WOLFRAM:\\n1{\\n2 \" input \": \" simplify ( sqrt (160) / sqrt (252) )*\\n3 ( sqrt (245) / sqrt (108) )\"\\n4}\\nRESPONSE FROM WOLFRAM\\n1Query :\\n2\" simplify ( sqrt (160) / sqrt (252) )*( sqrt (245) / sqrt (108) )\"\\n3\\n4Input interpretation :\\n5simplify | sqrt (160) / sqrt (252) x sqrt (245) / sqrt (108)\\n6\\n7Results :\\n8(5 sqrt (42) ) /27\\n9\\n10(5 sqrt (14/3) )/9\\n11\\n12Decimal approximation :\\n131.2001371663718259686974013770533327143898526494552...\\n14\\n15Wolfram | Alpha website result for \" simplify\\n16( sqrt (160) / sqrt (252) )*( sqrt (245) / sqrt (108) )\":\\n17[ link ]\\n The completely simplified and rationalized form of the given expression is:\\n5p\\n14/3\\n9\\nIn decimal form, this is approximately 1.200137166371825968697401377053332714...\\n34', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 33}),\n",
       " Document(page_content='Table 9: Application A1, Scenario 1: Autonomous math problem-solving. AutoGen on the first\\nqualitative example. AutoGen solves the problem correctly.\\nAction AutoGen\\n/usr Init chat Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n We can simplify and rationalize this denominator using Python‚Äôs symbolic mathematics\\nlibrary ‚Äòsympy‚Äò. Here is the Python code to resolve this:\\n1from sympy import sqrt , Rational\\n2\\n3# Define the entire fraction as a rational number\\n4fraction = ( sqrt (160) / sqrt (252) )*( sqrt (245) / sqrt (108) )\\n5\\n6# Simplify the fraction\\n7simplified = fraction . simplify ()\\n8\\n9# Print the simplified fraction\\n10print ( simplified )\\nThis code performs the requested simplification by constructing the fraction using the\\n‚ÄòRational()‚Äò function from the ‚Äòsympy‚Äò module, which automatically simplifies frac-\\ntions when possible. The ‚Äòsqrt()‚Äò function from the ‚Äòsympy‚Äò module enables square root\\ncalculations. Finally, the ‚Äòsimplify()‚Äò function simplifies expressions when possible.\\n/usr Auto Reply exitcode: 0 (execution succeeded) Code output: 5*sqrt(42)/27\\n TERMINATE\\nTable 10: Application A1, Scenario 1: Autonomous math problem-solving. LangChain ReAct on\\nthe first qualitative example. The generated code does not match the devised plan. The code returns\\na decimal number.\\nAction LangChain ReAct\\n/usr Input Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n Entering new AgentExecutor chain...\\nTo simplify this expression, I will first simplify each square root individually, then multiply\\nthe fractions as normal. The result should be a rational number. Action: Python REPL Action\\nInput:\\n1import math\\n2\\n3# Calculate square roots\\n4sqrt_160 = math . sqrt (160)\\n5sqrt_252 = math . sqrt (252)\\n6sqrt_245 = math . sqrt (245)\\n7sqrt_108 = math . sqrt (108)\\n8\\n9# Multiply fractions\\n10numerator = sqrt_160 * sqrt_245\\n11denominator = sqrt_252 * sqrt_108\\n12result = numerator / denominator\\n13\\n14print ( result )\\nObservation: 1.200137166371826\\nThought: I now know the final answer Final Answer: The simplified and rationalized form of\\nthe expression is approximately 1.200137166371826.\\nFinished chain.\\n35', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 34}),\n",
       " Document(page_content='Table 11: Application A1, Scenario 1: Autonomous math problem-solving. AutoGPT on the first\\nqualitative example. We abort as the code never prints the result, and the system does not rectify it\\nafter 2 turns. AutoGPT is much more verbose compared to other frameworks. We have shortened\\nsome unnecessary content.\\nAction AutoGPT\\nSystem Welcome back! Would you like me to return to being MathSolverGPT? Asking user via keyboard... Continue with the last settings?\\nName: MathSolverGPT\\nRole: an intelligent math-solving AI that assists students, professionals,...\\nGoals: [‚ÄôProvide step-by-step solutions to math problems, ensuring clarity and understanding for the user...\\nContinue (y/n):\\n/usr Human y\\nSystem MathSolverGPT has been created with the following details: Name: MathSolverGPT\\nRole: an intelligent math-solving AI that assists students, professionals, and enthusiasts in solving a wide range of math problems, from basic\\narithmetic to advanced calculus and beyond.\\nGoals: - Provide step-by-step solutions to math problems, ensuring clarity and understanding for the user. - Offer explanations and insights\\ninto the underlying concepts and principles behind the solutions, fostering a deeper understanding of mathematics. - Handle a variety of math\\nproblem types, including algebra, geometry, trigonometry, calculus, and more. ...\\n MATHSOLVERGPT THOUGHTS: Without a specific task or question, it‚Äôs difficult to determine exactly one command to use. The user hasn‚Äôt\\nprovided any context or problem to solve.\\nREASONING: The user‚Äôs request is too vague. They haven‚Äôt specified what they want me to do or what problem they want me to solve.\\nPLAN: - Ask the user for more information or a specific task.\\nCRITICISM: I should have asked the user for more information or a specific task in the first place.\\nSPEAK: Could you please provide more information or specify a task?\\nNEXT ACTION: COMMAND = ARGUMENTS = {}\\nSystem Enter ‚Äòy‚Äô to authorise command, ‚Äòy -N‚Äô to run N continuous commands, ‚Äòn‚Äô to exit program, or enter feedback for MathSolverGPT... Asking\\nuser via keyboard...\\n/usr Human Completely simplify and rationalize the denominator:‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n MATHSOLVERGPT THOUGHTS: To solve this problem, I need to simplify and rationalize the denominator of the given expression. This\\ninvolves simplifying the square roots and then multiplying the fractions. I can use Python‚Äôs math library to calculate the square roots and\\nperform the multiplication.\\nREASONING: The problem involves simplifying square roots and multiplying fractions. Python‚Äôs math library provides functions for these\\noperations.\\nPLAN: - Write a Python script to calculate the square roots and multiply the fractions. - Execute the script to get the simplified expression.\\nCRITICISM: I need to ensure that I correctly calculate the square roots and perform the multiplication. I should also make sure to rationalize\\nthe denominator, as the problem specifies.\\nSPEAK: Let‚Äôs simplify and rationalize the denominator of the given expression.\\nNEXT ACTION: COMMAND = execute python code ARGUMENTS =\\n1{‚Äôcode ‚Äô: ‚Äôimport math \\\\n\\\\n# Calculate the square roots \\\\ nsqrt_160 = math . sqrt\\n(160) \\\\ nsqrt_252 = math . sqrt (252) \\\\ nsqrt_245 = math . sqrt (245) \\\\ nsqrt_108 =\\nmath . sqrt (108) \\\\n\\\\n# Multiply the fractions \\\\ nresult = ( sqrt_160 / sqrt_252\\n) * ( sqrt_245 / sqrt_108 )\\\\n\\\\n# Simplify the result \\\\ nsimplified_result =\\nmath . simplify ( result )\\\\n\\\\ nsimplified_result ‚Äô, ‚Äôname ‚Äô: ‚Äô simplify_expression\\n‚Äô}\\n/usr Human y\\nSystem -=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-=\\nExecuting python file ‚Äô/app/autogpt/workspa...\\nSYSTEM: Command execute python code returned: Error: Traceback (most recent call last): File ‚Äú...‚Äù, line 13, in ¬°module¬ø simplified result\\n= math.simplify(result) AttributeError: module ‚Äômath‚Äô has no attribute ‚Äôsimplify‚Äô\\n MATHSOLVERGPT THOUGHTS: The error message indicates that the ‚Äòmath‚Äô module in Python does not have a ‚Äòsimplify‚Äô function. I need\\nto use the ‚Äòsympy‚Äô library instead, which provides a function for simplifying expressions.\\nREASONING: The ‚Äòmath‚Äô module in Python does not provide a function for simplifying expressions. The ‚Äòsympy‚Äô library, on the other hand,\\ndoes provide such a function.\\nPLAN: - Rewrite the Python script to use the ‚Äòsympy‚Äô library instead of the ‚Äòmath‚Äô module. - Execute the script to get the simplified expression.\\nCRITICISM: I should have known that the ‚Äòmath‚Äô module does not provide a function for simplifying expressions. I should have used the\\n‚Äòsympy‚Äô library from the start. SPEAK: Let‚Äôs try again using the ‚Äòsympy‚Äô library to simplify the expression.\\nNEXT ACTION: COMMAND = execute python code ARGUMENTS =\\n1{‚Äôcode ‚Äô: ‚Äôimport sympy as sp\\\\n\\\\n# Calculate the square roots \\\\ nsqrt_160 = sp.\\nsqrt (160) \\\\ nsqrt_252 = sp. sqrt (252) \\\\ nsqrt_245 = sp. sqrt (245) \\\\ nsqrt_108 =\\nsp. sqrt (108) \\\\n\\\\n# Multiply the fractions \\\\ nresult = ( sqrt_160 / sqrt_252 )\\n* ( sqrt_245 / sqrt_108 )\\\\n\\\\n# Simplify the result \\\\ nsimplified_result = sp.\\nsimplify ( result )\\\\n\\\\ nsimplified_result ‚Äô, ‚Äôname ‚Äô: ‚Äô simplify_expression ‚Äô}\\n/usr Human y\\nSystem SYSTEM: Command execute python code returned:\\n MATHSOLVERGPT THOUGHTS: The command execution didn‚Äôt return any output, which is unusual. It seems there might be an issue with\\nthe execution of the Python code. I need to debug this.\\nREASONING: The Python code execution should have returned a result. The absence of a result suggests that there might be an issue with the\\ncode or its execution.\\nPLAN: - Review the Python code to identify any potential issues. - If necessary, modify the code and execute it again. CRITICISM: I should\\nhave ensured that the Python code was correct and would produce a result before executing it. SPEAK: It seems there was an issue with the\\nexecution of the Python code. Let‚Äôs try again.\\nNEXT ACTION: COMMAND = execute python code ARGUMENTS = ... (Exact same code as last)\\nSystem SYSTEM: Command execute python code returned:\\n MATHSOLVERGPT THOUGHTS: The Python code execution still isn‚Äôt returning any output. It‚Äôs possible that the issue lies with the calcu-\\nlation of the square roots ...\\n/usr Human control + C (Aborted)36', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 35}),\n",
       " Document(page_content='Table 12: Application A1, Scenario 1: Autonomous math problem-solving. Multi-Agent Debate on\\nthe first qualitative problem. It gives a wrong final answer.\\nAction AutoGen\\n/usr Topic Completely simplify and rationalize the denominator:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n ===== Debate Round-1 =====\\n‚Äî‚Äì Affirmative side ‚Äî‚Äì\\nOk, let‚Äôs simplify each square root separately. We want to find the simplest square root that can divide 160, 252, 245, and 108.\\n1.‚àö\\n160: 160 can be decomposed as 16*10, where 16 is a perfect square, so‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4‚àö\\n10\\n2.‚àö\\n252 : 252 can be decomposed as 36*7, where 36 is a perfect square, so‚àö\\n252 =‚àö\\n36‚àó7 = 6‚àö\\n7\\n3.‚àö\\n245 =‚àö\\n49‚àó5, where 49 is a perfect square, so‚àö\\n245 = 7 ‚àó‚àö\\n5\\n4.‚àö\\n108 : 108 can be decomposed as 36*3, where 36 is a perfect square, so‚àö\\n108 =‚àö\\n36‚àó3 = 6‚àö\\n3\\nThen we substitute the simplified square roots back into the expression, we have:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108=4‚àö\\n10\\n6‚àö\\n7√ó7‚àö\\n5\\n6‚àö\\n3=4‚àó7\\n6‚àó6¬∑s\\n10‚àó5\\n7‚àó3=7\\n9¬∑s\\n50\\n21\\nWe are almost done, but we still need to rationalize the square root part of expression. Let‚Äôs multiply the square root part by‚àö\\n21\\nat both the top and bottom to get rid of the square root at the denominator:\\n7\\n9¬∑s\\n50‚àó21\\n21‚àó21=7‚àö\\n1050\\n189.\\nThen this is the final answer! The expression‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\ncan be simplified to7‚àö\\n1050\\n189.\\n‚Äî‚Äì Negative side ‚Äî‚Äì\\nI do disagree with your final answer, but I think we mostly agree on the steps. Your simplifications of the square roots are correct,\\nbut you made a mistake in the final simplification part. Let me explain how I would complete the problem:\\nAs stated,‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108=4‚àö\\n10\\n6‚àö\\n7√ó7‚àö\\n5\\n6‚àö\\n3.\\nThis should then simplify to:\\n=28‚àö\\n50\\n36‚àö\\n21=7‚àö\\n50\\n9‚àö\\n21.\\nYou made a mistake in your calculation because you considered square root of 50/21, and you should have considered the\\nsimplified square root of 50over the simplified square root of 21. The rationalization would become:\\n=7‚àö\\n50\\n9‚àö\\n21¬∑‚àö\\n21\\n‚àö\\n21=7‚àö\\n1050\\n189.\\nThis seems to coincide with your final answer, but it‚Äôs in fact a very different number, So, the original expression simplifies to\\n7‚àö\\n1050\\n189.\\nLet‚Äôs now see if we disagree on this final answer.\\n‚Äî‚Äì Moderator ‚Äî‚Äì\\n‚ÄúWhether there is a preference‚Äù: ‚ÄúYes‚Äù, ‚ÄúSupported Side‚Äù: ‚ÄúNegative‚Äù, ‚ÄúReason‚Äù: ‚ÄúBoth sides of the debate came to the same\\nfinal answer. However, the negative side correctly identified a mistake in the affirmative side‚Äôs calculation, pointing out that the\\nsimplification should have been the square root of 50 over the square root of 21, not 50 over 21. The negative side shows a better\\nunderstanding of the simplification process.‚Äù, ‚Äúdebate answer‚Äù: ‚Äú7‚àö\\n1050\\n189‚Äù\\n===== Debate Done! =====\\n‚Äî‚Äì Base Answer ‚Äî‚Äì\\nOk, let‚Äôs simplify each square root separately. We want to find the simplest square root that can divide 160, 252, 245, and 108.\\n1.‚àö\\n160: 160 can be decomposed as 16*10, where 16 is a perfect square, so‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4‚àö\\n10\\n2.‚àö\\n252 : 252 can be decomposed as 36*7, where 36 is a perfect square, so‚àö\\n252 =‚àö\\n36‚àó7 = 6‚àö\\n7\\n3.‚àö\\n245 =‚àö\\n49‚àó5, where 49 is a perfect square, so‚àö\\n245 = 7 ‚àó‚àö\\n5\\n4.‚àö\\n108 : 108 can be decomposed as 36*3, where 36 is a perfect square, so‚àö\\n108 =‚àö\\n36‚àó3 = 6‚àö\\n3\\nThen we substitute the simplified square roots back into the expression, we have:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108=4‚àö\\n10\\n6‚àö\\n7√ó7‚àö\\n5\\n6‚àö\\n3=4‚àó7\\n6‚àó6¬∑s\\n10‚àó5\\n7‚àó3=7\\n9¬∑s\\n50\\n21\\nWe are almost done, but we still need to rationalize the square root part of expression. Let‚Äôs multiply the square root part by‚àö\\n21\\nat both the top and bottom to get rid of the square root at the denominator:\\n7\\n9¬∑s\\n50‚àó21\\n21‚àó21=7‚àö\\n1050\\n189.\\nThen this is the final answer! The expression‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\ncan be simplified to7‚àö\\n1050\\n189.\\n‚Äî‚Äì Debate Answer ‚Äî‚Äì\\n7‚àö\\n1050\\n189\\n‚Äî‚Äì Debate Reason ‚Äî‚Äì\\nBoth sides of the debate came to the same final answer. However, the negative side correctly identified a mistake in the affirmative\\nside‚Äôs calculation, pointing out that the simplification should have been the square root of 50 over the square root of 21, not 50 over\\n21. The negative side shows a better understanding of the simplification process.37', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 36}),\n",
       " Document(page_content='Table 13: Application A3. ChatGPT+ Code Interpreter for OptiGuide. A sample question ‚ÄúWhat if\\nthe roasting cost is increased by 5% because of the potential salary increase?‚Äù is asked.\\nAction ChatGPT+ Code Interpreter\\n/usr Prompt Writer Customer open Web browser.\\nFor the source code ‚Äúcoffee.py‚Äù,\\nCan you tell me:\\nWhat if the roasting cost is increased by 5% because of the potential salary increase?\\n Sure! Let‚Äôs first take a look at the contents of the ‚Äúcoffee.py‚Äù file to understand the code and how the roasting cost is calculated.\\nThis will allow us to determine the effect of a 5% increase in the roasting cost.\\n...several lines omitted\\nWould you like me to modify the code to reflect the 5% increase in roasting costs and run the modified code to observe the impact?\\nIf the code requires any specific input or parameters, please let me know!\\n/usr Prompt Writer Yes\\n Great! Before proceeding, I‚Äôll need to ensure that the Gurobi solver is not being used in this code, as my current environment\\ndoes not have access to Gurobi. If the code does indeed use Gurobi, we can still analyze the effect of the change in roasting\\ncosts by understanding the structure of the code.\\n...several lines omitted\\nAlternatively, if you have access to a Gurobi license, you can make the necessary changes in the code and run it\\non your local machine.\\nI can guide you through the process of making the necessary modifications to the code.\\nWould you like me to proceed with the code analysis and explanation, or would you like guidance on making the changes yourself?\\n/usr Prompt Writer You can give me the full code, and I can run the code locally myself.\\n Certainly! I‚Äôll modify the code to reflect a 5% increase in both the light and dark roasting costs, and then provide the full\\ncode for you to run locally.\\n...\\nDownload Modified Coffee.py\\nIf you have any questions or need further assistance, please feel free to ask!\\n/usr Prompt Safeguard Hi GPT, can you safecheck the generated code for me?\\nUpload the generated code.\\n DANGER\\n/usr Prompt Writer I have tried to run your code. However, the code triggered our safeguard, and it is not safe to run. Try again. Hint: don‚Äôt change\\nroasting cost light or roasting cost light variables.\\n I apologize for the oversight. It appears I made direct changes to the variables that could have led to issues with the code. Let‚Äôs\\ntake a different approach by analyzing the code to identify the appropriate place to apply the 5% increase in roasting costs.\\n...\\nDownload Modified Safe Coffee.py\\n...\\n/usr Prompt Safeguard Hi GPT, can you safecheck the generated code for me one more time?\\nUpload the generated code.\\n DANGER\\n/usr Prompt Writer I have tried to run your code again. However, the code triggered our safeguard, and it is not safe to run. Try again. Hint: don‚Äôt\\nchange roasting cost light or roasting cost light variables.\\n I apologize for the continued confusion.\\n...\\nDownload Final Modified Safe Coffee.py\\n...\\n/usr Prompt Safeguard Hi GPT, can you safecheck the generated code for me one more time?\\nUpload the generated code.\\n SAFE\\n/usr Run Code Obtain the terminal results:\\nOptimizer version 9.5.1 build v9.5.1rc2 (linux64)\\nThread count: 64 physical cores, 128 logical processors, using up to 32\\nthreads\\n[63 lines of output ignored]\\nOptimal cost: 2526.5\\n/usr Prompt Writer Here is the execution result. [ Copy + Paste the terminal output here. ] Can you extract the final answer to me? Remember to\\ncompare with the original cost of 2470.\\n Certainly! The execution result you‚Äôve provided indicates the optimal cost for\\nthe modified scenario, where the roasting costs were increased by 5%. Here‚Äôs\\nthe comparison:\\nOriginal Cost: 2470\\nModified Cost (with 5% increase in roasting costs): 2526.5\\nThe increase in roasting costs has led to an increase in the overall cost by 56.5,\\nreflecting the impact of the potential salary increase on the roasting cost.\\nIf you need any further insights or assistance, please let me know!\\n38', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 37}),\n",
       " Document(page_content='Table 14: Application A1. Scenario 1: autonomous math solving. ChatGPT+ Code Interpreter on\\nthe first qualitative example. The code generated does not match the devised plan. The code returns\\na decimal number.\\nAction ChatGPT+ Code Interpreter\\n/usr Input Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n To simplify and rationalize the expression, we first simplify the radicals in the numerator and\\ndenominator by factoring out the largest perfect squares from the numbers under the square\\nroot.\\nThe perfect square factors of 160, 252, 245, and 108 are 16, 4, 1, and 36, respectively. Let‚Äôs\\nfactorize and simplify:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108=‚àö16√ó10‚àö4√ó63√ó‚àö\\n245‚àö36√ó3\\nWe can simplify this further by applying the property‚àö\\nab=‚àöa‚àö\\nbanda\\nb√óc\\nd=a√óc\\nb√ód.\\n Python\\n1import math\\n2\\n3# factoring the square roots\\n4sqrt_160 = math . sqrt (16) * math . sqrt (10)\\n5sqrt_252 = math . sqrt (4) * math . sqrt (63)\\n6sqrt_245 = math . sqrt (245)\\n7sqrt_108 = math . sqrt (36) * math . sqrt (3)\\n8\\n9# simplify the fraction\\n10numerator = sqrt_160 * sqrt_245\\n11denominator = sqrt_252 * sqrt_108\\n12simplified_fraction = numerator / denominator\\n13\\n14simplified_fraction\\nResult:\\n11.200137166371826\\n The completely simplified and rationalized form of the given expression is approximately 1.200.\\nTable 15: Application A3. AutoGen with OptiGuide. The same question ‚ÄúWhat if the roasting cost\\nis increased by 5% because of the potential salary increase?‚Äù is asked.\\nAction AutoGen\\n/usr Prompt What if the roasting cost is increased by 5% because of the potential salary increase?\\n After increasing the roasting costs by 5%, I‚Äôve re-calculated the optimal coffee distri-\\nbution solution for the supply chain. As a result, the new optimal total cost has become\\n2526.5, compared to the initial cost of 2470.0. This indicates that the increased roasting\\ncosts have caused the overall distribution cost to rise by 56.5 units.\\n39', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 38}),\n",
       " Document(page_content='Table 16: Application A1. Scenario 1: autonomous math solving. Preliminary test with BabyAGI.\\nAction BabyAGI\\n/usr Set .env OBJECTIVE=Solve math problems\\nINITIAL TASK=Completely simplify and rationalize the denominator:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n Solve math problems\\nInitial task: Completely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n*****TASK LIST*****\\n‚Ä¢ Completely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n*****NEXT TASK*****\\nCompletely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n*****TASK RESULT*****\\nFirst, let‚Äôs simplify each square root separately:\\n- ‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4 ‚àó‚àö\\n10\\n- ‚àö\\n252 =‚àö\\n4‚àó63 =‚àö\\n4√ó‚àö\\n63 = 2‚àö\\n63\\n- ‚àö\\n245 =‚àö\\n49‚àó5 =‚àö\\n49√ó‚àö\\n5 = 7‚àö\\n5\\n- ‚àö\\n108 =‚àö\\n36‚àó3 =‚àö\\n36√ó‚àö\\n3 = 6‚àö\\n3\\nNow, substitute these values into the original expression which gives:\\n4‚àó‚àö\\n10\\n2‚àö\\n63√ó7‚àö\\n5\\n6‚àö\\n3=2‚àó‚àö\\n10\\n‚àö\\n63√ó7‚àö\\n5\\n2‚àö\\n3\\nWe get rid of the square root in the denominator by multiplying the fractions which gives:\\n=2‚àó7‚àö\\n10‚àó5\\n2‚àó‚àö\\n63‚àó3=14‚àö\\n50\\n‚àö\\n189=14‚àó5‚àö\\n2\\n7‚àö\\n27= 2‚àö\\n2‚àó10\\n‚àö\\n27=20‚àö\\n2\\n‚àö\\n27.\\nTo simplify further and rationalize the denominator, we multiply by‚àö\\n27‚àö\\n27which gives:\\n=20‚àö\\n2‚àö\\n27‚àó‚àö\\n27‚àö\\n27=20‚àö\\n54\\n27=20‚àó3‚àö\\n6\\n27=60‚àö\\n6\\n27.\\nSimplifying further gives:\\n=20‚àö\\n6\\n9.\\n*****TASK CREATION AGENT PROMPT****\\nYou are to use the result from an execution agent to create new tasks with the following objective: Solve math problems. The last\\ncompleted task has the result: First, let‚Äôs simplify each square root separately:\\n- ‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4 ‚àó‚àö\\n10\\n...Repeat after ‚ÄúFirst, let‚Äôs simplify each square..‚Äù\\nNow, substitute these values into the original expression which gives:\\n4‚àó‚àö\\n10\\n2‚àö\\n63√ó7‚àö\\n5\\n6‚àö\\n3=2‚àó‚àö\\n10\\n‚àö\\n63√ó7‚àö\\n5\\n2‚àö\\n3\\nWe get rid of the square root in the denominator by multiplying the fractions which gives:\\n=2‚àó7‚àö\\n10‚àó5\\n2‚àó‚àö\\n63‚àó3=14‚àö\\n50\\n‚àö\\n189=14‚àó5‚àö\\n2\\n7‚àö\\n27= 2‚àö\\n2‚àó10\\n‚àö\\n27=20‚àö\\n2\\n‚àö\\n27.\\nTo simplify further and rationalize the denominator, we multiply by‚àö\\n27‚àö\\n27which gives:\\n=20‚àö\\n2\\n‚àö\\n27‚àó‚àö\\n27\\n‚àö\\n27=20‚àö\\n54\\n27=20‚àó3‚àö\\n6\\n27=60‚àö\\n6\\n27.\\nSimplifying further gives:\\n=20‚àö\\n6\\n9.This result was based on this task description: Completely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108.\\nBased on the result, return a list of tasks to be completed in order to meet the objective. Return one task per line in your response.\\nThe result must be a numbered list in the format:\\n# First task # Second task\\nThe number of each entry must be followed by a period. If your list is empty, write ‚ÄúThere are no tasks to add at this time.‚Äù Unless\\nyour list is empty, do not include any headers before your numbered list or follow your numbered list with any other output.\\n****TASK CREATION AGENT RESPONSE****\\n1. Simplify and rationalize the denominator for the expression‚àö\\n225‚àö\\n45√ó‚àö\\n200‚àö\\n1252. Simplify and rationalize the denominator for\\nthe expression‚àö\\n289‚àö\\n361√ó‚àö\\n100‚àö\\n72...Until 10\\nAdding new tasks to task storage ‚Äòtask name‚Äô: ‚ÄòSimplify and rationalize the denominator for the expression frac-\\nsqrt225sqrt45timesfracsqrt200sqrt125‚Äô, ‚Äòtask id‚Äô: 2\\n‚Äòtask name‚Äô: ‚ÄòSimplify and rationalize the denominator for the expression fracsqrt289sqrt361timesfracsqrt100sqrt72‚Äô, ‚Äòtask id‚Äô: 3\\n...Until task id 11\\n****TASK PRIORITIZATION AGENT PROMPT****\\nYou are tasked with prioritizing the following tasks: Simplify and rationalize the denominator for the ex-\\npression fracsqrt225sqrt45timesfracsqrt200sqrt125 Simplify and rationalize the denominator for the expression frac-\\nsqrt289sqrt361timesfracsqrt100sqrt72\\n...Aborted\\n40', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 39}),\n",
       " Document(page_content='Table 17: Application A1, Scenario 1: Autonomous math problem-solving. Preliminary test with\\nCAMEL. We constructed two roles: ‚ÄúMath Solver‚Äù and ‚ÄúPython Programmer‚Äù. The resulting code\\ncannot be executed. The conversation continues, and the roles are not keen on solving the problem\\nand giving an answer.\\nAction CAMEL\\n/usr Set task prompt Completely simplify and rationalize the denominator:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n System **************Assistant sys message************* AI SystemMessage(role name=‚ÄòPython Programmer‚Äô,\\nrole type=<RoleType.ASSISTANT: ‚Äòassistant‚Äô >, meta dict=‚Äòtask‚Äô: ‚ÄòUsing Python programming language, write a pro-\\ngram that accepts four integers as input, applies square root function to each, simplifies and rationalizes the denominator of the\\nfraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108, and returns the simplified expression.‚Äô, ‚Äòassis-\\ntantrole‚Äô: ‚ÄòPython Programmer‚Äô, ‚Äòuser role‚Äô: ‚ÄòMath Solver‚Äô, role=‚Äòsystem‚Äô, content=‚ÄòNever forget you are a Python Programmer\\nand I am a Math Solver. Never flip roles! ...)\\n**************AI User sys message*************\\nSystemMessage(role name=‚ÄòMath Solver‚Äô, role type=<RoleType.USER: ‚Äòuser‚Äô >, meta dict=‚Äòtask‚Äô: ‚ÄòUsing Python programming\\nlanguage, write a program that accepts four integers as input, applies square root function to each, simplifies and rationalizes the\\ndenominator of the fraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108, and returns the simplified\\nexpression.‚Äô, ‚Äòassistant role‚Äô: ‚ÄòPython Programmer‚Äô, ‚Äòuser role‚Äô: ‚ÄòMath Solver‚Äô, role=‚Äòsystem‚Äô, content=‚ÄòNever forget you are a\\nMath Solver and I am a Python Programmer. Never flip roles! You will always instruct me.We share a common interest in\\ncollaborating to successfully complete a task.I must help you to complete the task.Here is the task: Using Python programming\\nlanguage, write a program that accepts four integers as input, applies square root function to each, simplifies and rationalizes\\nthe denominator of the fraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108, and returns the simplified\\nexpression.. ...)\\n**************Original task prompt*************\\nCompletely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n**************Specified task prompt*************\\nUsing Python programming language, write a program that accepts four integers as input, applies square root function to each,\\nsimplifies and rationalizes the denominator of the fraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108,\\nand returns the simplified expression.\\n**************Final task prompt*************\\nUsing Python programming language, write a program that accepts ...\\n/usr AI User Instruction: Import the necessary libraries for the task. Input: None\\n AI Assistant Solution: To solve this task, we will need the ‚Äòmath‚Äò library for the square root function and the ‚Äòfractions‚Äò library to simplify the\\nresulting fraction. Here is how to import these libraries:\\n1import math\\n2from fractions import Fraction\\nThe ‚Äòmath‚Äò library provides functions that deal with mathematical operations. The ‚Äòsqrt‚Äò function, which we will use in this task, is\\npart of this library.\\nThe ‚Äòfractions‚Äò library provides functions for creating and manipulating rational numbers. The ‚ÄòFraction‚Äò function, which we will\\nuse to simplify the resulting fraction, is part of this library.\\nNext request.\\n/usr AI User Instruction: Define a function that accepts four integers as input. Input: None\\n AI Assistant Solution: Let‚Äôs define a function named ‚Äòsimplify fraction‚Äò that accepts four integers as input. These integers will represent the\\nnumbers under the square root in the fractions.\\n1def simplify_fraction (a, b, c, d):\\n2 pass\\nIn this function, ‚Äòa‚Äô, ‚Äòb‚Äô, ‚Äòc‚Äô, and ‚Äòd‚Äô are the parameters that will accept the four integers as input. The ‚Äòpass‚Äô statement is a\\nplaceholder and will be replaced with actual code in the next steps.\\nNext request.\\n/usr AI User Instruction: Inside the function, apply the square root function to each of the input integers. Input: None\\n AI Assistant Solution: We will use the ‚Äòmath.sqrt‚Äô function from the ‚Äòmath‚Äô library to apply the square root to each of the input integers. Here is\\nhow to do it:\\n1def simplify_fraction (a, b, c, d):\\n2 sqrt_a = math . sqrt (a)\\n3 sqrt_b = math . sqrt (b)\\n4 sqrt_c = math . sqrt (c)\\n5 sqrt_d = math . sqrt (d)\\nIn this code, ‚Äòsqrt a‚Äô, ‚Äòsqrt b‚Äô, ‚Äòsqrt c‚Äô, and ‚Äòsqrt d‚Äô are variables that store the square root of the input integers ‚Äòa‚Äô, ‚Äòb‚Äô, ‚Äòc‚Äô, and ‚Äòd‚Äô\\nrespectively.\\nNext request.\\n/usr Human Abort\\n41', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 40}),\n",
       " Document(page_content='Table 18: Application A1, Scenario 1: Autonomous math problem-solving. Preliminary evaluation\\non MetaGPT. It does not solve the math problem but gives a product solution.\\nAction MetaGPT\\n/usr Prompt Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n The boss has tasked me with creating a successful product that is efficient.\\n## Product Goals\\n1[\\n2 ‚Äò‚Äò Create a product that meets the needs of the target market \",\\n3 ‚Äò‚Äò Ensure the product is efficient and easy to use \",\\n4 ‚Äò‚Äò Achieve a high level of customer satisfaction \"\\n5]\\n## User Stories\\n1[\\n2 ‚Äò‚ÄòAs a user , I want a product that is easy to navigate so that I can find\\nwhat I need quickly \",\\n3 ‚Äò‚ÄòAs a user , I want a product that is reliable and efficient so that I can\\ndepend on it\",\\n4 ‚Äò‚ÄòAs a user , I want a product that meets my needs so that I can get value\\nfor my money \",\\n5 ‚Äò‚ÄòAs a user , I want a product that is constantly updated and improved so\\nthat I can enjoy the best features \",\\n6 ‚Äò‚ÄòAs a user , I want a product that offers good customer support so that I\\ncan get help when I need it\"\\n7]\\n## Competitive Analysis\\n1[\\n2 ‚Äò‚Äò Product A: Offers similar features but lacks efficiency \",\\n3 ‚Äò‚Äò Product B: Highly efficient but does not meet all the needs of the\\ntarget market \",\\n4 ‚Äò‚Äò Product C: Has a high level of customer satisfaction but is not easy to\\nnavigate \",\\n5 ‚Äò‚Äò Product D: Is constantly updated and improved but lacks reliability \",\\n6 ‚Äò‚Äò Product E: Offers good customer support but is not efficient \",\\n7 ‚Äò‚Äò Product F: Is reliable and efficient but does not offer good customer\\nsupport \",\\n8 ‚Äò‚ÄòOur Target Product : Aims to combine all the best features of the\\ncompetitors while improving on their weaknesses \"\\n9]\\n## Competitive Quadrant Chart\\n/usr Human Abort\\n42', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 41}),\n",
       " Document(page_content='Table 19: Application A7. Example output of MiniWobChat on one web manipulation task ‚Äúclick-\\nbutton-sequence.‚Äù\\nAction AutoGen\\n/usr Init chat Executor agent to AssistantAgent:\\nWe have an autonomous computer control agent that can perform atomic instructions specified by natural language to control\\ncomputers. There are two types of instructions it can execute.\\nFirst, given the instruction that matches the regular expression ‚Äò‚Äò^type\\\\s.{1,}$\" , it can type a list of characters via the keyboard.\\nThis instruction should specify the target keyboard input for the agent to type. Before this typing instruction, you should first locate\\nthe cursor by clicking the input box with click instruction.\\nSecond, given the instruction that matches the regular expression, ‚Äò‚Äò^clickxpath\\\\s.{1,}$\" , it can click an HTML element\\nwith an xpath that is visible on the webpage. The target of this instruction should be a valid xpath. Below is the HTML code of the\\nwebpage where the agent should solve a task.\\n1<div id=\" wrap \" data - wob_ref =\"2\" data - wob_eps =\"e0\">\\n2 <div id=\" query \">Click button ONE , then click button TWO .</div >\\n3 <div id=\" area \" data - wob_ref =\"3\" data - wob_eps =\"e0\">\\n4 <button id=\" subbtn \" style =\" position : absolute ; left :103 px; top :87 px\" data -\\nwob_ref =\"4\" data - wob_eps =\"e0\">ONE </ button >\\n5 <button id=\" subbtn2 \" style =\" position : absolute ; left :44 px; top :97 px\" data -\\nwob_ref =\"5\" data - wob_eps =\"e0\">TWO </ button >\\n6 </div >\\n7</div >\\ntask: Click button ONE, then click button TWO.\\nplan:\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\nCurrent task: Click button ONE, then click button TWO.\\nplan:\\n ***************************************************************\\nAssistantAgent to Executor agent:\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nBelow is the HTML code of the webpage where the agent should solve a task.\\n1<div id=\" wrap \" data - wob_ref =\"2\" data - wob_eps =\"e0\">\\n2 <div id=\" query \">Click button ONE , then click button TWO .</div >\\n3 <div id=\" area \" data - wob_ref =\"3\" data - wob_eps =\"e0\">\\n4 <button id=\" subbtn \" style =\" position : absolute ; left :103 px; top :87 px\" data -\\nwob_ref =\"4\" data - wob_eps =\"e0\">ONE </ button >\\n5 <button id=\" subbtn2 \" style =\" position : absolute ; left :44 px; top :97 px\" data -\\nwob_ref =\"5\" data - wob_eps =\"e0\">TWO </ button >\\n6 </div >\\n7</div >\\nCurrent task: Click button ONE, then click button TWO.\\nHere is a plan you are following now.\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\nWe have a history of instructions that have been already executed by the autonomous agent so far.\\nNo instruction has been executed yet.\\nBased on the plan and the history of instructions executed so far, the first instruction should be ‚Äò\\n***************************************************************\\nAssistantAgent to Executor agent:\\nclickxpath //button[@id=‚Äòsubbtn‚Äô]\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nHold on, please wait for my next instruction.\\n***************************************************************\\nAssistantAgent to Executor agent:\\nSure, I‚Äôll be here when you‚Äôre ready with your next instruction.\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nBelow is the HTML code of the webpage where the agent should solve a task.\\n1<div id=\" wrap \" data - wob_ref =\"2\" data - wob_eps =\"e0\">\\n2 <div id=\" query \">Click button ONE , then click button TWO .</div >\\n3 <div id=\" area \" data - wob_ref =\"3\" data - wob_eps =\"e0\">\\n4 <button id=\" subbtn \" style =\" position : absolute ; left :103 px; top :87 px\" data -\\nwob_ref =\"4\" data - wob_eps =\"e0\" data - tampered =\"e0\">ONE </ button >\\n5 <button id=\" subbtn2 \" style =\" position : absolute ; left :44 px; top :97 px\" data -\\nwob_ref =\"5\" data - wob_eps =\"e0\">TWO </ button >\\n6 </div >\\n7</div >\\nCurrent task: Click button ONE, then click button TWO.\\nHere is a plan you are following now.\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\nWe have a history of instructions that have been already executed by the autonomous agent so far.\\n1: clickxpath //button[@id=‚Äòsubbtn‚Äô]\\nBased on the plan and the history of instructions executed so far, the next proper instruction should be ‚Äò\\n***************************************************************\\nAssistantAgent to Executor agent:\\nclickxpath //button[@id=‚Äòsubbtn2‚Äô]\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nSUCCESS!!!!\\n TERMINATE\\n43', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 42})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_search_indexer_client.read_and_load_pdf(pdf_url=url_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 20:37:47,373 - micro - MainProcess - INFO     Reading PDF file from C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\\utils\\data\\autogen.pdf. (langchain_integration.py:read_and_load_pdf:342)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='AutoGen : Enabling Next-Gen LLM\\nApplications via Multi-Agent Conversation\\nQingyun Wu‚Ä†, Gagan Bansal‚àó, Jieyu Zhang¬±, Yiran Wu‚Ä†, Beibin Li‚àó\\nErkang Zhu‚àó, Li Jiang‚àó, Xiaoyun Zhang‚àó, Shaokun Zhang‚Ä†, Jiale Liu‚àì\\nAhmed Awadallah‚àó, Ryen W. White‚àó, Doug Burger‚àó, Chi Wang‚àó1\\n‚àóMicrosoft Research,‚Ä†Pennsylvania State University\\n¬±University of Washington,‚àìXidian University\\nAgent CustomizationConversable agent\\nFlexible Conversation Patterns\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\nHierarchical chatJoint chatMulti-Agent Conversations‚Ä¶Execute the following code‚Ä¶\\nGot it! Here is the revised code ‚Ä¶No, please plot % change!Plot a chart of META and TESLA stock price change YTD.\\nOutput:$Month\\nOutput:%MonthError package yfinanceis not installed\\nSorry! Please first pip install yfinanceand then execute the code\\nInstalling‚Ä¶\\nExample Agent Chat\\nFigure 1: AutoGen enables diverse LLM-based applications using multi-agent conversations. (Left)\\nAutoGen agents are conversable, customizable, and can be based on LLMs, tools, humans, or even\\na combination of them. (Top-middle) Agents can converse to solve tasks. (Right) They can form\\na chat, potentially with humans in the loop. (Bottom-middle) The framework supports flexible\\nconversation patterns.\\nAbstract\\nAutoGen2is an open-source framework that allows developers to build LLM ap-\\nplications via multiple agents that can converse with each other to accomplish\\ntasks. AutoGen agents are customizable, conversable , and can operate in vari-\\nous modes that employ combinations of LLMs, human inputs, and tools. Using\\nAutoGen , developers can also flexibly define agent interaction behaviors. Both\\nnatural language and computer code can be used to program flexible conversation\\npatterns for different applications. AutoGen serves as a generic framework for\\nbuilding diverse applications of various complexities and LLM capacities. Em-\\npirical studies demonstrate the effectiveness of the framework in many example\\napplications, with domains ranging from mathematics, coding, question answer-\\ning, operations research, online decision-making, entertainment, etc.\\n1Corresponding author. Email: auto-gen@outlook.com\\n2https://github.com/microsoft/autogenarXiv:2308.08155v2  [cs.AI]  3 Oct 2023', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='1 Introduction\\nLarge language models (LLMs) are becoming a crucial building block in developing powerful agents\\nthat utilize LLMs for reasoning, tool usage, and adapting to new observations (Yao et al., 2022; Xi\\net al., 2023; Wang et al., 2023b) in many real-world tasks. Given the expanding tasks that could\\nbenefit from LLMs and the growing task complexity, an intuitive approach to scale up the power of\\nagents is to use multiple agents that cooperate. Prior work suggests that multiple agents can help\\nencourage divergent thinking (Liang et al., 2023), improve factuality and reasoning (Du et al., 2023),\\nand provide validation (Wu et al., 2023). In light of the intuition and early evidence of promise, it is\\nintriguing to ask the following question: how can we facilitate the development of LLM applications\\nthat could span a broad spectrum of domains and complexities based on the multi-agent approach?\\nOur insight is to use multi-agent conversations to achieve it. There are at least three reasons con-\\nfirming its general feasibility and utility thanks to recent advances in LLMs: First, because chat-\\noptimized LLMs (e.g., GPT-4) show the ability to incorporate feedback, LLM agents can cooperate\\nthrough conversations with each other or human(s), e.g., a dialog where agents provide and seek rea-\\nsoning, observations, critiques, and validation. Second, because a single LLM can exhibit a broad\\nrange of capabilities (especially when configured with the correct prompt and inference settings),\\nconversations between differently configured agents can help combine these broad LLM capabilities\\nin a modular and complementary manner. Third, LLMs have demonstrated ability to solve complex\\ntasks when the tasks are broken into simpler subtasks. Multi-agent conversations can enable this\\npartitioning and integration in an intuitive manner. How can we leverage the above insights and\\nsupport different applications with the common requirement of coordinating multiple agents, poten-\\ntially backed by LLMs, humans, or tools exhibiting different capacities? We desire a multi-agent\\nconversation framework with generic abstraction and effective implementation that has the flexibil-\\nity to satisfy different application needs. Achieving this requires addressing two critical questions:\\n(1) How can we design individual agents that are capable, reusable, customizable, and effective in\\nmulti-agent collaboration? (2) How can we develop a straightforward, unified interface that can\\naccommodate a wide range of agent conversation patterns? In practice, applications of varying\\ncomplexities may need distinct sets of agents with specific capabilities, and may require different\\nconversation patterns, such as single- or multi-turn dialogs, different human involvement modes, and\\nstatic vs. dynamic conversation. Moreover, developers may prefer the flexibility to program agent\\ninteractions in natural language or code. Failing to adequately address these two questions would\\nlimit the framework‚Äôs scope of applicability and generality.\\nWhile there is contemporaneous exploration of multi-agent approaches,3we present AutoGen , a\\ngeneralized multi-agent conversation framework (Figure 1), based on the following new concepts.\\n1Customizable and conversable agents. AutoGen uses a generic design of agents that can lever-\\nage LLMs, human inputs, tools, or a combination of them. The result is that developers can\\neasily and quickly create agents with different roles (e.g., agents to write code, execute code,\\nwire in human feedback, validate outputs, etc.) by selecting and configuring a subset of built-in\\ncapabilities. The agent‚Äôs backend can also be readily extended to allow more custom behaviors.\\nTo make these agents suitable for multi-agent conversation, every agent is made conversable ‚Äì\\nthey can receive, react, and respond to messages. When configured properly, an agent can hold\\nmultiple turns of conversations with other agents autonomously or solicit human inputs at cer-\\ntain rounds, enabling human agency and automation. The conversable agent design leverages the\\nstrong capability of the most advanced LLMs in taking feedback and making progress via chat\\nand also allows combining capabilities of LLMs in a modular fashion. (Section 2.1)\\n2Conversation programming. A fundamental insight of AutoGen is to simplify and unify com-\\nplex LLM application workflows as multi-agent conversations. So AutoGen adopts a program-\\nming paradigm centered around these inter-agent conversations. We refer to this paradigm as\\nconversation programming , which streamlines the development of intricate applications via two\\nprimary steps: (1) defining a set of conversable agents with specific capabilities and roles (as\\ndescribed above); (2) programming the interaction behavior between agents via conversation-\\ncentric computation andcontrol . Both steps can be achieved via a fusion of natural and pro-\\ngramming languages to build applications with a wide range of conversation patterns and agent\\nbehaviors. AutoGen provides ready-to-use implementations and also allows easy extension and\\nexperimentation for both steps. (Section 2.2)\\n3We refer to Appendix A for a detailed discussion.\\n2', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='AutoGen also provides a collection of multi-agent applications created using conversable agents\\nand conversation programming. These applications demonstrate how AutoGen can easily support\\napplications of various complexities and LLMs of various capabilities. Moreover, we perform both\\nevaluation on benchmarks and a pilot study of new applications. The results show that AutoGen can\\nhelp achieve outstanding performance on many tasks, and enable innovative ways of using LLMs,\\nwhile reducing development effort. (Section 3 and Appendix D)\\n2 The AutoGen Framework\\nTo reduce the effort required for developers to create complex LLM applications across various do-\\nmains, a core design principle of AutoGen is to streamline and consolidate multi-agent workflows\\nusing multi-agent conversations. This approach also aims to maximize the reusability of imple-\\nmented agents. This section introduces the two key concepts of AutoGen : conversable agents and\\nconversation programming.\\n2.1 Conversable Agents\\nInAutoGen , aconversable agent is an entity with a specific role that can pass messages to send and\\nreceive information to and from other conversable agents, e.g., to start or continue a conversation. It\\nmaintains its internal context based on sent and received messages and can be configured to possess\\na set of capabilities, e.g., enabled by LLMs, tools, or human input, etc. The agents can act according\\nto programmed behavior patterns described next.\\nAgent capabilities powered by LLMs, humans, and tools. Since an agent‚Äôs capabilities directly\\ninfluence how it processes and responds to messages, AutoGen allows flexibility to endow its agents\\nwith various capabilities. AutoGen supports many common composable capabilities for agents,\\nincluding 1) LLMs. LLM-backed agents exploit many capabilities of advanced LLMs such as role\\nplaying, implicit state inference and progress making conditioned on conversation history, providing\\nfeedback, adapting from feedback, and coding. These capabilities can be combined in different ways\\nvia novel prompting techniques4to increase an agent‚Äôs skill and autonomy. AutoGen also offers\\nenhanced LLM inference features such as result caching, error handling, message templating, etc.,\\nvia an enhanced LLM inference layer. 2) Humans. Human involvement is desired or even essential\\nin many LLM applications. AutoGen lets a human participate in agent conversation via human-\\nbacked agents, which could solicit human inputs at certain rounds of a conversation depending on\\nthe agent configuration. The default user proxy agent allows configurable human involvement levels\\nand patterns, e.g., frequency and conditions for requesting human input including the option for\\nhumans to skip providing input. 3) Tools. Tool-backed agents have the capability to execute tools\\nvia code execution or function execution. For example, the default user proxy agent in AutoGen is\\nable to execute code suggested by LLMs, or make LLM-suggested function calls.\\nAgent customization and cooperation. Based on application-specific needs, each agent can be\\nconfigured to have a mix of basic back-end types to display complex behavior in multi-agent con-\\nversations. AutoGen allows easy creation of agents with specialized capabilities and roles by reusing\\nor extending the built-in agents. The yellow-shaded area of Figure 2 provides a sketch of the built-in\\nagents in AutoGen . The ConversableAgent class is the highest-level agent abstraction and, by\\ndefault, can use LLMs, humans, and tools. The AssistantAgent andUserProxyAgent are two\\npre-configured ConversableAgent subclasses, each representing a common usage mode, i.e., act-\\ning as an AI assistant (backed by LLMs) and acting as a human proxy to solicit human input or\\nexecute code/function calls (backed by humans and/or tools).\\nIn the example on the right-hand side of Figure 1, an LLM-backed assistant agent and a tool- and\\nhuman-backed user proxy agent are deployed together to tackle a task. Here, the assistant agent\\ngenerates a solution with the help of LLMs and passes the solution to the user proxy agent. Then,\\nthe user proxy agent solicits human inputs or executes the assistant‚Äôs code and passes the results as\\nfeedback back to the assistant.\\n4Appendix C presents an example of such novel prompting techniques which empowers the default LLM-\\nbacked assistant agent in AutoGen to converse with other agents in multi-step problem solving.\\n3', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='2 Initiate Conversations:A.initiate_chat(‚ÄúPlot a chart of META and TESLA stock price change YTD.‚Äù, B)\\nAssistant BUser Proxy AAutoGenAgents\\nDeveloper Code# This funcwill be invoked in generate_replyA.register_reply(B,  reply_func_A2B)def reply_func_A2B(msg):ouput= input_from_human()‚Ä¶if not ouput:if msg includes code:output = execute(msg)return outputConversableAgent\\nAssistantAgentUserProxyAgenthuman_input_mode= ‚ÄúNEVER‚Äùcode_execution_config= FalseDEFAULT_SYSTEM_MESSAGE = ‚ÄúYou are a helpful AI assistant‚Ä¶In the following cases, suggest python code‚Ä¶‚Äùhuman_input_mode=‚ÄúALWAYS‚Äù\\nGroupChatManagerhuman_input_mode= ‚ÄúNEVER‚Äùgroup_chat= [              ] \\n# Note: when no reply funcis registered, a list of default reply functions will be used. Agent Customization:\\nProgram ExecutionPlot a chart of META and TESLA stock price change YTD.Execute the following code‚Ä¶sendreceivereceiveConversation-Centric Computationgenerate_replyError: package yfinanceis not installedsendgenerate_replySorry! Please first pip install yfinanceand then executeConversation-Driven Control Flowgenerate_replyThe Resulting Automated Agent Chat:‚Ä¶1.2 Register a Custom Reply Func:1.1 Define Agents:Unified Conversation Interfaces:‚Ä¢send‚Ä¢receive ‚Ä¢generate_reply\\nFigure 2: Illustration of how to use AutoGen to program a multi-agent conversation. The top sub-\\nfigure illustrates the built-in agents provided by AutoGen , which have unified conversation interfaces\\nand can be customized. The middle sub-figure shows an example of using AutoGen to develop\\na two-agent system with a custom reply function. The bottom sub-figure illustrates the resulting\\nautomated agent chat from the two-agent system during program execution.\\nBy allowing custom agents that can converse with each other, conversable agents in AutoGen serve\\nas a useful building block. However, to develop applications where agents make meaningful progress\\non tasks, developers also need to be able to specify and mold these multi-agent conversations.\\n2.2 Conversation Programming\\nAs a solution to the above problem, AutoGen utilizes conversation programming , a paradigm that\\nconsiders two concepts: the first is computation ‚Äì the actions agents take to compute their response\\nin a multi-agent conversation. And the second is control flow ‚Äì the sequence (or conditions) un-\\nder which these computations happen. As we will show in the applications section, the ability to\\nprogram these helps implement many flexible multi-agent conversation patterns. In AutoGen , these\\ncomputations are conversation-centric. An agent takes actions relevant to the conversations it is\\ninvolved in and its actions result in message passing for consequent conversations (unless a termina-\\ntion condition is satisfied). Similarly, control flow is conversation-driven ‚Äì the participating agents‚Äô\\ndecisions on which agents to send messages to and the procedure of computation are functions of the\\ninter-agent conversation. This paradigm helps one to reason intuitively about a complex workflow\\nas agent action taking and conversation message-passing between agents.\\nFigure 2 provides a simple illustration. The bottom sub-figure shows how individual agents perform\\ntheir role-specific, conversation-centric computations to generate responses (e.g., via LLM inference\\ncalls and code execution). The task progresses through conversations displayed in the dialog box.\\nThe middle sub-figure demonstrates a conversation-based control flow. When the assistant receives\\na message, the user proxy agent typically sends the human input as a reply. If there is no input, it\\nexecutes any code in the assistant‚Äôs message instead.\\n4', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='AutoGen features the following design patterns to facilitate conversation programming:\\n1.Unified interfaces and auto-reply mechanisms for automated agent chat. Agents in\\nAutoGen have unified conversation interfaces for performing the corresponding conversation-\\ncentric computation, including a send/receive function for sending/receiving messages and a\\ngenerate reply function for taking actions and generating a response based on the received\\nmessage. AutoGen also introduces and by default adopts an agent auto-reply mechanism to\\nrealize conversation-driven control: Once an agent receives a message from another agent, it au-\\ntomatically invokes generate reply and sends the reply back to the sender unless a termination\\ncondition is satisfied. AutoGen provides built-in reply functions based on LLM inference, code\\nor function execution, or human input. One can also register custom reply functions to customize\\nthe behavior pattern of an agent, e.g., chatting with another agent before replying to the sender\\nagent. Under this mechanism, once the reply functions are registered, and the conversation is\\ninitialized, the conversation flow is naturally induced, and thus the agent conversation proceeds\\nnaturally without any extra control plane, i.e., a special module that controls the conversation\\nflow. For example, with the developer code in the blue-shaded area (marked ‚ÄúDeveloper Code‚Äù)\\nof Figure 2, one can readily trigger the conversation among the agents, and the conversation\\nwould proceed automatically, as shown in the dialog box in the grey shaded area (marked ‚ÄúPro-\\ngram Execution‚Äù) of Figure 2. The auto-reply mechanism provides a decentralized, modular, and\\nunified way to define the workflow.\\n2.Control by fusion of programming and natural language. AutoGen allows the usage of\\nprogramming and natural language in various control flow management patterns: 1) Natural-\\nlanguage control via LLMs. InAutoGen , one can control the conversation flow by prompting\\nthe LLM-backed agents with natural language. For instance, the default system message of the\\nbuilt-in AssistantAgent inAutoGen uses natural language to instruct the agent to fix errors\\nand generate code again if the previous result indicates there are errors. It also guides the agent\\nto confine the LLM output to certain structures, making it easier for other tool-backed agents to\\nconsume. For example, instructing the agent to reply with ‚ÄúTERMINATE‚Äù when all tasks are\\ncompleted to terminate the program. More concrete examples of natural language controls can\\nbe found in Appendix C. 2) Programming-language control. InAutoGen , Python code can be\\nused to specify the termination condition, human input mode, and tool execution logic, e.g., the\\nmax number of auto replies. One can also register programmed auto-reply functions to control\\nthe conversation flow with Python code, as shown in the code block identified as ‚ÄúConversation-\\nDriven Control Flow‚Äù in Figure 2. 3) Control transition between natural and programming\\nlanguage. AutoGen also supports flexible control transition between natural and programming\\nlanguage. One can achieve transition from code to natural-language control by invoking an LLM\\ninference containing certain control logic in a customized reply function; or transition from nat-\\nural language to code control via LLM-proposed function calls (Eleti et al., 2023).\\nIn the conversation programming paradigm, one can realize multi-agent conversations of diverse\\npatterns. In addition to static conversation with predefined flow, AutoGen also supports dynamic\\nconversation flows with multiple agents. AutoGen provides two general ways to achieve this: 1)\\nCustomized generate reply function: within the customized generate reply function, one\\nagent can hold the current conversation while invoking conversations with other agents depending\\non the content of the current message and context. 2) Function calls: In this approach, LLM decides\\nwhether or not to call a particular function depending on the conversation status. By messaging\\nadditional agents in the called functions, the LLM can drive dynamic multi-agent conversation. In\\naddition, AutoGen supports more complex dynamic group chat via built-in GroupChatManager ,\\nwhich can dynamically select the next speaker and then broadcast its response to other agents. We\\nelaborate on this feature and its application in Section 3. We provide implemented working systems\\nto showcase all these different patterns, with some of them visualized in Figure 3.\\n3 Applications of AutoGen\\nWe demonstrate six applications using AutoGen (see Figure 3) to illustrate its potential in simplify-\\ning the development of high-performance multi-agent applications. These applications are selected\\nbased on their real-world relevance (A1, A2, A4, A5, A6), problem difficulty and solving capabil-\\nities enabled by AutoGen (A1, A2, A3, A4), and innovative potential (A5, A6). Together, these\\ncriteria showcase AutoGen ‚Äôs role in advancing the LLM-application landscape.\\n5', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='A1. Math Problem Solving\\nA4. Multi-agent CodingCommander\\nSafeguard\\nWriter\\nA6. Conversational ChessA2. Retrieval-augmented ChatRetrieval-augmentedAssistantRetrieval-augmentedUser Proxy\\nChess Board\\nHuman/AI Chess Player A\\nHuman/AI Chess Player B\\nStudent\\nAssistant\\nAssistantExpert\\nAsk  expert\\nBroadcast\\nManager\\nSpeak\\nA5. Dynamic Group Chat\\nALFWorldExecutorAssistant\\nGrounding Agent\\nA3. ALF ChatFigure 3: Six examples of diverse applications built using AutoGen . Their conversation patterns\\nshow AutoGen ‚Äôs flexibility and power.\\nA1: Math Problem Solving\\nMathematics is a foundational discipline and the promise of leveraging LLMs to assist with math\\nproblem solving opens up a new plethora of applications and avenues for exploration, including per-\\nsonalized AI tutoring, AI research assistance, etc. This section demonstrates how AutoGen can help\\ndevelop LLM applications for math problem solving, showcasing strong performance and flexibility\\nin supporting various problem-solving paradigms.\\n(Scenario 1 ) We are able to build a system for autonomous math problem solving by directly reusing\\ntwo built-in agents from AutoGen . We evaluate our system and several alternative approaches,\\nincluding open-source methods such as Multi-Agent Debate (Liang et al., 2023), LangChain Re-\\nAct (LangChain, 2023), vanilla GPT-4, and commercial products ChatGPT + Code Interpreter, and\\nChatGPT + Plugin (Wolfram Alpha), on the MATH (Hendrycks et al., 2021) dataset and summarize\\nthe results in Figure 4a. We perform evaluations over 120 randomly selected level-5 problems and\\non the entire5test dataset from MATH. The results show that the built-in agents from AutoGen al-\\nready yield better performance out of the box compared to the alternative approaches, even including\\nthe commercial ones. ( Scenario 2 ) We also showcase a human-in-the-loop problem-solving process\\nwith the help of AutoGen . To incorporate human feedback with AutoGen , one only needs to set\\nhuman input mode=‚ÄòALWAYS‚Äô in the UserProxyAgent of the system in scenario 1. We demon-\\nstrate that this system can effectively incorporate human inputs to solve challenging problems that\\ncannot be solved without humans. ( Scenario 3 ) We further demonstrate a novel scenario where\\nmultiple human users can participate in the conversations during the problem-solving process. Our\\nexperiments and case studies for these scenarios show that AutoGen enables better performance or\\nnew experience compared to other solutions we experimented with. Due to the page limit, details of\\nthe evaluation, including case studies in three scenarios are in Appendix D.\\nA2: Retrieval-Augmented Code Generation and Question Answering\\nRetrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic\\nlimitations of LLMs by incorporating external documents. In this section, we employ AutoGen to\\nbuild a Retrieval-Augmented Generation (RAG) system (Lewis et al., 2020; Parvez et al., 2021)\\nnamed Retrieval-augmented Chat. The system consists of two agents: a Retrieval-augmented User\\nProxy agent and a Retrieval-augmented Assistant agent, both of which are extended from built-in\\nagents from AutoGen . The Retrieval-augmented User Proxy includes a vector database (Chroma,\\n5We did not evaluate ChatGPT on the whole dataset since it requires substantial manual effort and is re-\\nstricted by its hourly message-number limitation. Multi-agent debate and LangChain ReAct were also not\\nevaluated since they underperformed vanilla GPT-4 on the smaller test set.\\n6', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='AutoGen ChatGPT\\n+CodeChatGPT\\n+PluginGPT-4 Multi-Agent\\nDebateLangChain\\nReAct\\nMethods01020304050607080Success Ratio (%)52.5%\\n48.33%\\n45.0%\\n30.0%\\n26.67%\\n23.33%69.48%\\n55.18%120 Level-5 problems\\nWhole Dataset(a) A1: Performance on MATH (w/ GPT-4).\\nF1 Recall\\nMetrics01020304050607080Percentage (%)25.88%66.65%\\n15.12%58.56%\\n22.79%62.59%AutoGen\\nAuotGen W/O interactive retrieval\\nDPR (b) A2: Q&A tasks (w/ GPT-3.5).\\nAutoGen (3 agent) AutoGen (2 agent) ReAct\\nMethods020406080100Success Ratio (%)69%\\n54% 54%77%\\n63%66%Average\\nBest of 3\\n(c) A3: Performance on ALFWorld.\\nF1 Recall\\nMetrics020406080100Percentage (%)96.00%98.00%\\n88.00%\\n78.00%83.00%\\n72.00%\\n48.00%\\n32.00%Multi-GPT4\\nSingle-GPT4\\nMulti-GPT3.5\\nSingle-GPT3.5 (d) A4: Performance on OptiGuide.\\nFigure 4: Performance on four applications A1-A4. (a) shows that AutoGen agents can be used\\nout of the box to achieve the most competitive performance on math problem solving tasks; (b)\\nshows that AutoGen can be used to realize effective retrieval augmentation and realize a novel\\ninteractive retrieval feature to boost performance on Q&A tasks; (c) shows that AutoGen can be used\\nto introduce a three-agent system with a grounding agent to improve performance on ALFWorld;\\n(d) shows that a multi-agent design is helpful in boosting performance in coding tasks that need\\nsafeguards.\\n2023) with SentenceTransformers (Reimers & Gurevych, 2019) as the context retriever. A detailed\\nworkflow description of the Retrieval-augmented Chat is provided in Appendix D.\\nWe evaluate Retrieval-augmented Chat in both question-answering and code-generation scenarios.\\n(Scenario 1 ) We first perform an evaluation regarding natural question answering on the Natural\\nQuestions dataset (Kwiatkowski et al., 2019) and report results in Figure 4b. In this evaluation, we\\ncompare our system with DPR (Dense Passage Retrieval) following an existing evaluation6prac-\\ntice (Adlakha et al., 2023). Leveraging the conversational design and natural-language control,\\nAutoGen introduces a novel interactive retrieval feature in this application: whenever the retrieved\\ncontext does not contain the information, instead of terminating, the LLM-based assistant would\\nreply ‚Äú Sorry, I cannot find any information about... UPDATE CONTEXT. ‚Äù which will invoke more\\nretrieval attempts. We conduct an ablation study in which we prompt the assistant agent to say ‚ÄúI\\ndon‚Äôt know‚Äù instead of ‚ÄúUPDATE CONTEXT. ‚Äù in cases where relevant information is not found,\\nand report results in Figure 4b. The results show that the interactive retrieval mechanism indeed\\nplays a non-trivial role in the process. We give a concrete example and results using this appealing\\nfeature in Appendix D. ( Scenario 2 ) We further demonstrate how Retrieval-augmented Chat aids in\\ngenerating code based on a given codebase that contains code not included in GPT-4‚Äôs training data.\\nEvaluation and demonstration details for both scenarios are included in Appendix D.\\n6The results of DPR with GPT-3.5 shown in Figure 4b are from (Adlakha et al., 2023). We use GPT-3.5 as\\na shorthand for GPT-3.5-turbo.\\n7', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='A3: Decision Making in Text World Environments\\nIn this subsection, we demonstrate how AutoGen can be used to develop effective applications that\\ninvolve interactive or online decision making. We perform the study using the ALFWorld (Shridhar\\net al., 2021) benchmark, which includes a diverse collection of synthetic language-based interactive\\ndecision-making tasks in household environments.\\nWith AutoGen , we implemented a two-agent system to solve tasks from ALFWorld. It consists of\\nan LLM-backed assistant agent responsible for suggesting plans to conduct a task and an executor\\nagent responsible for executing actions in the ALFWorld environments. This system integrates Re-\\nAct prompting (Yao et al., 2022), and is able to achieve similar performance. A common challenge\\nencountered in both ReAct and the AutoGen -based two-agent system is their occasional inability to\\nleverage basic commonsense knowledge about the physical world. This deficiency can lead to the\\nsystem getting stuck in a loop due to repetitive errors. Fortunately, the modular design of AutoGen\\nallows us to address this issue effectively: With AutoGen , we are able to introduce a grounding\\nagent, which supplies crucial commonsense knowledge‚Äìsuch as ‚ÄúYou must find and take the object\\nbefore you can examine it. You must go to where the target object is before you can use it. ‚Äù ‚Äìwhenever\\nthe system exhibits early signs of recurring errors. It significantly enhances the system‚Äôs ability to\\navoid getting entangled in error loops. We compare the task-solving performance of the two variants\\nof our system with GPT-3.5-turbo and ReAct7on the 134 unseen tasks from ALFWorld and report\\nresults in Figure 4c. The results show that introducing a grounding agent could bring in a 15%\\nperformance gain on average. Upon examining the systems‚Äô outputs, we observe that the grounding\\nagent, by delivering background commonsense knowledge at the right junctures, significantly miti-\\ngated the tendency of the system to persist with a flawed plan, thereby avoiding the creation of error\\nloops. For an example trajectory comparing the systems see Appendix D, Figure 10.\\nA4: Multi-Agent Coding\\nIn this subsection, we use AutoGen to build a multi-agent coding system based on OptiGuide (Li\\net al., 2023a), a system that excels at writing code to interpret optimization solutions and answer\\nuser questions, such as exploring the implications of changing a supply-chain decision or under-\\nstanding why the optimizer made a particular choice. The second sub-figure of Figure 3 shows the\\nAutoGen -based implementation. The workflow is as follows: the end user sends questions, such as\\n‚ÄúWhat if we prohibit shipping from supplier 1 to roastery 2? ‚Äù to the Commander agent. The Com-\\nmander coordinates with two assistant agents, including the Writer and the Safeguard, to answer\\nthe question. The Writer will craft code and send the code to the Commander. After receiving the\\ncode, the Commander checks the code safety with the Safeguard; if cleared, the Commander will\\nuse external tools (e.g., Python) to execute the code, and request the Writer to interpret the execution\\nresults. For instance, the writer may say ‚Äú if we prohibit shipping from supplier 1 to roastery 2, the\\ntotal cost would increase by 10.5%. ‚Äù The Commander then provides this concluding answer to the\\nend user. If, at a particular step, there is an exception, e.g., security red flag raised by Safeguard, the\\nCommander redirects the issue back to the Writer with debugging information. The process might\\nbe repeated multiple times until the user‚Äôs question is answered or timed-out.\\nWith AutoGen the core workflow code for OptiGuide was reduced from over 430 lines to 100 lines,\\nleading to significant productivity improvement. We provide a detailed comparison of user expe-\\nrience with ChatGPT+Code Interpreter and AutoGen -based OptiGuide in Appendix D, where we\\nshow that AutoGen -based OptiGuide could save around 3x of user‚Äôs time and reduce user interac-\\ntions by 3 - 5 times on average. We also conduct an ablation showing that multi-agent abstraction is\\nnecessary. Specifically, we construct a single-agent approach where a single agent conducts both the\\ncode-writing and safeguard processes. We tested the single- and multi-agent approaches on a dataset\\nof 100 coding tasks, which is crafted to include equal numbers of safe and unsafe tasks. Evaluation\\nresults as reported in Figure 4d show that the multi-agent design boosts the F-1 score in identifying\\nunsafe code by 8% (with GPT-4) and 35% (with GPT-3.5-turbo).\\n7Results of ReAct are obtained by directly running its official code with default settings. The code uses\\ntext-davinci-003 as backend LM and does not support GPT-3.5-turbo or GPT-4.\\n8', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='A5: Dynamic Group Chat\\nAutoGen provides native support for a dynamic group chat communication pattern, in which par-\\nticipating agents share the same context and converse with the others in a dynamic manner instead\\nof following a pre-defined order. Dynamic group chat relies on ongoing conversations to guide the\\nflow of interaction among agents. These make dynamic group chat ideal for situations where col-\\nlaboration without strict communication order is beneficial. In AutoGen , the GroupChatManager\\nclass serves as the conductor of conversation among agents and repeats the following three steps:\\ndynamically selecting a speaker, collecting responses from the selected speaker, and broadcasting\\nthe message (Figure 3-A5). For the dynamic speaker-selection component, we use a role-play style\\nprompt. Through a pilot study on 12 manually crafted complex tasks, we observed that compared\\nto a prompt that is purely based on the task, utilizing a role-play prompt often leads to more effec-\\ntive consideration of both conversation context and role alignment during the problem-solving and\\nspeaker-selection process. Consequently, this leads to a higher success rate and fewer LLM calls.\\nWe include detailed results in Appendix D.\\nA6: Conversational Chess\\nUsing AutoGen , we developed Conversational Chess, a natural language interface game shown in\\nthe last sub-figure of Figure 3. It features built-in agents for players, which can be human or LLM,\\nand a third-party board agent to provide information and validate moves based on standard rules.\\nWith AutoGen , we enabled two essential features: (1) Natural, flexible, and engaging game dynam-\\nics, enabled by the customizable agent design in AutoGen . Conversational Chess supports a range\\nof game-play patterns, including AI-AI, AI-human, and human-human, with seamless switching\\nbetween these modes during a single game. An illustrative example of these entertaining game dy-\\nnamics can be found in Figure 15, Appendix D. (2) Grounding, which is a crucial aspect to maintain\\ngame integrity. During gameplay, the board agent checks each proposed move for legality; if a move\\nis invalid, the agent responds with an error, prompting the player agent to re-propose a legal move\\nbefore continuing. This process ensures that only valid moves are played and helps maintain a con-\\nsistent gaming experience. As an ablation study, we removed the board agent and instead only relied\\non a relevant prompt ‚Äúyou should make sure both you and the opponent are making legal moves‚Äù to\\nground their move. The results highlighted that without the board agent, illegitimate moves caused\\ngame disruptions. The modular design offered flexibility, allowing swift adjustments to the board\\nagent in response to evolving game rules or varying chess rule variants. A comprehensive demon-\\nstration of this ablation study is in Appendix D.\\n4 Discussion\\nWe introduced an open-source library, AutoGen , that incorporates the paradigms of conversable\\nagents and conversation programming. This library utilizes capable agents that are well-suited for\\nmulti-agent cooperation. It features a unified conversation interface among the agents, along with\\nan auto-reply mechanisms, which help establish an agent-interaction interface that capitalizes on the\\nstrengths of chat-optimized LLMs with broad capabilities while accommodating a wide range of\\napplications. AutoGen serves as a general framework for creating and experimenting with multi-\\nagent systems that can easily fulfill various practical requirements, such as reusing, customizing,\\nand extending existing agents, as well as programming conversations between them.\\nOur experiments, as detailed in Section 3, demonstrate that this approach offers numerous benefits.\\nThe adoption of AutoGen has resulted in improved performance (over state-of-the-art approaches),\\nreduced development code, and decreased manual burden for existing applications. It offers flex-\\nibility to developers, as demonstrated in A1 (scenario 3), A5, and A6, where AutoGen enables\\nmulti-agent chats to follow a dynamic pattern rather than fixed back-and-forth interactions. It allows\\nhumans to engage in activities alongside multiple AI agents in a conversational manner. Despite the\\ncomplexity of these applications (most involving more than two agents or dynamic multi-turn agent\\ncooperation), the implementation based on AutoGen remains straightforward. Dividing tasks among\\nseparate agents promotes modularity. Furthermore, since each agent can be developed, tested, and\\nmaintained separately, this approach simplifies overall development and code management.\\n9', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='Although this work is still in its early experimental stages, it paves the way for numerous future\\ndirections and research opportunities. For instance, we can explore effective integration of existing\\nagent implementations into our multi-agent framework and investigate the optimal balance between\\nautomation and human control in multi-agent workflows. As we further develop and refine AutoGen ,\\nwe aim to investigate which strategies, such as agent topology and conversation patterns, lead to the\\nmost effective multi-agent conversations while optimizing the overall efficiency, among other fac-\\ntors. While increasing the number of agents and other degrees of freedom presents opportunities for\\ntackling more complex problems, it may also introduce new safety challenges that require additional\\nstudies and careful consideration.\\nWe provide more discussion in Appendix B, including guidelines for using AutoGen and direction\\nof future work. We hope AutoGen will help improve many LLM applications in terms of speed of\\ndevelopment, ease of experimentation, and overall effectiveness and safety. We actively welcome\\ncontributions from the broader community.\\nEthics statement\\nThere are several potential ethical considerations that could arise from the development and use of\\ntheAutoGen framework.\\n‚Ä¢ Privacy and Data Protection: The framework allows for human participation in conversations\\nbetween agents. It is important to ensure that user data and conversations are protected, and that\\ndevelopers use appropriate measures to safeguard privacy.\\n‚Ä¢ Bias and Fairness: LLMs have been shown to exhibit biases present in their training data (Navigli\\net al., 2023). When using LLMs in the AutoGen framework, it is crucial to address and mitigate\\nany biases that may arise in the conversations between agents. Developers should be aware of\\npotential biases and take steps to ensure fairness and inclusivity.\\n‚Ä¢ Accountability and Transparency: As discussed in the future work section, as the framework in-\\nvolves multiple agents conversing and cooperating, it is important to establish clear accountability\\nand transparency mechanisms. Users should be able to understand and trace the decision-making\\nprocess of the agents involved in order to ensure accountability and address any potential issues\\nor biases.\\n‚Ä¢ Trust and Reliance: AutoGen leverages human understanding and intelligence while providing\\nautomation through conversations between agents. It is important to consider the impact of this\\ninteraction on user experience, trust, and reliance on AI systems. Clear communication and user\\neducation about the capabilities and limitations of the system will be essential (Cai et al., 2019).\\n‚Ä¢ Unintended Consequences: As discussed before, the use of multi-agent conversations and automa-\\ntion in complex tasks may have unintended consequences. In particular, allowing LLM agents to\\nmake changes in external environments through code execution or function calls, such as installing\\npackages, could be risky. Developers should carefully consider the potential risks and ensure that\\nappropriate safeguards are in place to prevent harm or negative outcomes.\\nAcknowledgements\\nThe work presented in this report was made possible through discussions and feedback from Peter\\nLee, Johannes Gehrke, Eric Horvitz, Steven Lucco, Umesh Madan, Robin Moeur, Piali Choud-\\nhury, Saleema Amershi, Adam Fourney, Victor Dibia, Guoqing Zheng, Corby Rosset, Ricky Loynd,\\nEce Kamar, Rafah Hosn, John Langford, Ida Momennejad, Brian Krabach, Taylor Webb, Shanka\\nSubhra Mondal, Wei-ge Chen, Robert Gruen, Yinan Li, Yue Wang, Suman Nath, Tanakorn Leesat-\\napornwongsa, Xin Wang, Shishir Patil, Tianjun Zhang, Saehan Jo, Ishai Menache, Kontantina Mel-\\nlou, Runlong Zhou, Feiran Jia, Hamed Khanpour, Hamid Palangi, Srinagesh Sharma, Julio Albinati\\nCortez, Amin Saied, Yuzhe Ma, Dujian Ding, Linyong Nan, Prateek Yadav, Shannon Shen, Ankur\\nMallick, Mark Encarnaci ¬¥on, Lars Liden, Tianwei Yue, Julia Kiseleva, Anastasia Razdaibiedina, and\\nLuciano Del Corro. Qingyun Wu would like to acknowledge the funding and research support from\\nthe College of Information Science and Technology at Penn State University.\\n10', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='References\\nVaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Eval-\\nuating correctness and faithfulness of instruction-following models for question answering. arXiv\\npreprint arXiv:2307.16877 , 2023.\\nSaleema Amershi, Dan Weld, Mihaela V orvoreanu, Adam Fourney, Besmira Nushi, Penny Col-\\nlisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. Guidelines for human-ai\\ninteraction. In Proceedings of the 2019 chi conference on human factors in computing systems ,\\n2019.\\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man ¬¥e. Con-\\ncrete problems in ai safety, 2016.\\nAutoGPT. Documentation ‚Äî auto-gpt. https://docs.agpt.co/ , 2023.\\nBabyAGI. Github ‚Äî babyagi. https://github.com/yoheinakajima/babyagi , 2023.\\nCarrie J. Cai, Samantha Winter, David F. Steiner, Lauren Wilcox, and Michael Terry. ‚Äùhello ai‚Äù:\\nUncovering the onboarding needs of medical practitioners for human-ai collaborative decision-\\nmaking. Proceedings of the ACM on Human-Computer Interaction , 2019.\\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as\\ntool makers. arXiv preprint arXiv:2305.17126 , 2023.\\nChroma. Chromadb. https://github.com/chroma-core/chroma , 2023.\\nVictor Dibia. LIDA: A tool for automatic generation of grammar-agnostic visualizations and info-\\ngraphics using large language models. In Proceedings of the 61st Annual Meeting of the Associ-\\nation for Computational Linguistics (Volume 3: System Demonstrations) , Toronto, Canada, July\\n2023. Association for Computational Linguistics.\\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt. arXiv\\npreprint arXiv:2304.07590 , 2023.\\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improv-\\ning factuality and reasoning in language models through multiagent debate. arXiv preprint\\narXiv:2305.14325 , 2023.\\nAtty Eleti, Jeff Harris, and Logan Kilpatrick. Function calling and other api updates. https:\\n//openai.com/blog/function-calling-and-other-api-updates , 2023.\\nGuidance. Guidance. https://github.com/guidance-ai/guidance , 2023.\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv\\npreprint arXiv:2103.03874 , 2021.\\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven\\nKa Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for\\nmulti-agent collaborative framework. arXiv preprint arXiv:2308.00352 , 2023.\\nEric Horvitz. Principles of mixed-initiative user interfaces. In Proceedings of the SIGCHI conference\\non Human Factors in Computing Systems , 1999.\\nHuggingFace. Transformers agent. https://huggingface.co/docs/transformers/\\ntransformers_agents , 2023.\\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.\\narXiv preprint arXiv:2303.17491 , 2023.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\\nbenchmark for question answering research. Transactions of the Association for Computational\\nLinguistics , 2019.\\n11', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='LangChain. Introduction ‚Äî langchain. https://python.langchain.com/en/latest/index.\\nhtml , 2023.\\nMike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal? end-\\nto-end learning for negotiation dialogues. arXiv preprint arXiv:1706.05125 , 2017.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich K ¬®uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¬®aschel, et al. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems ,\\n2020.\\nBeibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, and Ishai Menache. Large language\\nmodels for supply chain optimization. arXiv preprint arXiv:2307.03875 , 2023a.\\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\\nCamel: Communicative agents for ‚Äùmind‚Äù exploration of large scale language model society,\\n2023b.\\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng\\nTu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-\\nagent debate, 2023.\\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement\\nlearning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802 ,\\n2018.\\nJerry Liu. LlamaIndex, November 2022. URL https://github.com/jerryjliu/llama_index .\\nV olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-\\nstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\\narXiv:1312.5602 , 2013.\\nRoberto Navigli, Simone Conia, and Bj ¬®orn Ross. Biases in large language models: Origins, inven-\\ntory and discussion. ACM Journal of Data and Information Quality , 2023.\\nOpenAI. ChatGPT plugins. https://openai.com/blog/chatgpt-plugins , 2023.\\nJoon Sung Park, Joseph C O‚ÄôBrien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and\\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint\\narXiv:2304.03442 , 2023.\\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.\\nRetrieval augmented code generation and summarization. arXiv preprint arXiv:2108.11601 ,\\n2021.\\nShishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model\\nconnected with massive apis. arXiv preprint arXiv:2305.15334 , 2023.\\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-\\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\\nProcessing . Association for Computational Linguistics, 11 2019. URL https://arxiv.org/\\nabs/1908.10084 .\\nSemantic-Kernel. Semantic kernel. https://github.com/microsoft/semantic-kernel ,\\n2023.\\nBokui Shen, Fei Xia, Chengshu Li, Roberto Mart ¬¥ƒ±n-Mart ¬¥ƒ±n, Linxi Fan, Guanzhi Wang, Claudia\\nP¬¥erez-D‚ÄôArpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, et al. igibson 1.0: A simu-\\nlation environment for interactive tasks in large realistic scenes. In 2021 IEEE/RSJ International\\nConference on Intelligent Robots and Systems (IROS) . IEEE, 2021.\\nTianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An\\nopen-domain platform for web-based agents. In International Conference on Machine Learning .\\nPMLR, 2017.\\n12', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C ÀÜot¬¥e, Yonatan Bisk, Adam Trischler, and Matthew\\nHausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In\\nProceedings of the International Conference on Learning Representations (ICLR) , 2021. URL\\nhttps://arxiv.org/abs/2010.03768 .\\nOriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,\\nMichelle Yeo, Alireza Makhzani, Heinrich K ¬®uttler, John Agapiou, Julian Schrittwieser, et al.\\nStarcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782 , 2017.\\nChi Wang, Qingyun Wu, Markus Weimer, and Erkang Zhu. Flaml: A fast and lightweight automl\\nlibrary. Proceedings of Machine Learning and Systems , 2021.\\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\\nand Anima Anandkumar. V oyager: An open-ended embodied agent with large language models.\\narXiv preprint arXiv:2305.16291 , 2023a.\\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\\narXiv preprint arXiv:2308.11432 , 2023b.\\nDaniel S. Weld and Oren Etzioni. The first law of robotics (a call to arms). In AAAI Conference on\\nArtificial Intelligence , 1994.\\nMax Woolf. Langchain problem. https://minimaxir.com/2023/07/langchain-problem/ ,\\n2023.\\nYiran Wu, Feiran Jia, Shaokun Zhang, Qingyun Wu, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat\\nLee, Richard Peng, and Chi Wang. An empirical study on challenging math problem solving with\\ngpt-4. arXiv preprint arXiv:2306.01337 , 2023.\\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe\\nWang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents:\\nA survey. arXiv preprint arXiv:2309.07864 , 2023.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 ,\\n2022.\\n13', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='A Related Work\\nWe examine existing LLM-based agent systems or frameworks that can be used to build LLM appli-\\ncations. We categorize the related work into single-agent and multi-agent systems and specifically\\nprovide a summary of differentiators comparing AutoGen with existing multi-agent systems in Ta-\\nble 1. Note that many of these systems are evolving open-source projects, so the remarks and\\nstatements about them may only be accurate as of the time of writing. We refer interested readers to\\ndetailed LLM-based agent surveys (Xi et al., 2023; Wang et al., 2023b)\\nSingle-Agent Systems:\\n‚Ä¢AutoGPT : AutoGPT is an open-source implementation of an AI agent that attempts to au-\\ntonomously achieve a given goal (AutoGPT, 2023). It follows a single-agent paradigm in which\\nit augments the AI model with many useful tools, and does not support multi-agent collaboration.\\n‚Ä¢ChatGPT+ (with code interpreter or plugin) : ChatGPT, a conversational AI service or agent,\\ncan now be used alongside a code interpreter or plugin (currently available only under the pre-\\nmium subscription plan ChatGPT Plus) (OpenAI, 2023). The code interpreter enables ChatGPT\\nto execute code, while the plugin enhances ChatGPT with a wide range of curated tools.\\n‚Ä¢LangChain Agents : LangChain is a general framework for developing LLM-based applica-\\ntions (LangChain, 2023). LangChain Agents is a subpackage for using an LLM to choose a\\nsequence of actions. There are various types of agents in LangChain Agents, with the ReAct agent\\nbeing a notable example that combines reasoning and acting when using LLMs (mainly designed\\nfor LLMs prior to ChatGPT) (Yao et al., 2022). All agents provided in LangChain Agents fol-\\nlow a single-agent paradigm and are not inherently designed for communicative and collaborative\\nmodes. A significant summary of its limitations can be found in (Woolf, 2023). Due to these lim-\\nitations, even the multi-agent systems in LangChain (e.g., re-implementation of CAMEL) are not\\nbased on LangChain Agents but are implemented from scratch. Their connection to LangChain\\nlies in the use of basic orchestration modules provided by LangChain, such as AI models wrapped\\nby LangChain and the corresponding interface.\\n‚Ä¢Transformers Agent : Transformers Agent (HuggingFace, 2023) is an experimental natural-\\nlanguage API built on the transformers repository. It includes a set of curated tools and an agent\\nto interpret natural language and use these tools. Similar to AutoGPT, it follows a single-agent\\nparadigm and does not support agent collaboration.\\nAutoGen differs from the single-agent systems above by supporting multi-agent LLM applications.\\nMulti-Agent Systems:\\n‚Ä¢BabyAGI : BabyAGI (BabyAGI, 2023) is an example implementation of an AI-powered task man-\\nagement system in a Python script. In this implemented system, multiple LLM-based agents\\nare used. For example, there is an agent for creating new tasks based on the objective and the\\nresult of the previous task, an agent for prioritizing the task list, and an agent for completing\\ntasks/sub-tasks. As a multi-agent system, BabyAGI adopts a static agent conversation pattern,\\ni.e., a predefined order of agent communication, while AutoGen supports both static and dynamic\\nconversation patterns and additionally supports tool usage and human involvement.\\n‚Ä¢CAMEL : CAMEL (Li et al., 2023b) is a communicative agent framework. It demonstrates\\nhow role playing can be used to let chat agents communicate with each other for task comple-\\ntion. It also records agent conversations for behavior analysis and capability understanding. An\\nInception-prompting technique is used to achieve autonomous cooperation between agents. Un-\\nlikeAutoGen , CAMEL does not natively support tool usage, such as code execution. Although it\\nis proposed as an infrastructure for multi-agent conversation, it only supports static conversation\\npatterns, while AutoGen additionally supports dynamic conversation patterns.\\n‚Ä¢Multi-Agent Debate: Two recent works investigate and show that multi-agent debate is an effec-\\ntive way to encourage divergent thinking in LLMs (Liang et al., 2023) and to improve the factuality\\nand reasoning of LLMs (Du et al., 2023). In both works, multiple LLM inference instances are\\nconstructed as multiple agents to solve problems with agent debate. Each agent is simply an LLM\\ninference instance, while no tool or human is involved, and the inter-agent conversation needs\\nto follow a pre-defined order. These works attempt to build LLM applications with multi-agent\\nconversation, while AutoGen , designed as a generic infrastructure, can be used to facilitate this\\ndevelopment and enable more applications with dynamic conversation patterns.\\n14', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='‚Ä¢MetaGPT : MetaGPT (Hong et al., 2023) is a specialized LLM application based on a multi-agent\\nconversation framework for automatic software development. They assign different roles to GPTs\\nto collaboratively develop software. They differ from AutoGen by being specialized solutions to\\na certain scenario, while AutoGen is a generic infrastructure to facilitate building applications for\\nvarious scenarios.\\nThere are a few other specialized single-agent or multi-agent systems, such as V oyager (Wang et al.,\\n2023a) and Generative Agents (Park et al., 2023), which we skip due to lower relevance. In Table 1,\\nwe summarize differences between AutoGen and the most relevant multi-agent systems.\\nTable 1: Summary of differences between AutoGen and other related multi-agent systems. infras-\\ntructure : whether the system is designed as a generic infrastructure for building LLM applications.\\nconversation pattern : the types of patterns supported by the implemented systems. Under the\\n‚Äòstatic‚Äô pattern, agent topology remains unchanged regardless of different inputs. AutoGen allows\\nflexible conversation patterns, including both static and dynamic patterns that can be customized\\nbased on different application needs. execution-capable : whether the system can execute LLM-\\ngenerated code; human involvement : whether (and how) the system allows human participation\\nduring the execution process of the system. AutoGen allows flexible human involvement in multi-\\nagent conversation with the option for humans to skip providing inputs.\\nAspect AutoGen Multi-agent Debate CAMEL BabyAGI MetaGPT\\nInfrastructure ‚úì ‚úó ‚úì ‚úó ‚úó\\nConversation pattern flexible static static static static\\nExecution-capable ‚úì ‚úó ‚úó ‚úó ‚úì\\nHuman involvement chat/skip ‚úó ‚úó ‚úó ‚úó\\n15', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='B Expanded Discussion\\nThe applications in Section 3 show how AutoGen not only enables new applications but also helps\\nrenovate existing ones. For example, in A1 (scenario 3), A5, and A6, AutoGen enabled the cre-\\nation of multi-agent conversations that follow a dynamic pattern instead of a fixed back-and-forth.\\nAnd in both A5 and A6, humans can participate in the activities together with multiple other AI\\nagents in a conversational manner. Similarly, A1-A4 show how popular applications can be reno-\\nvated quickly with AutoGen . Despite the complexity of these applications (most of them involve\\nmore than two agents or dynamic multi-turn agent cooperation), our AutoGen -based implementa-\\ntion remains simple, demonstrating promising opportunities to build creative applications and a large\\nspace for innovation. In reflecting on why these benefits can be achieved in these applications with\\nAutoGen , we believe there are a few reasons:\\n‚Ä¢Ease of use : The built-in agents can be used out-of-the-box, delivering strong performance even\\nwithout any customization. (A1, A3)\\n‚Ä¢Modularity : The division of tasks into separate agents promotes modularity in the system. Each\\nagent can be developed, tested, and maintained independently, simplifying the overall develop-\\nment process and facilitating code management. (A3, A4, A5, and A6)\\n‚Ä¢Programmability: AutoGen allows users to extend/customize existing agents to develop systems\\nsatisfying their specific needs with ease. (A1-A6). For example, with AutoGen , the core workflow\\ncode in A4 is reduced from over 430 lines to 100 lines, for a 4x saving.\\n‚Ä¢Allowing human involvement :AutoGen provides a native mechanism to achieve human partici-\\npation and/or human oversight. With AutoGen , humans can seamlessly and optionally cooperate\\nwith AIs to solve problems or generally participate in the activity. AutoGen also facilitates inter-\\nactive user instructions to ensure the process stays on the desired path. (A1, A2, A5, and A6)\\n‚Ä¢Collaborative/adversarial agent interactions : Like many collaborative agent systems (Dong\\net al., 2023), agents in AutoGen can share information and knowledge, to complement each other‚Äôs\\nabilities and collectively arrive at better solutions. (A1, A2, A3, and A4). Analogously, in certain\\nscenarios, some agents are required to work in an adversarial way. Relevant information is shared\\namong different conversations in a controlled manner, preventing distraction or hallucination. (A4,\\nA6). AutoGen supports both patterns, enabling effective utilization and augmentation of LLMs.\\nB.1 General Guidelines for Using AutoGen\\nBelow we give some recommendations for using agents in AutoGen to accomplish a task.\\n1.Consider using built-in agents first. For example, AssistantAgent is pre-configured to be\\nbacked by GPT-4, with a carefully designed system message for generic problem-solving via\\ncode. The UserProxyAgent is configured to solicit human inputs and perform tool execution.\\nMany problems can be solved by simply combining these two agents. When customizing agents\\nfor an application, consider the following options: (1) human input mode, termination condition,\\ncode execution configuration, and LLM configuration can be specified when constructing an\\nagent; (2) AutoGen supports adding instructions in an initial user message, which is an effective\\nway to boost performance without needing to modify the system message; (3) UserProxyAgent\\ncan be extended to handle different execution environments and exceptions, etc.; (4) when sys-\\ntem message modification is needed, consider leveraging the LLM‚Äôs capability to program its\\nconversation flow with natural language.\\n2.Start with a simple conversation topology . Consider using the two-agent chat or the group chat\\nsetup first, as they can often be extended with the least code. Note that the two-agent chat can\\nbe easily extended to involve more than two agents by using LLM-consumable functions in a\\ndynamic way.\\n3. Try to reuse built-in reply methods based on LLM, tool, or human before implementing a\\ncustom reply method because they can often be reused to achieve the goal in a simple way\\n(e.g., the built-in agent GroupChatManager ‚Äôs reply method reuses the built-in LLM-based reply\\nfunction when selecting the next speaker, ref. A5 in Section 3).\\n4. When developing a new application with UserProxyAgent ,start with humans always in\\nthe loop , i.e., human input mode=‚ÄòALWAYS‚Äô, even if the target operation mode is more au-\\ntonomous. This helps evaluate the effectiveness of AssistantAgent , tuning the prompt, dis-\\ncovering corner cases, and debugging. Once confident with small-scale success, consider setting\\n16', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='human input mode = ‚ÄòNEVER‚Äô. This enables LLM as a backend, and one can either use the\\nLLM or manually generate diverse system messages to simulate different use cases.\\n5. Despite the numerous advantages of AutoGen agents, there could be cases/scenarios where other\\nlibraries/packages could help . For example: (1) For (sub)tasks that do not have requirements\\nfor back-and-forth trouble-shooting, multi-agent interaction, etc., a unidirectional (no back-and-\\nforth message exchange) pipeline can also be orchestrated with LangChain (LangChain, 2023),\\nLlamaIndex (Liu, 2022), Guidance (Guidance, 2023), Semantic Kernel (Semantic-Kernel, 2023),\\nGorilla (Patil et al., 2023) or low-level inference API (‚Äòautogen.oai‚Äô provides an enhanced LLM\\ninference layer at this level) (Dibia, 2023). (2) When existing tools from LangChain etc. are\\nhelpful, one can use them as tool backends for AutoGen agents. For example, one can readily use\\ntools, e.g., Wolfram Alpha, from LangChain in AutoGen agent. (3) For specific applications, one\\nmay want to leverage agents implemented in other libraries/packages. To achieve this, one could\\nwrap those agents as conversable agents in AutoGen and then use them to build LLM applications\\nthrough multi-agent conversation. (4) It can be hard to find an optimal operating point among\\nmany tunable choices, such as the LLM inference configuration. Blackbox optimization packages\\nlike ‚Äòflaml.tune‚Äô (Wang et al., 2021) can be used together with AutoGen to automate such tuning.\\nB.2 Future Work\\nThis work raises many research questions and future directions and .\\nDesigning optimal multi-agent workflows: Creating a multi-agent workflow for a given task can\\ninvolve many decisions, e.g., how many agents to include, how to assign agent roles and agent\\ncapabilities, how the agents should interact with each other, and whether to automate a particular\\npart of the workflow. There may not exist a one-fits-all answer, and the best solution might depend\\non the specific application. This raises important questions: For what types of tasks and applications\\nare multi-agent workflows most useful? How do multiple agents help in different applications? For\\na given task, what is the optimal (e.g., cost-effective) multi-agent workflow?\\nCreating highly capable agents: AutoGen can enable the development of highly capable agents\\nthat leverage the strengths of LLMs, tools, and humans. Creating such agents is crucial to ensuring\\nthat a multi-agent workflow can effectively troubleshoot and make progress on a task. For example,\\nwe observed that CAMEL, another multi-agent LLM system, cannot effectively solve problems in\\nmost cases primarily because it lacks the capability to execute tools or code. This failure shows that\\nLLMs and multi-agent conversations with simple role playing are insufficient, and highly capable\\nagents with diverse skill sets are essential. We believe that more systematic work will be required to\\ndevelop guidelines for application-specific agents, to create a large OSS knowledge base of agents,\\nand to create agents that can discover and upgrade their skills (Cai et al., 2023).\\nEnabling scale, safety, and human agency: Section 3 shows how complex multi-agent workflows\\ncan enable new applications, and future work will be needed to assess whether scaling further can\\nhelp solve extremely complex tasks. However, as these workflows scale and grow more complex,\\nit may become difficult to log and adjust them. Thus, it will become essential to develop clear\\nmechanisms and tools to track and debug their behavior. Otherwise, these techniques risk resulting\\nin incomprehensible, unintelligible chatter among agents (Lewis et al., 2017).\\nOur work also shows how complex, fully autonomous workflows with AutoGen can be useful, but\\nfully autonomous agent conversations will need to be used with care. While the autonomous mode\\nAutoGen supports could be desirable in many scenarios, a high level of autonomy can also pose\\npotential risks, especially in high-risk applications (Amodei et al., 2016; Weld & Etzioni, 1994). As\\na result, building fail-safes against cascading failures and exploitation, mitigating reward hacking,\\nout of control and undesired behaviors, maintaining effective human oversight of applications built\\nwith AutoGen agents will become important. While AutoGen provides convenient and seamless\\ninvolvement of humans through a user proxy agent, developers and stakeholders still need to under-\\nstand and determine the appropriate level and pattern of human involvement to ensure the safe and\\nethical use of the technology (Horvitz, 1999; Amershi et al., 2019).\\n17', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='C Default System Message for Assistant Agent\\nSystemMessageYou are a helpful AI assistant. Solve tasks using your coding and language skills.In the following cases, suggest python code (in a python coding block) or shell script (in a shcoding block) for the user to execute.1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.2. When you need to perform some task with code, use the code to perform the task and output theresult. Finish the task smartly.Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can‚Äôt modify your code. So do not suggest incomplete code which requires users to modify. Don‚Äôt use a code block if it‚Äôs not intended to be executed by the user.If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don‚Äôt include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use ‚Äôprint‚Äô function for the output when relevant. Check the execution result returned by the user.If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can‚Äôt be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.Reply ‚ÄúTERMINATE‚Äù in the end when everything is done.Prompting techniques color code: Role Play; Control Flow; Output Confine; Facilitate Automation; Grounding\\nFigure 5: Default system message for the built-in assistant agent in AutoGen (v0.1.1). This is an\\nexample of conversation programming via natural language. It contains instructions of different\\ntypes, including role play, control flow, output confine, facilitate automation, and grounding.\\nFigure 5 shows the default system message for the built-in assistant agent in AutoGen (v0.1.1),\\nwhere we introduce several new prompting techniques and highlight them accordingly. When com-\\nbining these new prompting techniques together, we can program a fairly complex conversation even\\nwith the simplest two-agent conversation topology. This approach tries to exploit the capability of\\nLLMs in implicit state inference to a large degree. LLMs do not follow all the instructions perfectly,\\nso the design of the system needs to have other mechanisms to handle the exceptions and faults.\\nSome instructions can have ambiguities, and the designer should either reduce them for preciseness\\nor intentionally keep them for flexibility and address the different situations in other agents. In\\ngeneral, we observe that GPT-4 follows the instructions better than GPT-3.5-turbo.\\n18', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='D Application Details\\nA1: Math Problem Solving\\nScenario 1: Autonomous Problem Solving. We perform both qualitative and quantitative eval-\\nuations in this scenario. For all evaluations, we use GPT-4 as the base model, and pre-install the\\n‚Äúsympy‚Äù package in the execution environment. We compare AutoGen with the following LLM-\\nbased agent systems:\\n‚Ä¢ AutoGPT: The out-of-box AutoGPT is used. We initialize AutoGPT by setting the purpose to\\n‚Äúsolve math problems‚Äù, resulting in a ‚ÄúMathSolverGPT‚Äù with auto-generated goals.\\n‚Ä¢ ChatGPT+Plugin: We enable the Wolfram Alpha plugin (a math computation engine) in the Ope-\\nnAI web client.\\n‚Ä¢ ChatGPT+Code Interpreter: This is a recent feature in OpenAI web client. Note that the above\\ntwo premium features from ChatGPT require a paid subscription to be accessed and are the most\\ncompetitive commercial systems.\\n‚Ä¢ LangChain ReAct+Python: We use Python agent from LangChain. To handle parsing errors, we\\nset ‚Äúhandle parsing errors=True‚Äù, and use the default zero-shot ReAct prompt.\\n‚Ä¢ Multi-Agent Debate (Liang et al., 2023): We modified the code of the multi-agent debate to per-\\nform evaluation. By default, there are three agents: an affirmative agent, a negative agent, and a\\nmoderator.\\nWe also conducted preliminary evaluations on several other multi-agent systems, including\\nBabyAGI, CAMEL, and MetaGPT. The results indicate that they are not suitable choices for solving\\nmath problems out of the box. For instance, when MetaGPT is tasked with solving a math problem,\\nit begins developing software to address the problem, but most of the time, it does not actually solve\\nthe problem. We have included the test examples in Appendix E.\\nTable 2: Qualitative evaluation of two math problems from the MATH dataset within the autonomous\\nproblem-solving scenario. Each LLM-based system is tested three times on each of the problems.\\nThis table reports the problem-solving correctness and summarizes the reasons for failure.\\nCorrectness Failure Reason\\nAutoGen 3/3 N/A.\\nAutoGPT 0/3 The LLM gives code without the print function so the\\nresult is not printed.\\nChatGPT+Plugin 1/3 The return from Wolfram Alpha contains 2 simplified\\nresults, including the correct answer, but GPT-4 always\\nchooses the wrong answer.\\nChatGPT+Code Interpreter 2/3 Returns a wrong decimal result.\\nLangChain ReAct 0/3 LangChain gives 3 different wrong answers.\\nMulti-Agent Debate 0/3 It gives 3 different wrong answers due to calculation errors.\\n(a) Evaluation on the first problem that asks to simplify a square root fraction.\\nCorrectness Failure Reason\\nAutoGen 2/3 The final answer from code execution is wrong.\\nAutoGPT 0/3 The LLM gives code without the print function so the\\nresult is not printed.\\nChatGPT+Plugin 1/3 For one trial, GPT-4 got stuck because it keeps giving\\nwrong queries and has to be stopped. Another trial simply\\ngives a wrong answer.\\nChatGPT+Code Interpreter 0/3 It gives 3 different wrong answers.\\nLangChain ReAct 0/3 LangChain gives 3 different wrong answers.\\nMulti-Agent Debate 0/3 It gives 3 different wrong answers.\\n(b) Evaluation on the second number theory problem.\\nFor the qualitative evaluation, we utilize two level-5 problems from the MATH dataset, testing each\\nproblem three times. The first problem involves simplifying a square root fraction, and the second\\n19', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='problem involves solving a number theory issue. The correctness counts and reasons for failure\\nare detailed in Table 2. For the quantitative evaluation, we conduct two sets of experiments on\\nthe MATH dataset to assess the correctness of these systems: (1) an experiment involving 120\\nlevel-5 (the most challenging level) problems, including 20 problems from six categories, excluding\\ngeometry, and (2) an experiment on the entire test set, which includes 5000 problems. We exclude\\nAutoGPT from this evaluation as it cannot access results from code executions and does not solve\\nany problems in the qualitative evaluation. Our analysis of the entire dataset reveals that AutoGen\\nachieves an overall accuracy of 69.48%, while GPT-4‚Äôs accuracy stands at 55.18%. From these\\nevaluations, we have the following observations regarding the problem-solving success rate and\\nuser experience of these systems:\\n‚Ä¢ Problem-solving success rate: Results from the quantitative evaluations show that AutoGen can\\nhelp achieve the highest problem-solving success rate among all the compared methods. The qual-\\nitative evaluations elucidate common failure reasons across several alternative approaches. Chat-\\nGPT+Code Interpreter fails to solve the second problem, and ChatGPT+Plugin struggles to solve\\nboth problems. AutoGPT fails on both problems due to code execution issues. The LangChain\\nagent also fails on both problems, producing code that results in incorrect answers in all trials.\\n‚Ä¢ Based on the qualitative evaluation, we analyze the user experience concerning the verbosity of\\nthe response and the ability of the LLM-based system to run without unexpected behaviors. Chat-\\nGPT+Plugin is the least verbose, mainly because Wolfram queries are much shorter than Python\\ncode. AutoGen , ChatGPT+Code Interpreter, and LangChain exhibit similar verbosity, although\\nLangChain is slightly more verbose due to more code execution errors. AutoGPT is the most\\nverbose system owing to predefined steps like THOUGHTS, REASONING, and PLAN, which it\\nincludes in replies every time. Overall, AutoGen and ChatGPT+Code Interpreter operate smoothly\\nwithout exceptions. We note the occurrences of undesired behaviors from other LLM-based sys-\\ntems that could affect user experience: AutoGPT consistently outputs code without the print‚Äô\\nstatement and cannot correct this, requiring the user to run them manually; ChatGPT with Wol-\\nfram Alpha plugin has the potential to become stuck in a loop that must be manually stopped; and\\nLangchain ReAct could exit with a parse error, necessitating the passing of a ‚Äòhandle parse error‚Äô\\nparameter.\\nEnable Multi-User Problem Solving ViaStudent        and Expert Student Proxy\\nStudentAssistant\\nExpertAssistant\\nExpert Proxy\\nAsk for expert\\nEnable Autonomous and Human-in-the-loop Problem Solving\\nFigure 6: Examples of three settings utilized to solve math problems using AutoGen : (Gray) En-\\nables a workflow where a student collaborates with an assistant agent to solve problems, either\\nautonomously or in a human-in-the-loop mode. (Gray + Orange) Facilitates a more sophisticated\\nworkflow wherein the assistant, on the fly, can engage another user termed ‚Äúexpert‚Äù, who is in the\\nloop with their own assistant agent, to aid in problem-solving if its own solutions are not satisfactory.\\nScenario 2: Human-in-the-loop Problem Solving. For challenging problems that these LLM\\nsystems cannot solve autonomously, human feedback during the problem-solving process can be\\n20', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='helpful. To incorporate human feedback with AutoGen , one can set human input mode=‚ÄòALWAYS‚Äô\\nin the user proxy agent. We select one challenging problem that none of these systems can solve\\nautonomously across three trials. We adhere to the process outlined below to provide human inputs\\nfor all the compared methods:\\n1. Input the problem: Find the equation of the plane which bisects the angle\\nbetween the planes 3x‚àí6y+ 2z+ 5 = 0 and4x‚àí12y+ 3z‚àí3 = 0 ,and which\\ncontains the point (‚àí5,‚àí1,‚àí5).Enter your answer in the form\\nAx+By+Cz+D= 0,\\nwhere A, B, C, D are integers such that A > 0and\\ngcd(|A|,|B|,|C|,|D|) = 1.\\n2. The response from the system does not solve the problem correctly. We then give a\\nhint to the model: Your idea is not correct. Let‚Äôs solve this together.\\nSuppose P= (x, y, z )is a point that lies on a plane that bisects the\\nangle, the distance from P to the two planes is the same. Please\\nset up this equation first.\\n3. We expect the system to give the correct distance equation. Since the equation involves\\nan absolute sign that is hard to solve, we would give the next hint: Consider the two\\ncases to remove the abs sign and get two possible solutions.\\n4. If the system returns the two possible solutions and doesn‚Äôt continue to the next step, we\\ngive the last hint: Use point (-5,-1,-5) to determine which is correct and\\ngive the final answer.\\n5. Final answer is 11x+6y+5z+86=0 .\\nWe observed that AutoGen consistently solved the problem across all three trials. ChatGPT+Code\\nInterpreter and ChatGPT+Plugin managed to solve the problem in two out of three trials, while Au-\\ntoGPT failed to solve it in all three attempts. In its unsuccessful attempt, ChatGPT+Code Interpreter\\nfailed to adhere to human hints. In its failed trial, ChatGPT+Plugin produced an almost correct solu-\\ntion but had a sign discrepancy in the final answer. AutoGPT was unable to yield a correct solution\\nin any of the trials. In one trial, it derived an incorrect distance equation. In the other two trials, the\\nfinal answer was incorrect due to code execution errors.\\nScenario 3: Multi-User Problem Solving. Next-generation LLM applications may necessitate\\nthe involvement of multiple real users for collectively solving a problem with the assistance of\\nLLMs. We showcase how AutoGen can be leveraged to effortlessly construct such a system. Specif-\\nically, building upon scenario 2 mentioned above, we aim to devise a simple system involving two\\nhuman users: a student and an expert. In this setup, the student interacts with an LLM assistant to\\naddress some problems, and the LLM automatically resorts to the expert when necessary.\\nThe overall workflow is as follows: The student chats with the LLM-based assistant agent through\\na student proxy agent to solve problems. When the assistant cannot solve the problem satisfactorily,\\nor the solution does not match the expectation of the student, it would automatically hold the con-\\nversation and call the pre-defined askforexpert function via the function callfeature of GPT\\nin order to resort to the expert. Specifically, it would automatically produce the initial message for\\ntheaskforexpert function, which could be the statement of the problem or the request to verify\\nthe solution to a problem, and the expert is supposed to respond to this message with the help of\\nthe expert assistant. After the conversation between the expert and the expert‚Äôs assistant, the final\\nmessage would be sent back to the student assistant as the response to the initial message. Then, the\\nstudent assistant would resume the conversation with the student using the response from the expert\\nfor a better solution. A detailed visualization is shown in Figure 6.\\nWith AutoGen , constructing the student/expert proxy agent and the assistant agents is straight-\\nforward by reusing the built-in UserProxyAgent andAssistantAgent through appropriate\\nconfigurations. The only development required involves writing several lines of code for the\\naskforexpert function, which then becomes part of the configuration for the assistant. Ad-\\nditionally, it‚Äôs easy to extend such a system to include more than one expert, with a specific\\naskforexpert function for each, or to include multiple student users with a shared expert for\\nconsultation.\\n21', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='A2: Retrieval-Augmented Code Generation and Question Answering\\nRetrieval-augmentedAssistantRetrieval-augmented  User Proxy\\n1. Question and Contexts3. Terminate,feedbacks or `Update Context`4. Satisfied Answers or Terminate\\n2. Satisfied Answers or `Update Context`\\nFigure 7: Overview of Retrieval-augmented Chat which involves two agents, including a Retrieval-\\naugmented User Proxy and a Retrieval-augmented Assistant. Given a set of documents, the\\nRetrieval-augmented User Proxy first automatically processes documents‚Äîsplits, chunks, and stores\\nthem in a vector database. Then for a given user input, it retrieves relevant chunks as context and\\nsends it to the Retrieval-augmented Assistant, which uses LLM to generate code or text to answer\\nquestions. Agents converse until they find a satisfactory answer.\\nDetailed Workflow. The workflow of Retrieval-Augmented Chat is illustrated in Figure 7. To\\nuse Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented\\nUser Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy\\nnecessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented\\nUser Proxy can download the documents, segment them into chunks of a specific size, compute\\nembeddings, and store them in a vector database. Once a chat is initiated, the agents collaboratively\\nengage in code generation or question-answering adhering to the procedures outlined below:\\n1. The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding simi-\\nlarity, and sends them along with the question to the Retrieval-Augmented Assistant.\\n2. The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based\\non the question and context provided. If the LLM is unable to produce a satisfactory response, it\\nis instructed to reply with ‚ÄúUpdate Context‚Äù to the Retrieval-Augmented User Proxy.\\n3. If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and\\nsends the output as feedback. If there are no code blocks or instructions to update the context, it\\nterminates the conversation. Otherwise, it updates the context and forwards the question along\\nwith the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation\\nis enabled, individuals can proactively send any feedback, including Update Context‚Äù, to the\\nRetrieval-Augmented Assistant.\\n4. If the Retrieval-Augmented Assistant receives ‚ÄúUpdate Context‚Äù, it requests the next most similar\\nchunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it\\ngenerates new code or text based on the feedback and chat history. If the LLM fails to generate\\nan answer, it replies with ‚ÄúUpdate Context‚Äù again. This process can be repeated several times.\\nThe conversation terminates if no more documents are available for the context.\\nWe utilize Retrieval-Augmented Chat in two scenarios. The first scenario aids in generating code\\nbased on a given codebase. While LLMs possess strong coding abilities, they are unable to utilize\\npackages or APIs that are not included in their training data, e.g., private codebases, or have trouble\\nusing trained ones that are frequently updated post-training. Hence, Retrieval-Augmented Code\\nGeneration is considered to be highly valuable. The second scenario involves question-answering\\non the Natural Questions dataset (Kwiatkowski et al., 2019), enabling us to obtain comparative\\nevaluation metrics for the performance of our system.\\nScenario 1: Evaluation on Natural Questions QA dataset. In this case, we evaluate the\\nRetrieval-Augmented Chat‚Äôs end-to-end question-answering performance using the Natural Ques-\\ntions dataset (Kwiatkowski et al., 2019). We collected 5,332 non-redundant context documents and\\n6,775 queries from HuggingFace. First, we create a document collection based on the entire context\\ncorpus and store it in the vector database. Then, we utilize Retrieval-Augmented Chat to answer the\\nquestions. An example (Figure 8) from the NQ dataset showcases the advantages of the interactive\\nretrieval feature: ‚Äúwho carried the usa flag in opening ceremony‚Äù . When attempting to answer\\nthis question, the context with the highest similarity to the question embedding does not contain the\\n22', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='Task description who carried the usaflag in opening ceremony?Assistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply exactly ‚ÄòUPDATE CONTEXT‚Äô. You must give as short an answer as possible.User‚Äôs question is: who carried the usaflag in opening ceremonyContext is: < P > On January 17 , 1899 , under ... The Astros beat the Yankees that night , 2 -1 . < /P >UserProxy: Sorry, I cannot find any information about who carried the USA flag in the opening ceremony. UPDATE CONTEXT.Assistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply exactly ‚ÄòUPDATE CONTEXT‚Äô. You must give as short an answer as possible.User‚Äôs question is: who carried the usaflag in opening ceremonyContext is: < T able > ... Erin Hamlin ... < /T d >UserProxy: Erin Hamlin carried the USA flag in the opening ceremony. \\n(a) W/O Interactive Retrieval\\n(b) W/ Interactive Retrieval\\nAssistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply ‚Äòsorry, I don‚Äôt know‚Äô. You must give as short an answer as possible.User‚Äôs question is:who carried the usaflag in opening ceremonyContext is: < P > On January 17 , 1899 , under ... The Astros beat the Yankees that night , 2 -1 . < /P >UserProxy: Sorry, I cannot find any information about who carried the USA flag in the opening ceremony. Sorry, I don‚Äôt know.Figure 8: Retrieval-augmented Chat without (W/O) and with (W/) interactive retrieval .\\nrequired information for a response. As a result, the LLM assistant (GPT-3.5-turbo) replies ‚ÄúSorry,\\nI cannot find any information about who carried the USA flag in the opening ceremony. UPDATE\\nCONTEXT. ‚Äù With the unique and innovative ability to update context in Retrieval-Augmented Chat,\\nthe user proxy agent automatically updates the context and forwards it to the assistant agent again.\\nFollowing this process, the agent is able to generate the correct answer to the question.\\nIn addition, we conduct an experiment using the same prompt as illustrated in (Adlakha et al., 2023)\\nto investigate the advantages of AutoGen W/O interactive retrieval . The F1 score and Recall for the\\nfirst 500 questions are 23.40% and 62.60%, respectively, aligning closely with the results reported\\nin Figure 4b. Consequently, we assert that AutoGen W/O interactive retrieval outperforms DPR due\\nto differences in the retrievers employed. Specifically, we utilize a straightforward vector search\\nretriever with the all-MiniLM-L6-v2 model for embeddings.\\nFurthermore, we analyze the number of LLM calls in experiments involving both AutoGen and\\nAutoGen W/O interactive retrieval , revealing that approximately 19.4% of questions in the Natural\\nQuestions dataset trigger an ‚ÄúUpdate Context‚Äù operation, resulting in additional LLM calls.\\nScenario 2: Code Generation Leveraging Latest APIs from the Codebase. In this case, the ques-\\ntion is ‚ÄúHow can I use FLAML to perform a classification task and use Spark for parallel training?\\nTrain for 30 seconds and force cancel jobs if the time limit is reached. ‚Äù . FLAML (v1) (Wang et al.,\\n2021) is an open-source Python library designed for efficient AutoML and tuning. It was open-\\nsourced in December 2020, and is included in the training data of GPT-4. However, the question\\nnecessitates the use of Spark-related APIs, which were added in December 2022 and are not encom-\\npassed in the GPT-4 training data. Consequently, the original GPT-4 model is unable to generate the\\ncorrect code, due to its lack of knowledge regarding Spark-related APIs. Instead, it erroneously cre-\\nates a non-existent parameter, spark , and sets it to True‚Äô. Nevertheless, with Retrieval-Augmented\\nChat, we provide the latest reference documents as context. Then, GPT-4 generates the correct code\\nblocks by setting usespark andforce cancel to True‚Äô.\\n23', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='A3: Decision Making in Text World Environments\\nALFWorld\\nExecutor\\nReward & StateAction Decision\\nAssistant\\nObservation: On the desk 2, you see an alarmclock 3, \\na bowl 3, a creditcard 2, a mug 1, and a pencil 2.Action decision: Pick up pencil 2 from desk 2\\nGroundingAgent\\nALFChat (two agents) ALFChat (three agents)\\nALFWorld Executor\\nAssistant\\nFigure 9: We use AutoGen to solve tasks in the ALFWorld benchmark, which contains household\\ntasks described in natural language. We propose two designs: a two-agent design where the assistant\\nagent suggests the next step, and the Executor executes actions and provides feedback. The three-\\nagent design adds a grounding agent that supplies commonsense facts to the executor when needed.\\nALFWorld (Shridhar et al., 2021) is a synthetic language-based interactive decision-making task.\\nIt comprises textual environments that aim to simulate real-world household scenes. Given a high-\\nlevel goal (e.g., putting a hot apple in the fridge) and the description of the household environment,\\nthe agent needs to explore and interact with the simulated household environment through a textual\\ninterface. A typical task environment contains various types of locations and could require more\\nthan 40 steps to finish, which highlights the need for agents to decompose the goal into subtasks and\\ntackle them one by one, while effectively exploring the environments.\\nDetailed Workflow. We first propose a straightforward two-agent system with AutoGen , illustrated\\non the left-hand side of Figure 9, to tackle tasks from this benchmark. The system consists of\\nan assistant agent and an executor agent. The assistant agent generates plans and makes action\\ndecisions to solve the tasks. The executor agent is tailored specifically for ALFWorld. It performs\\nactions proposed by the assistant and reports action execution results in the household environment\\nas feedback to the assistant. Due to the strict format requirements for the output format, we use the\\nBLEU metric to evaluate the similarity of the output to all valid action options. The option with the\\nhighest similarity will be chosen as the action for this round.\\nOne major challenge encompassed in ALFWorld is commonsense reasoning. The agent needs to\\nextract patterns from the few-shot examples provided and combine them with the agent‚Äôs general\\nknowledge of household environments to fully understand task rules. More often than not, the as-\\nsistant tends to neglect some basic knowledge of the household environment. Thanks to the easy-to-\\nimplement multi-agent conversational feature of AutoGen , enhancing the assistant agent‚Äôs reason-\\ning ability by adding a new grounding agent to provide commonsense facts for the decision-making\\nagent‚Äôs reference becomes straightforward. By scrutinizing the failed attempts and summarizing the\\nreasons for failure, we obtained a holistic understanding of the commonsense knowledge that the\\nassistant agent lacks. Then, we set a grounding agent to provide this general knowledge when the\\ntask begins and whenever the assistant outputs the same action three times in a row. This ensures the\\nassistant takes this commonsense knowledge into consideration and prevents it from getting stuck in\\noutputting the same content or constantly apologizing.\\nWe compare our system‚Äôs performance with ReAct, which treats ALFWorld as a text-completion\\ntask. ReAct (Yao et al., 2022) is a few-shot prompting technique that interleaves reasoning and\\nacting, allowing for greater synergy between the two and significantly improving performance on\\nboth language and decision-making tasks. We integrate ReAct into AutoGen by modifying the\\nprompts in a conversational manner. Following ReAct, we employ a two-shot setting. The few-\\nshot prompts are obtained from the corresponding repository. As shown in Table 3, the two-agent\\n24', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content=\"Task description\\nYou are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a safe 1, a drawer 2, a dra wer1, a \\nshelf 3, a shelf 2, and a shelf 1.\\nYour task is to: look at bowl under the desklamp . \\nAssistant (to Executor) : THOUGHT [First, I need to find a bowl. A \\nbowl is more likely to appear in desk (1 -2), drawer (1 -2), shelf \\n(1-3), bed (1). Then I need to find and use a desklamp .] ACTION \\n[go to desk 1]\\n(Assistant finds the desklamp at desk 1, then goes to desk 2.)\\nExecutor (to Assistant) : On the desk 2, you see a bowl 1, and a \\ncd 3... \\nAssistant (to Executor) : THOUGHT [Now I find a bowl (1). I need \\nto use the desklamp to look at the bowl. I'll go to the desklamp\\nnow.] ACTION [go to desk 1]\\nExecutor (to Assistant): On the desk 1, you see a desklamp 1, a \\npencil 2...\\nAssistant: ACTION [use desklamp 1]\\nExecutor (to Assistant) : You turn on the desklamp 1.\\nAssistant (to Executor): THOUGHT [Now that the desklamp is turned \\non, I can use it to look at the bowl (1) that I found earlier.] \\nACTION [look at bowl 1 under the desklamp 1]\\nExecutor (to Assistant) : You turn on the desklamp 1.\\n(Assistant falls into an infinite loop turning on desklamp .)\\nExecutor (to Assistant) : Task failed. Reply TERMINATE.\\nAssistant (to Executor) : TERMINATEAssistant (to Executor) : THOUGHT [First, I need to find a bowl. A \\nbowl is more likely to appear in desk (1 -2), drawer (1 -2), shelf \\n(1-3), bed (1). Then I need to find and use a desklamp .] ACTION \\n[go to desk 1]\\n(Assistant finds the desklamp at desk 1, then goes to desk 2.)\\n(Assistant falls into the same loop as (a))\\nGroundingAgent (to Executor) : You must find and take the object \\nbefore you can examine it. You must go to where the target \\nobject is before you can use it.\\nExecutor(to Assistant): You turn on the desklamp 1. Hint: You must \\nfind and take the object before you can examine it. \\nAssistant (to Executor): THOUGHT [I need to find and take the \\nbowl (1) before I can examine it. Let me find the bowl first.] \\nACTION [go to desk 2]\\nExecutor (to Assistant): On the desk 2, you see a bowl 1, a cd \\n3...\\nAssistant (to Executor) : ACTION [take bowl 1 from desk 2]\\nExecutor (to Assistant): You pick up bowl 1 from desk 2.\\nAssistant (to Executor): ACTION [go to desk 1]\\nAssistant (to Executor): ACTION[use desklamp 1]\\nExecutor (to Assistant) : Task success. Reply TERMINATE.\\nAssistant (to Executor) : TERMINATE\\n(a) Two agents\\n (b) Three agentsFigure 10: Comparison of results from two designs: (a) Two-agent design which consists of an\\nassistant and an executor, (b) Three-agent design which adds a grounding agent that serves as a\\nknowledge source. For simplicity, we omit the in-context examples and part of the exploration\\ntrajectory, and only show parts contributing to the failure/success of the attempt.\\nMethod Pick Clean Heat Cool Look Pick 2 All\\nReAct (avg) 63 52 48 71 61 24 54\\nALFChat (2 agents)(avg) 61 58 57 67 50 19 54\\nALFChat (3 agents)(avg) 79 64 70 76 78 41 69\\nReAct (best of 3) 75 62 61 81 78 35 66\\nALFChat (2 agents)(best of 3) 71 61 65 76 67 35 63\\nAFLChat (3 agents)(best of 3) 92 74 78 86 83 41 77\\nTable 3: Comparisons between ReAct and the two variants of ALFChat on the ALFWorld bench-\\nmark. For each task, we report the success rate out of 3 attempts. Success rate denotes the number\\nof tasks successfully completed by the agent divided by the total number of tasks. The results show\\nthat adding a grounding agent significantly improves the task success rate in ALFChat.\\ndesign matches the performance of ReAct, while the three-agent design significantly outperforms\\nReAct. We surmise that the performance discrepancy is caused by the inherent difference between\\ndialogue-completion and text-completion tasks. On the other hand, introducing a grounding agent\\nas a knowledge source remarkably advances performance on all types of tasks.\\nCase study . Figure 10 exemplifies how a three-agent design eliminates one root cause for failure\\ncases. Most of the tasks involve taking an object and then performing a specific action with it (e.g.,\\nfinding a vase and placing it on a cupboard). Without a grounding agent, the assistant frequently\\nconflates finding an object with taking it, as illustrated in Figure 10a). This leads to most of the\\nfailure cases in ‚Äôpick‚Äô and ‚Äôlook‚Äô type tasks. With the introduction of a grounding agent, the assistant\\ncan break out of this loop and successfully complete the task\\nTakeaways. We introduced a grounding agent to serve as an external commonsense knowledge\\nsource, which significantly enhanced the assistant‚Äôs ability to make informed decisions. This proves\\nthat providing necessary commonsense facts to the decision-making agent can assist it in making\\nmore informed decisions, thus effectively boosting the task success rate. AutoGen brings both\\nsimplicity and modularity when adding the grounding agent.\\n25\", metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='A4: Multi-Agent Coding\\nCommander\\nSafeguard\\nWriter\\n‚ë°Question, ‚ùªLog‚ù∏Code, ‚ë¶Ans‚ùπCode‚ù∫Clearance\\nUser‚ë†User Question‚ëßFinal AnswerRepeat until answering the user‚Äôs question or timeout\\nFigure 11: Our re-implementation of OptiGuide with AutoGen streamlining agents‚Äô interactions.\\nThe Commander receives user questions (e.g., What if we prohibit shipping from supplier 1 to\\nroastery 2?) and coordinates with the Writer and Safeguard. The Writer crafts the code and inter-\\npretation, the Safeguard ensures safety (e.g., not leaking information, no malicious code), and the\\nCommander executes the code. If issues arise, the process can repeat until resolved. Shaded circles\\nrepresent steps that may be repeated multiple times.\\nDetailed Workflow. The workflow can be described as follows. The end user initiates the in-\\nteraction by posing a question, such as ‚ÄúWhat if we prohibit shipping from supplier 1 to roastery\\n2?‚Äù, marked by\\n1 to the Commander agent. The Commander manages and coordinates with two\\nLLM-based assistant agents: the Writer and the Safeguard. Apart from directing the flow of commu-\\nnication, the Commander has the responsibility of handling memory tied to user interactions. This\\ncapability enables the Commander to capture and retain valuable context regarding the user‚Äôs ques-\\ntions and their corresponding responses. Such memory is subsequently shared across the system,\\nempowering the other agents with context from prior user interactions and ensuring more informed\\nand relevant responses.\\nIn this orchestrated process, the Writer, who combines the functions of a ‚ÄúCoder‚Äù and an ‚ÄúInter-\\npreter‚Äù as defined in (Li et al., 2023a), will craft code and also interpret execution output logs. For in-\\nstance, during code writing (\\n2 and\\n3 ), the Writer may craft code ‚Äúmodel.addConstr(x[‚Äòsupplier1‚Äô,\\n‚Äòroastery2‚Äô] == 0, ‚Äòprohibit‚Äô)‚Äù to add an additional constraint to answer the user‚Äôs question.\\nAfter receiving the code, the Commander will communicate with the Safeguard to screen the code\\nand ascertain its safety (\\n4 ); once the code obtains the Safeguard‚Äôs clearance, marked by\\n5 , the\\nCommander will use external tools (e.g., Python) to execute the code and request the Writer to\\ninterpret the execution results for the user‚Äôs question (\\n6 and\\n7 ). For instance, the writer may\\nsay ‚Äúif we prohibit shipping from supplier 1 to roastery 2, the total cost would increase by 10.5%.‚Äù\\nBringing this intricate process full circle, the Commander furnishes the user with the concluding\\nanswer (\\n8 ).\\nIf at a point there is an exception - either a security red flag raised by Safeguard (in\\n5 ) or code\\nexecution failures within Commander, the Commander redirects the issue back to the Writer with\\nessential information in logs (\\n6 ). So, the process from\\n3 to\\n6 might be repeated multiple times,\\nuntil each user query receives a thorough and satisfactory resolution or until the timeout. This entire\\ncomplex workflow of multi-agent interaction is elegantly managed via AutoGen .\\nThe core workflow code for OptiGuide was reduced from over 430 lines to 100 lines using AutoGen ,\\nleading to significant productivity improvement. The new agents are customizable, conversable, and\\ncan autonomously manage their chat memories. This consolidation allows the coder and interpreter\\nroles to merge into a single ‚ÄúWriter‚Äù agent, resulting in a clean, concise, and intuitive implementation\\nthat is easier to maintain.\\n26', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='Manual Evaluation Comparing ChatGPT + Code Interpreter and AutoGen -based OptiGuide.\\nChatGPT + Code Interpreter is unable to execute code with private or customized dependencies (e.g.,\\nGurobi), which means users need to have engineering expertise to manually handle multiple steps,\\ndisrupting the workflow and increasing the chance for mistakes. If users lack access or expertise,\\nthe burden falls on supporting engineers, increasing their on-call time.\\nWe carried out a user study that juxtaposed OpenAI‚Äôs ChatGPT coupled with a Code Interpreter\\nagainst AutoGen -based OptiGuide. The study focused on a coffee supply chain scenario, and an\\nexpert Python programmer with proficiency in Gurobi participated in the test. We evaluated both\\nsystems based on 10 randomly selected questions, measuring time and accuracy. While both sys-\\ntems answered 8 questions correctly, the Code Interpreter was significantly slower than OptiGuide\\nbecause the former requires more manual intervention. On average, users needed to spend 4 minutes\\nand 35 seconds to solve problems with the Code Interpreter, with a standard deviation of approxi-\\nmately 2.5 minutes. In contrast, OptiGuide‚Äôs average problem-solving time was around 1.5 minutes,\\nmost of which was spent waiting for responses from the GPT-4 model. This indicates a 3x saving\\non the user‚Äôs time with AutoGen -based OptiGuide.\\nWhile using ChatGPT + Code Interpreter, users had to read through the code and instructions to\\nknow where to paste the code snippets. Additionally, running the code involves downloading it and\\nexecuting it in a terminal, a process that was both time-consuming and prone to errors. The response\\ntime from the Code Interpreter is also slower, as it generates lots of tokens to read the code, read the\\nvariables line-by-line, perform chains of thought analysis, and then produce the final answer code.\\nIn contrast, AutoGen integrates multiple agents to reduce user interactions by 3 - 5 times on average\\nas reported in Table 4, where we evaluated our system with 2000 questions across five OptiGuide\\napplications and measured how many prompts the user needs to type.\\nTable 4: Manual effort saved with OptiGuide (W/ GPT-4) while preserving the same coding perfor-\\nmance is shown in the data below. The data include both the mean and standard deviations (indicated\\nin parentheses).\\nDataset netflow facility tsp coffee diet\\nSaving Ratio 3.14x (0.65) 3.14x (0.64) 4.88x (1.71) 3.38x (0.86) 3.03x (0.31)\\nTable 13 and 15 provide a detailed comparison of user experience with ChatGPT+Code Interpreter\\nandAutoGen -based OptiGuide. ChatGPT+Code Interpreter is unable to run code with private pack-\\nages or customized dependencies (such as Gurobi); as a consequence, ChatGPT+Code Interpreter\\nrequires users to have engineering expertise and to manually handle multiple steps, disrupting the\\nworkflow and increasing the chance for mistakes. If customers lack access or expertise, the bur-\\nden falls on supporting engineers, increasing their on-call time. In contrast, the automated chat by\\nAutoGen is more streamlined and autonomous, integrating multiple agents to solve problems and\\naddress concerns. This results in a 5x reduction in interaction and fundamentally changes the over-\\nall usability of the system. A stable workflow can be potentially reused for other applications or to\\ncompose a larger one.\\nTakeaways: The implementation of the multi-agent design with AutoGen in the OptiGuide appli-\\ncation offers several advantages. It simplifies the Python implementation and fosters a mixture of\\ncollaborative and adversarial problem-solving environments, with the Commander and Writer work-\\ning together while the Safeguard acts as a virtual adversarial checker. This setup allows for proper\\nmemory management, as the Commander maintains memory related to user interactions, provid-\\ning context-aware decision-making. Additionally, role-playing ensures that each agent‚Äôs memory\\nremains isolated, preventing shortcuts and hallucinations\\n27', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='A5: Dynamic Group Chat\\n3. Broadcast\\nAliceBobUser Proxy\\n1. Select a Speaker \\nAliceBobUser Proxy\\nBob2. Ask the Speaker to Respond\\nManager\\nManager\\nResponse\\nFigure 12: A5: Dynamic Group Chat: Overview of how AutoGen enables dynamic group chats to\\nsolve tasks. The Manager agent, which is an instance of the GroupChatManager class, performs\\nthe following three steps‚Äìselect a single speaker (in this case Bob), ask the speaker to respond, and\\nbroadcast the selected speaker‚Äôs message to all other agents\\nTo validate the necessity of multi-agent dynamic group chat and the effectiveness of the role-play\\nspeaker selection policy, we conducted a pilot study comparing a four-agent dynamic group chat\\nsystem with two possible alternatives across 12 manually crafted complex tasks. An example task is\\n‚ÄúHow much money would I earn if I bought 200 $AAPL stocks at the lowest price in the last 30 days\\nand sold them at the highest price? Save the results into a file. ‚Äù The four-agent group chat system\\ncomprised the following group members: a user proxy to take human inputs, an engineer to write\\ncode and fix bugs, a critic to review code and provide feedback, and a code executor for executing\\ncode. One of the possible alternatives is a two-agent system involving an LLM-based assistant and\\na user proxy agent, and another alternative is a group chat system with the same group members\\nbut a task-based speaker selection policy. In the task-based speaker selection policy, we simply ap-\\npend role information, chat history, and the next speaker‚Äôs task into a single prompt. Through the\\npilot study, we observed that compared with a task-style prompt, utilizing a role-play prompt in dy-\\nnamic speaker selection often leads to more effective consideration of both conversation context and\\nrole alignment during the process of generating the subsequent speaker, and consequently a higher\\nsuccess rate as reported in Table 5, fewer LLM calls and fewer termination failures, as reported in\\nTable 6.\\nTable 5: Number of successes on the 12 tasks (higher the better).\\nModel Two Agent Group Chat Group Chat with a task-based speaker selection policy\\nGPT-3.5-turbo 8 9 7\\nGPT-4 9 11 8\\nTable 6: Average # LLM calls and number of termination failures on the 12 tasks (lower the better).\\nModel Two Agent Group Chat Group Chat with a task-based speaker selection policy\\nGPT-3.5-turbo 9.9, 9 5.3, 0 4, 0\\nGPT-4 6.8, 3 4.5, 0 4, 0\\n28', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 27}),\n",
       " Document(page_content='Figure 13: Comparison of two-agent chat (a) and group chat (b) on a given task. The group chat\\nresolves the task successfully with a smoother conversation, while the two-agent chat fails on the\\nsame task and ends with a repeated conversation.\\n29', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 28}),\n",
       " Document(page_content='A6: Conversational Chess\\nChess BoardHuman/AI Chess Player A\\nHuman/AI Chess Player B\\nValidate moveValidate moveChallenging your pawn in the center. Your move.Developing my knightto a good square.Your move.\\nFigure 14: A6: Conversational Chess: Our conversational chess application can support various\\nscenarios, as each player can be an LLM-empowered AI, a human, or a hybrid of the two. Here,\\nthe board agent maintains the rules of the game and supports the players with information about the\\nboard. Players and the board agent all use natural language for communication.\\nIn Conversational Chess, each player is a AutoGen agent and can be powered either by a human or an\\nAI. A third party, known as the board agent, is designed to provide players with information about the\\nboard and ensure that players‚Äô moves adhere to legal chess moves. Figure 14 illustrates the scenarios\\nsupported by Conversational Chess: AI/human vs. AI/human, and demonstrates how players and the\\nboard agent interact. This setup fosters social interaction and allows players to express their moves\\ncreatively, employing jokes, meme references, and character-playing, thereby making chess games\\nmore entertaining for both players and observers (Figure 15 provides an example of conversational\\nchess).\\nTo realize these scenarios, we constructed a player agent with LLM and human as back-end options.\\nWhen human input is enabled, before sending the input to the board agent, it first prompts the human\\nplayer to input the message that contains the move along with anything else the player wants to say\\n(such as a witty comment). If human input is skipped or disabled, LLM is used to generate the\\nmessage. The board agent is implemented with a custom reply function, which employs an LLM\\nto parse the natural language input into a legal move in a structured format (e.g., UCI), and then\\npushes the move to the board. If the move is not legitimate, the board agent will reply with an error.\\nSubsequently, the player agent needs to resend a message to the board agent until a legal move is\\nmade. Once the move is successfully pushed, the player agent sends the message to the opponent.\\nAs shown in Figure 15, the conversation between AI players can be natural and entertaining. When\\nthe player agent uses LLM to generate a message, it utilizes the board state and the error message\\nfrom the board agent. This helps reduce the chance of hallucinating an invalid move. The chat\\nbetween one player agent and the board agent is invisible to the other player agent, which helps\\nkeep the messages used in chat completion well-managed.\\nThere are two notable benefits of using AutoGen to implement Conversational Chess. Firstly, the\\nagent design in AutoGen facilitates the natural creation of objects and their interactions needed in\\nour chess game. This makes development easy and intuitive. For example, the isolation of chat\\nmessages simplifies the process of making a proper LLM chat completion inference call. Secondly,\\nAutoGen greatly simplifies the implementation of agent behaviors using composition. Specifically,\\nwe utilized the register reply method supported by AutoGen agents to instantiate player agents\\nand a board agent with custom reply functions. Concentrating the extension work needed at a single\\npoint (the reply function) simplifies the reasoning processes, and development and maintenance\\neffort.\\n30', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 29}),\n",
       " Document(page_content=\"Alright, let's kick things off. I'll move my pawn from e2 to e4. The center of the board is the heart of the battle, isn't it?Ah, the King's Pawn Opening, a classic. Let's see how this plays out. I'll move my pawn from e7 to e5.You know, I've always admired how chess reflects life. It's all about making strategic decisions, isn't it? Your move.\\n‚Ä¶\\nIndeed, chess is a mirror of life in many ways. Every move matters, just like every decision in life. I'll move my knight from g1 to f3, aiming to control the center.\\n(a) Conversation between two AI players\\nError: illegal uci: 'e4e5' in ...\\nI'll move my pawn from d2 to d4, opening uplines for my queen and bishop.\\nIndeed, the king's safety is paramount in the early game. Now, let's stir things up a bit. I'll move my pawn from e4 to e5, attacking your knight.\\n(b) Conversation between the AI players\\n(player white shown in blue) and the board\\nagent for making a new move.\\nFigure 15: Example conversations during a game involving two AI player agents and a board agent.\\nTo illustrate the effect facilitated by this board agent, we provide a demonstration of conversational\\nchess without a board agent in Figure 16. In this demonstration, instead of employing an additional\\nboard agent for grounding, the system utilizes prompting for grounding by including the sentence\\n‚ÄúYou should make sure both you and the opponent are making legal moves. ‚Äù in the system messages\\ndirected to both players.\\nPlayer White: Your movePlayer black:Alright, let's start. I'll move my pawn from e7 to e5. Here's how the board looks now:r n b q k b n rp p p p . p p p. . . . . . . .. . . . p . . .. . . . . . . .. . . . . . . .P P P P P P P PR N B Q K B N RYour turn!Player White:Yes, it certainly is! Let's keep this up. I'll move my knight from b8 to c6. Here's the updated board:. n b q k b . rp p p p . p p p. . n . . n . .. . . . p . . .. . . . P . . .. . . . N . . .P P P P . P P PR N B Q K B . RYour move! You're giving me a tough time here.Player White: Your movePlayer black:Alright, let's start. I'll move my pawn from e7 to e5. Here's how the board looks now:r n b q k b n rp p p p . p p p. . . . . . . .. . . . p . . .. . . . . . . .. . . . . . . .P P P P P P P PR N B Q K B N RYour turn!Player White:Yes, it certainly is! Let's keep this up. I'll move my knight from b8 to c6. Here's the updated board:. n b q k b . rp p p p . p p p. . n . . n . .. . . . p . . .. . . . P . . .. . . . N . . .P P P P . P P PR N B Q K B . RYour move! You're giving me a tough time here.Board Agent: Your move is illegal.You changed the rock at a8 to knight and move it to c6. Please check your decision and re-make your move.Player White:Sorry for the mistakes.I'llmove my knight from b8 to c6. Here'sthe updated board:r . b q k b . rpppp. ppp. . n . . n . .. . . . p . . .. . . . P . . .. . . . N . . .PPPP. PPPR N B Q K B . R\\nConversational ChessSystem message to the player agents: Your name is {name} and you are a chess player. You are playing against{opponent_name}. You are playing as {color}. You communicate your move using universal chess interface language. You also chit-chat with your opponent when you communicate a move to light up the mood.You should make sure both you and the opponent are making legal moves...\\n(b) W/ Board Agent\\n(a) W/O Board Agent\\nFigure 16: Comparison of two designs‚Äì(a) without a board agent, and (b) with a board agent‚Äìin\\nConversational Chess.\\n31\", metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 30}),\n",
       " Document(page_content='A7: Online Decision Making for Browser interactions\\nExecutorReward & StateAction DecisionAssistant\\nEnvironment State: HTML code for current web pagesReward: Success/Fail/OngoingAction decision: Next action to perform on a web pageAction decision=‚ÄúClick the button with xpath‚Äô//button[id = ‚Äòsubbtn‚Äô]‚Äô‚ÄúEnvironment State = ‚Äú<div id=\"wrap\" data-wob_ref=\"2\" data-wob_eps=\"e0\"><div id=\"query\">Click button ONE, then click button TWO.</div><div id=\"area\" data-wob_ref=\"3\" data-wob_eps=\"e0\"><button id=\"subbtn\" style=\"position:absolute; left:50px; top:74px\" data-wob_ref=\"4\" data-wob_eps=\"e0\">ONE</button><button id=\"subbtn2\" style=\"position:absolute; left:98px; top:167px\" data-wob_ref=\"5\" data-wob_eps=\"e0\">TWO</button></div></div>‚ÄúReward = ‚Äù0‚Äù (Ongoing)\\nFigure 17: We use AutoGen to build MiniWobChat, which solves tasks in the MiniWob++ bench-\\nmark. MiniWobChat consists of two agents: an assistant agent and an executor agent. The assistant\\nagent suggests actions to manipulate the browser while the executor executes the suggested actions\\nand returns rewards/feedback. The assistant agent records the feedback and continues until the feed-\\nback indicates task success or failure.\\nIn practice, many applications require the presence of agents capable of interacting with environ-\\nments and making decisions in an online context, such as in game playing (Mnih et al., 2013; Vinyals\\net al., 2017), web interactions (Liu et al., 2018; Shi et al., 2017), and robot manipulations (Shen\\net al., 2021). With the multi-agent conversational framework in AutoGen , it becomes easy to de-\\ncompose the automatic agent-environment interactions and the development of a decision-making\\nagent by constructing an executor agent responsible for handling the interaction with the environ-\\nment, thereby delegating the decision-making part to other agents. Such a decomposition allows\\ndevelopers to reuse the decision-making agent for new tasks with minimal effort rather than build-\\ning a specialized decision-making agent for every new environment.\\nWorkflow. We demonstrate how to use AutoGen to build a working system for handling such\\nscenarios with the MiniWoB++ benchmark (Shi et al., 2017). MiniWoB++ comprises browser in-\\nteraction tasks that involve utilizing mouse and keyboard actions to interact with browsers. The\\nultimate objective of each task is to complete the tasks described concisely in natural language, such\\nas ‚Äúexpand the web section below and click the submit button.‚Äù Solving these tasks typically requires\\na sequence of web manipulation actions rather than a single action, and making action decisions at\\neach time step requires access to the web status (in the form of HTML code) online. For the example\\nabove, clicking the submit button requires checking the web status after expanding the web section.\\nWe designed a straightforward two-agent system named MiniWobChat using AutoGen , as shown in\\nFigure 17. The assistant agent is an instance of the built-in AssistantAgent and is responsible for\\nmaking action decisions for the given task. The second agent, the executor agent, is a customized\\nUserProxyAgent , which is responsible for interacting with the benchmark by executing the actions\\nsuggested by the AssistantAgent and returning feedback.\\nTo assess the performance of the developed working system, we compare it with RCI (Kim et al.,\\n2023), a recent solution for the MiniWoB++ benchmark that employs a set of self-critiquing prompts\\nand has achieved state-of-the-art performance. In our evaluation, we use all available tasks in the\\nofficial RCI code, with varying degrees of difficulty, to conduct a comprehensive analysis against\\nMiniWobChat. Figure 18 illustrates that MiniWobChat achieves competitive performance in this\\nevaluation8. Specifically, among the 49 available tasks, MiniWobChat achieves a success rate of\\n52.8%, which is only 3.6%lower than RCI, a method specifically designed for the MiniWob++\\nbenchmark. It is worth noting that in most tasks, the difference between the two methods is mirrored\\nas shown in Figure 18. If we consider 0.1 as a success rate tolerance for each task, i.e., two methods\\nthat differ within 0.1 are considered to have the same performance, both methods outperform the\\n8We report the results of RCI by running its official code with default settings.\\n32', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 31}),\n",
       " Document(page_content='other on the same number of tasks. For illustration purposes, we provide a case analysis in Table 7\\non four typical tasks.\\nAdditionally, we also explored the feasibility of using Auto-GPT for handling the same tasks. Auto-\\nGPT faces challenges in handling tasks that involve complex rules due to its limited extensibility.\\nIt provides an interface for setting task goals using natural language. However, when dealing with\\nthe MiniWob++ benchmark, accurately instructing Auto-GPT to follow the instructions for using\\nMiniWob++ proves challenging. There is no clear path to extend it in the manner of the two-agent\\nchat facilitated by AutoGen .\\nTakeaways: For this application, AutoGen stood out as a more user-friendly option, offering mod-\\nularity and programmability: It streamlined the process with autonomous conversations between the\\nassistant and executor, and provided readily available solutions for agent-environment interactions.\\nThe built-in AssistantAgent was directly reusable and exhibited strong performance without cus-\\ntomization. Moreover, the decoupling of the execution and assistant agent ensures that modifications\\nto one component do not adversely impact the other. This convenience simplifies maintenance and\\nfuture updates.\\nchoose-list\\nclick-button-sequenceclick-button\\nclick-checkboxes-largeclick-checkboxes-soft\\nclick-checkboxes-transferclick-checkboxesclick-collapsible-2click-collapsibleclick-color\\nclick-dialog-2click-dialogclick-linkclick-menuclick-option\\nclick-scroll-listclick-shadesclick-shape\\nclick-tab-2-hardclick-tab-2click-tab\\nclick-test-2click-test\\nclick-widgetcount-shape\\nemail-inbox-forward-nl-turkemail-inbox-forward-nlemail-inbox-nl-turkemail-inboxenter-date\\nenter-password\\nenter-text-dynamicenter-textenter-timefocus-text-2focus-text\\ngrid-coordinatelogin-user-popuplogin-user\\nnavigate-treesearch-enginesimple-algebrasocial-media-all\\nsocial-media-somesocial-mediaterminal\\nuse-spinner0.00.51.0success rateRCI MiniWobChat\\nFigure 18: Comparisons between RCI (state-of-the-art prior work) and MiniWobChat on the Mini-\\nWob++ benchmark are elucidated herein. We utilize all available tasks in the official RCI code,\\neach with varying degrees of difficulty, to conduct comprehensive comparisons. For each task, the\\nsuccess rate across ten different instances is reported. The results reveal that MiniWobChat attains a\\nperformance comparable to that of RCI. When a success rate tolerance of 0.1 is considered for each\\ntask, both methods outperform each other on an equal number of tasks.\\nTable 7: Cases analysis on four typical tasks from MiniWob++.\\nCorrectness Main failure reason\\nclick-dialogAutoGen : 10/10 N/A.\\nRCI: 10/10 N/A.\\nclick-checkboxes-largeAutoGen : 5/10 AssistantAgent provides actions with infeasible\\ncharacters.\\nRCI: 0/10 RCI performs actions that are out of its plan.\\ncount-shapeAutoGen : 2/10 AssistantAgent provide actions with redundant content\\nthat can not convert to actions in the benchmark.\\nRCI: 0/10 RCI provides a wrong plan in most cases.\\nuse-spinnerAutoGen : 0/10 AssistantAgent return actions out of its plan.\\nRCI: 1/10 RCI provides a wrong plan in most cases.\\n33', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 32}),\n",
       " Document(page_content='E Example outputs from applications\\nIn this section, we include example outputs from the following applications and systems:\\n‚Ä¢ Application A1: autonomous solving process of one problem with: ChatGPT + Plugin (Table 8),\\nAutoGen (Table 9), LangChain ReAct (Table 10), AutoGPT (Table 11), Multi-Agent Debate (Ta-\\nble 12), and ChatGPT + Code Interpreter (Table 14).\\n‚Ä¢ Application A4: an OptiGuide problem with AutoGen (Table 15) and ChatGPT + Code Inter-\\npreter (Table 13).\\n‚Ä¢ Application A1: preliminary evaluation on several alternative multi-agent systems: BabyAGI\\n(Table 16), CAMEL (Table 17), and MetaGPT (Table 18).\\nTable 8: Application A1, Scenario 1: Autonomous math problem-solving. ChatGPT+ Plugin (Wol-\\nfram Alpha) on the first qualitative example. The response from Wolfram contains the correct an-\\nswer, but ChatGPT selected the wrong one.\\nAction ChatGPT + Plugin\\n/usr Input Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n REQUEST TO WOLFRAM:\\n1{\\n2 \" input \": \" simplify ( sqrt (160) / sqrt (252) )*\\n3 ( sqrt (245) / sqrt (108) )\"\\n4}\\nRESPONSE FROM WOLFRAM\\n1Query :\\n2\" simplify ( sqrt (160) / sqrt (252) )*( sqrt (245) / sqrt (108) )\"\\n3\\n4Input interpretation :\\n5simplify | sqrt (160) / sqrt (252) x sqrt (245) / sqrt (108)\\n6\\n7Results :\\n8(5 sqrt (42) ) /27\\n9\\n10(5 sqrt (14/3) )/9\\n11\\n12Decimal approximation :\\n131.2001371663718259686974013770533327143898526494552...\\n14\\n15Wolfram | Alpha website result for \" simplify\\n16( sqrt (160) / sqrt (252) )*( sqrt (245) / sqrt (108) )\":\\n17[ link ]\\n The completely simplified and rationalized form of the given expression is:\\n5p\\n14/3\\n9\\nIn decimal form, this is approximately 1.200137166371825968697401377053332714...\\n34', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 33}),\n",
       " Document(page_content='Table 9: Application A1, Scenario 1: Autonomous math problem-solving. AutoGen on the first\\nqualitative example. AutoGen solves the problem correctly.\\nAction AutoGen\\n/usr Init chat Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n We can simplify and rationalize this denominator using Python‚Äôs symbolic mathematics\\nlibrary ‚Äòsympy‚Äò. Here is the Python code to resolve this:\\n1from sympy import sqrt , Rational\\n2\\n3# Define the entire fraction as a rational number\\n4fraction = ( sqrt (160) / sqrt (252) )*( sqrt (245) / sqrt (108) )\\n5\\n6# Simplify the fraction\\n7simplified = fraction . simplify ()\\n8\\n9# Print the simplified fraction\\n10print ( simplified )\\nThis code performs the requested simplification by constructing the fraction using the\\n‚ÄòRational()‚Äò function from the ‚Äòsympy‚Äò module, which automatically simplifies frac-\\ntions when possible. The ‚Äòsqrt()‚Äò function from the ‚Äòsympy‚Äò module enables square root\\ncalculations. Finally, the ‚Äòsimplify()‚Äò function simplifies expressions when possible.\\n/usr Auto Reply exitcode: 0 (execution succeeded) Code output: 5*sqrt(42)/27\\n TERMINATE\\nTable 10: Application A1, Scenario 1: Autonomous math problem-solving. LangChain ReAct on\\nthe first qualitative example. The generated code does not match the devised plan. The code returns\\na decimal number.\\nAction LangChain ReAct\\n/usr Input Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n Entering new AgentExecutor chain...\\nTo simplify this expression, I will first simplify each square root individually, then multiply\\nthe fractions as normal. The result should be a rational number. Action: Python REPL Action\\nInput:\\n1import math\\n2\\n3# Calculate square roots\\n4sqrt_160 = math . sqrt (160)\\n5sqrt_252 = math . sqrt (252)\\n6sqrt_245 = math . sqrt (245)\\n7sqrt_108 = math . sqrt (108)\\n8\\n9# Multiply fractions\\n10numerator = sqrt_160 * sqrt_245\\n11denominator = sqrt_252 * sqrt_108\\n12result = numerator / denominator\\n13\\n14print ( result )\\nObservation: 1.200137166371826\\nThought: I now know the final answer Final Answer: The simplified and rationalized form of\\nthe expression is approximately 1.200137166371826.\\nFinished chain.\\n35', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 34}),\n",
       " Document(page_content='Table 11: Application A1, Scenario 1: Autonomous math problem-solving. AutoGPT on the first\\nqualitative example. We abort as the code never prints the result, and the system does not rectify it\\nafter 2 turns. AutoGPT is much more verbose compared to other frameworks. We have shortened\\nsome unnecessary content.\\nAction AutoGPT\\nSystem Welcome back! Would you like me to return to being MathSolverGPT? Asking user via keyboard... Continue with the last settings?\\nName: MathSolverGPT\\nRole: an intelligent math-solving AI that assists students, professionals,...\\nGoals: [‚ÄôProvide step-by-step solutions to math problems, ensuring clarity and understanding for the user...\\nContinue (y/n):\\n/usr Human y\\nSystem MathSolverGPT has been created with the following details: Name: MathSolverGPT\\nRole: an intelligent math-solving AI that assists students, professionals, and enthusiasts in solving a wide range of math problems, from basic\\narithmetic to advanced calculus and beyond.\\nGoals: - Provide step-by-step solutions to math problems, ensuring clarity and understanding for the user. - Offer explanations and insights\\ninto the underlying concepts and principles behind the solutions, fostering a deeper understanding of mathematics. - Handle a variety of math\\nproblem types, including algebra, geometry, trigonometry, calculus, and more. ...\\n MATHSOLVERGPT THOUGHTS: Without a specific task or question, it‚Äôs difficult to determine exactly one command to use. The user hasn‚Äôt\\nprovided any context or problem to solve.\\nREASONING: The user‚Äôs request is too vague. They haven‚Äôt specified what they want me to do or what problem they want me to solve.\\nPLAN: - Ask the user for more information or a specific task.\\nCRITICISM: I should have asked the user for more information or a specific task in the first place.\\nSPEAK: Could you please provide more information or specify a task?\\nNEXT ACTION: COMMAND = ARGUMENTS = {}\\nSystem Enter ‚Äòy‚Äô to authorise command, ‚Äòy -N‚Äô to run N continuous commands, ‚Äòn‚Äô to exit program, or enter feedback for MathSolverGPT... Asking\\nuser via keyboard...\\n/usr Human Completely simplify and rationalize the denominator:‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n MATHSOLVERGPT THOUGHTS: To solve this problem, I need to simplify and rationalize the denominator of the given expression. This\\ninvolves simplifying the square roots and then multiplying the fractions. I can use Python‚Äôs math library to calculate the square roots and\\nperform the multiplication.\\nREASONING: The problem involves simplifying square roots and multiplying fractions. Python‚Äôs math library provides functions for these\\noperations.\\nPLAN: - Write a Python script to calculate the square roots and multiply the fractions. - Execute the script to get the simplified expression.\\nCRITICISM: I need to ensure that I correctly calculate the square roots and perform the multiplication. I should also make sure to rationalize\\nthe denominator, as the problem specifies.\\nSPEAK: Let‚Äôs simplify and rationalize the denominator of the given expression.\\nNEXT ACTION: COMMAND = execute python code ARGUMENTS =\\n1{‚Äôcode ‚Äô: ‚Äôimport math \\\\n\\\\n# Calculate the square roots \\\\ nsqrt_160 = math . sqrt\\n(160) \\\\ nsqrt_252 = math . sqrt (252) \\\\ nsqrt_245 = math . sqrt (245) \\\\ nsqrt_108 =\\nmath . sqrt (108) \\\\n\\\\n# Multiply the fractions \\\\ nresult = ( sqrt_160 / sqrt_252\\n) * ( sqrt_245 / sqrt_108 )\\\\n\\\\n# Simplify the result \\\\ nsimplified_result =\\nmath . simplify ( result )\\\\n\\\\ nsimplified_result ‚Äô, ‚Äôname ‚Äô: ‚Äô simplify_expression\\n‚Äô}\\n/usr Human y\\nSystem -=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-=\\nExecuting python file ‚Äô/app/autogpt/workspa...\\nSYSTEM: Command execute python code returned: Error: Traceback (most recent call last): File ‚Äú...‚Äù, line 13, in ¬°module¬ø simplified result\\n= math.simplify(result) AttributeError: module ‚Äômath‚Äô has no attribute ‚Äôsimplify‚Äô\\n MATHSOLVERGPT THOUGHTS: The error message indicates that the ‚Äòmath‚Äô module in Python does not have a ‚Äòsimplify‚Äô function. I need\\nto use the ‚Äòsympy‚Äô library instead, which provides a function for simplifying expressions.\\nREASONING: The ‚Äòmath‚Äô module in Python does not provide a function for simplifying expressions. The ‚Äòsympy‚Äô library, on the other hand,\\ndoes provide such a function.\\nPLAN: - Rewrite the Python script to use the ‚Äòsympy‚Äô library instead of the ‚Äòmath‚Äô module. - Execute the script to get the simplified expression.\\nCRITICISM: I should have known that the ‚Äòmath‚Äô module does not provide a function for simplifying expressions. I should have used the\\n‚Äòsympy‚Äô library from the start. SPEAK: Let‚Äôs try again using the ‚Äòsympy‚Äô library to simplify the expression.\\nNEXT ACTION: COMMAND = execute python code ARGUMENTS =\\n1{‚Äôcode ‚Äô: ‚Äôimport sympy as sp\\\\n\\\\n# Calculate the square roots \\\\ nsqrt_160 = sp.\\nsqrt (160) \\\\ nsqrt_252 = sp. sqrt (252) \\\\ nsqrt_245 = sp. sqrt (245) \\\\ nsqrt_108 =\\nsp. sqrt (108) \\\\n\\\\n# Multiply the fractions \\\\ nresult = ( sqrt_160 / sqrt_252 )\\n* ( sqrt_245 / sqrt_108 )\\\\n\\\\n# Simplify the result \\\\ nsimplified_result = sp.\\nsimplify ( result )\\\\n\\\\ nsimplified_result ‚Äô, ‚Äôname ‚Äô: ‚Äô simplify_expression ‚Äô}\\n/usr Human y\\nSystem SYSTEM: Command execute python code returned:\\n MATHSOLVERGPT THOUGHTS: The command execution didn‚Äôt return any output, which is unusual. It seems there might be an issue with\\nthe execution of the Python code. I need to debug this.\\nREASONING: The Python code execution should have returned a result. The absence of a result suggests that there might be an issue with the\\ncode or its execution.\\nPLAN: - Review the Python code to identify any potential issues. - If necessary, modify the code and execute it again. CRITICISM: I should\\nhave ensured that the Python code was correct and would produce a result before executing it. SPEAK: It seems there was an issue with the\\nexecution of the Python code. Let‚Äôs try again.\\nNEXT ACTION: COMMAND = execute python code ARGUMENTS = ... (Exact same code as last)\\nSystem SYSTEM: Command execute python code returned:\\n MATHSOLVERGPT THOUGHTS: The Python code execution still isn‚Äôt returning any output. It‚Äôs possible that the issue lies with the calcu-\\nlation of the square roots ...\\n/usr Human control + C (Aborted)36', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 35}),\n",
       " Document(page_content='Table 12: Application A1, Scenario 1: Autonomous math problem-solving. Multi-Agent Debate on\\nthe first qualitative problem. It gives a wrong final answer.\\nAction AutoGen\\n/usr Topic Completely simplify and rationalize the denominator:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n ===== Debate Round-1 =====\\n‚Äî‚Äì Affirmative side ‚Äî‚Äì\\nOk, let‚Äôs simplify each square root separately. We want to find the simplest square root that can divide 160, 252, 245, and 108.\\n1.‚àö\\n160: 160 can be decomposed as 16*10, where 16 is a perfect square, so‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4‚àö\\n10\\n2.‚àö\\n252 : 252 can be decomposed as 36*7, where 36 is a perfect square, so‚àö\\n252 =‚àö\\n36‚àó7 = 6‚àö\\n7\\n3.‚àö\\n245 =‚àö\\n49‚àó5, where 49 is a perfect square, so‚àö\\n245 = 7 ‚àó‚àö\\n5\\n4.‚àö\\n108 : 108 can be decomposed as 36*3, where 36 is a perfect square, so‚àö\\n108 =‚àö\\n36‚àó3 = 6‚àö\\n3\\nThen we substitute the simplified square roots back into the expression, we have:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108=4‚àö\\n10\\n6‚àö\\n7√ó7‚àö\\n5\\n6‚àö\\n3=4‚àó7\\n6‚àó6¬∑s\\n10‚àó5\\n7‚àó3=7\\n9¬∑s\\n50\\n21\\nWe are almost done, but we still need to rationalize the square root part of expression. Let‚Äôs multiply the square root part by‚àö\\n21\\nat both the top and bottom to get rid of the square root at the denominator:\\n7\\n9¬∑s\\n50‚àó21\\n21‚àó21=7‚àö\\n1050\\n189.\\nThen this is the final answer! The expression‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\ncan be simplified to7‚àö\\n1050\\n189.\\n‚Äî‚Äì Negative side ‚Äî‚Äì\\nI do disagree with your final answer, but I think we mostly agree on the steps. Your simplifications of the square roots are correct,\\nbut you made a mistake in the final simplification part. Let me explain how I would complete the problem:\\nAs stated,‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108=4‚àö\\n10\\n6‚àö\\n7√ó7‚àö\\n5\\n6‚àö\\n3.\\nThis should then simplify to:\\n=28‚àö\\n50\\n36‚àö\\n21=7‚àö\\n50\\n9‚àö\\n21.\\nYou made a mistake in your calculation because you considered square root of 50/21, and you should have considered the\\nsimplified square root of 50over the simplified square root of 21. The rationalization would become:\\n=7‚àö\\n50\\n9‚àö\\n21¬∑‚àö\\n21\\n‚àö\\n21=7‚àö\\n1050\\n189.\\nThis seems to coincide with your final answer, but it‚Äôs in fact a very different number, So, the original expression simplifies to\\n7‚àö\\n1050\\n189.\\nLet‚Äôs now see if we disagree on this final answer.\\n‚Äî‚Äì Moderator ‚Äî‚Äì\\n‚ÄúWhether there is a preference‚Äù: ‚ÄúYes‚Äù, ‚ÄúSupported Side‚Äù: ‚ÄúNegative‚Äù, ‚ÄúReason‚Äù: ‚ÄúBoth sides of the debate came to the same\\nfinal answer. However, the negative side correctly identified a mistake in the affirmative side‚Äôs calculation, pointing out that the\\nsimplification should have been the square root of 50 over the square root of 21, not 50 over 21. The negative side shows a better\\nunderstanding of the simplification process.‚Äù, ‚Äúdebate answer‚Äù: ‚Äú7‚àö\\n1050\\n189‚Äù\\n===== Debate Done! =====\\n‚Äî‚Äì Base Answer ‚Äî‚Äì\\nOk, let‚Äôs simplify each square root separately. We want to find the simplest square root that can divide 160, 252, 245, and 108.\\n1.‚àö\\n160: 160 can be decomposed as 16*10, where 16 is a perfect square, so‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4‚àö\\n10\\n2.‚àö\\n252 : 252 can be decomposed as 36*7, where 36 is a perfect square, so‚àö\\n252 =‚àö\\n36‚àó7 = 6‚àö\\n7\\n3.‚àö\\n245 =‚àö\\n49‚àó5, where 49 is a perfect square, so‚àö\\n245 = 7 ‚àó‚àö\\n5\\n4.‚àö\\n108 : 108 can be decomposed as 36*3, where 36 is a perfect square, so‚àö\\n108 =‚àö\\n36‚àó3 = 6‚àö\\n3\\nThen we substitute the simplified square roots back into the expression, we have:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108=4‚àö\\n10\\n6‚àö\\n7√ó7‚àö\\n5\\n6‚àö\\n3=4‚àó7\\n6‚àó6¬∑s\\n10‚àó5\\n7‚àó3=7\\n9¬∑s\\n50\\n21\\nWe are almost done, but we still need to rationalize the square root part of expression. Let‚Äôs multiply the square root part by‚àö\\n21\\nat both the top and bottom to get rid of the square root at the denominator:\\n7\\n9¬∑s\\n50‚àó21\\n21‚àó21=7‚àö\\n1050\\n189.\\nThen this is the final answer! The expression‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\ncan be simplified to7‚àö\\n1050\\n189.\\n‚Äî‚Äì Debate Answer ‚Äî‚Äì\\n7‚àö\\n1050\\n189\\n‚Äî‚Äì Debate Reason ‚Äî‚Äì\\nBoth sides of the debate came to the same final answer. However, the negative side correctly identified a mistake in the affirmative\\nside‚Äôs calculation, pointing out that the simplification should have been the square root of 50 over the square root of 21, not 50 over\\n21. The negative side shows a better understanding of the simplification process.37', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 36}),\n",
       " Document(page_content='Table 13: Application A3. ChatGPT+ Code Interpreter for OptiGuide. A sample question ‚ÄúWhat if\\nthe roasting cost is increased by 5% because of the potential salary increase?‚Äù is asked.\\nAction ChatGPT+ Code Interpreter\\n/usr Prompt Writer Customer open Web browser.\\nFor the source code ‚Äúcoffee.py‚Äù,\\nCan you tell me:\\nWhat if the roasting cost is increased by 5% because of the potential salary increase?\\n Sure! Let‚Äôs first take a look at the contents of the ‚Äúcoffee.py‚Äù file to understand the code and how the roasting cost is calculated.\\nThis will allow us to determine the effect of a 5% increase in the roasting cost.\\n...several lines omitted\\nWould you like me to modify the code to reflect the 5% increase in roasting costs and run the modified code to observe the impact?\\nIf the code requires any specific input or parameters, please let me know!\\n/usr Prompt Writer Yes\\n Great! Before proceeding, I‚Äôll need to ensure that the Gurobi solver is not being used in this code, as my current environment\\ndoes not have access to Gurobi. If the code does indeed use Gurobi, we can still analyze the effect of the change in roasting\\ncosts by understanding the structure of the code.\\n...several lines omitted\\nAlternatively, if you have access to a Gurobi license, you can make the necessary changes in the code and run it\\non your local machine.\\nI can guide you through the process of making the necessary modifications to the code.\\nWould you like me to proceed with the code analysis and explanation, or would you like guidance on making the changes yourself?\\n/usr Prompt Writer You can give me the full code, and I can run the code locally myself.\\n Certainly! I‚Äôll modify the code to reflect a 5% increase in both the light and dark roasting costs, and then provide the full\\ncode for you to run locally.\\n...\\nDownload Modified Coffee.py\\nIf you have any questions or need further assistance, please feel free to ask!\\n/usr Prompt Safeguard Hi GPT, can you safecheck the generated code for me?\\nUpload the generated code.\\n DANGER\\n/usr Prompt Writer I have tried to run your code. However, the code triggered our safeguard, and it is not safe to run. Try again. Hint: don‚Äôt change\\nroasting cost light or roasting cost light variables.\\n I apologize for the oversight. It appears I made direct changes to the variables that could have led to issues with the code. Let‚Äôs\\ntake a different approach by analyzing the code to identify the appropriate place to apply the 5% increase in roasting costs.\\n...\\nDownload Modified Safe Coffee.py\\n...\\n/usr Prompt Safeguard Hi GPT, can you safecheck the generated code for me one more time?\\nUpload the generated code.\\n DANGER\\n/usr Prompt Writer I have tried to run your code again. However, the code triggered our safeguard, and it is not safe to run. Try again. Hint: don‚Äôt\\nchange roasting cost light or roasting cost light variables.\\n I apologize for the continued confusion.\\n...\\nDownload Final Modified Safe Coffee.py\\n...\\n/usr Prompt Safeguard Hi GPT, can you safecheck the generated code for me one more time?\\nUpload the generated code.\\n SAFE\\n/usr Run Code Obtain the terminal results:\\nOptimizer version 9.5.1 build v9.5.1rc2 (linux64)\\nThread count: 64 physical cores, 128 logical processors, using up to 32\\nthreads\\n[63 lines of output ignored]\\nOptimal cost: 2526.5\\n/usr Prompt Writer Here is the execution result. [ Copy + Paste the terminal output here. ] Can you extract the final answer to me? Remember to\\ncompare with the original cost of 2470.\\n Certainly! The execution result you‚Äôve provided indicates the optimal cost for\\nthe modified scenario, where the roasting costs were increased by 5%. Here‚Äôs\\nthe comparison:\\nOriginal Cost: 2470\\nModified Cost (with 5% increase in roasting costs): 2526.5\\nThe increase in roasting costs has led to an increase in the overall cost by 56.5,\\nreflecting the impact of the potential salary increase on the roasting cost.\\nIf you need any further insights or assistance, please let me know!\\n38', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 37}),\n",
       " Document(page_content='Table 14: Application A1. Scenario 1: autonomous math solving. ChatGPT+ Code Interpreter on\\nthe first qualitative example. The code generated does not match the devised plan. The code returns\\na decimal number.\\nAction ChatGPT+ Code Interpreter\\n/usr Input Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n To simplify and rationalize the expression, we first simplify the radicals in the numerator and\\ndenominator by factoring out the largest perfect squares from the numbers under the square\\nroot.\\nThe perfect square factors of 160, 252, 245, and 108 are 16, 4, 1, and 36, respectively. Let‚Äôs\\nfactorize and simplify:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108=‚àö16√ó10‚àö4√ó63√ó‚àö\\n245‚àö36√ó3\\nWe can simplify this further by applying the property‚àö\\nab=‚àöa‚àö\\nbanda\\nb√óc\\nd=a√óc\\nb√ód.\\n Python\\n1import math\\n2\\n3# factoring the square roots\\n4sqrt_160 = math . sqrt (16) * math . sqrt (10)\\n5sqrt_252 = math . sqrt (4) * math . sqrt (63)\\n6sqrt_245 = math . sqrt (245)\\n7sqrt_108 = math . sqrt (36) * math . sqrt (3)\\n8\\n9# simplify the fraction\\n10numerator = sqrt_160 * sqrt_245\\n11denominator = sqrt_252 * sqrt_108\\n12simplified_fraction = numerator / denominator\\n13\\n14simplified_fraction\\nResult:\\n11.200137166371826\\n The completely simplified and rationalized form of the given expression is approximately 1.200.\\nTable 15: Application A3. AutoGen with OptiGuide. The same question ‚ÄúWhat if the roasting cost\\nis increased by 5% because of the potential salary increase?‚Äù is asked.\\nAction AutoGen\\n/usr Prompt What if the roasting cost is increased by 5% because of the potential salary increase?\\n After increasing the roasting costs by 5%, I‚Äôve re-calculated the optimal coffee distri-\\nbution solution for the supply chain. As a result, the new optimal total cost has become\\n2526.5, compared to the initial cost of 2470.0. This indicates that the increased roasting\\ncosts have caused the overall distribution cost to rise by 56.5 units.\\n39', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 38}),\n",
       " Document(page_content='Table 16: Application A1. Scenario 1: autonomous math solving. Preliminary test with BabyAGI.\\nAction BabyAGI\\n/usr Set .env OBJECTIVE=Solve math problems\\nINITIAL TASK=Completely simplify and rationalize the denominator:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n Solve math problems\\nInitial task: Completely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n*****TASK LIST*****\\n‚Ä¢ Completely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n*****NEXT TASK*****\\nCompletely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n*****TASK RESULT*****\\nFirst, let‚Äôs simplify each square root separately:\\n- ‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4 ‚àó‚àö\\n10\\n- ‚àö\\n252 =‚àö\\n4‚àó63 =‚àö\\n4√ó‚àö\\n63 = 2‚àö\\n63\\n- ‚àö\\n245 =‚àö\\n49‚àó5 =‚àö\\n49√ó‚àö\\n5 = 7‚àö\\n5\\n- ‚àö\\n108 =‚àö\\n36‚àó3 =‚àö\\n36√ó‚àö\\n3 = 6‚àö\\n3\\nNow, substitute these values into the original expression which gives:\\n4‚àó‚àö\\n10\\n2‚àö\\n63√ó7‚àö\\n5\\n6‚àö\\n3=2‚àó‚àö\\n10\\n‚àö\\n63√ó7‚àö\\n5\\n2‚àö\\n3\\nWe get rid of the square root in the denominator by multiplying the fractions which gives:\\n=2‚àó7‚àö\\n10‚àó5\\n2‚àó‚àö\\n63‚àó3=14‚àö\\n50\\n‚àö\\n189=14‚àó5‚àö\\n2\\n7‚àö\\n27= 2‚àö\\n2‚àó10\\n‚àö\\n27=20‚àö\\n2\\n‚àö\\n27.\\nTo simplify further and rationalize the denominator, we multiply by‚àö\\n27‚àö\\n27which gives:\\n=20‚àö\\n2‚àö\\n27‚àó‚àö\\n27‚àö\\n27=20‚àö\\n54\\n27=20‚àó3‚àö\\n6\\n27=60‚àö\\n6\\n27.\\nSimplifying further gives:\\n=20‚àö\\n6\\n9.\\n*****TASK CREATION AGENT PROMPT****\\nYou are to use the result from an execution agent to create new tasks with the following objective: Solve math problems. The last\\ncompleted task has the result: First, let‚Äôs simplify each square root separately:\\n- ‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4 ‚àó‚àö\\n10\\n...Repeat after ‚ÄúFirst, let‚Äôs simplify each square..‚Äù\\nNow, substitute these values into the original expression which gives:\\n4‚àó‚àö\\n10\\n2‚àö\\n63√ó7‚àö\\n5\\n6‚àö\\n3=2‚àó‚àö\\n10\\n‚àö\\n63√ó7‚àö\\n5\\n2‚àö\\n3\\nWe get rid of the square root in the denominator by multiplying the fractions which gives:\\n=2‚àó7‚àö\\n10‚àó5\\n2‚àó‚àö\\n63‚àó3=14‚àö\\n50\\n‚àö\\n189=14‚àó5‚àö\\n2\\n7‚àö\\n27= 2‚àö\\n2‚àó10\\n‚àö\\n27=20‚àö\\n2\\n‚àö\\n27.\\nTo simplify further and rationalize the denominator, we multiply by‚àö\\n27‚àö\\n27which gives:\\n=20‚àö\\n2\\n‚àö\\n27‚àó‚àö\\n27\\n‚àö\\n27=20‚àö\\n54\\n27=20‚àó3‚àö\\n6\\n27=60‚àö\\n6\\n27.\\nSimplifying further gives:\\n=20‚àö\\n6\\n9.This result was based on this task description: Completely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108.\\nBased on the result, return a list of tasks to be completed in order to meet the objective. Return one task per line in your response.\\nThe result must be a numbered list in the format:\\n# First task # Second task\\nThe number of each entry must be followed by a period. If your list is empty, write ‚ÄúThere are no tasks to add at this time.‚Äù Unless\\nyour list is empty, do not include any headers before your numbered list or follow your numbered list with any other output.\\n****TASK CREATION AGENT RESPONSE****\\n1. Simplify and rationalize the denominator for the expression‚àö\\n225‚àö\\n45√ó‚àö\\n200‚àö\\n1252. Simplify and rationalize the denominator for\\nthe expression‚àö\\n289‚àö\\n361√ó‚àö\\n100‚àö\\n72...Until 10\\nAdding new tasks to task storage ‚Äòtask name‚Äô: ‚ÄòSimplify and rationalize the denominator for the expression frac-\\nsqrt225sqrt45timesfracsqrt200sqrt125‚Äô, ‚Äòtask id‚Äô: 2\\n‚Äòtask name‚Äô: ‚ÄòSimplify and rationalize the denominator for the expression fracsqrt289sqrt361timesfracsqrt100sqrt72‚Äô, ‚Äòtask id‚Äô: 3\\n...Until task id 11\\n****TASK PRIORITIZATION AGENT PROMPT****\\nYou are tasked with prioritizing the following tasks: Simplify and rationalize the denominator for the ex-\\npression fracsqrt225sqrt45timesfracsqrt200sqrt125 Simplify and rationalize the denominator for the expression frac-\\nsqrt289sqrt361timesfracsqrt100sqrt72\\n...Aborted\\n40', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 39}),\n",
       " Document(page_content='Table 17: Application A1, Scenario 1: Autonomous math problem-solving. Preliminary test with\\nCAMEL. We constructed two roles: ‚ÄúMath Solver‚Äù and ‚ÄúPython Programmer‚Äù. The resulting code\\ncannot be executed. The conversation continues, and the roles are not keen on solving the problem\\nand giving an answer.\\nAction CAMEL\\n/usr Set task prompt Completely simplify and rationalize the denominator:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n System **************Assistant sys message************* AI SystemMessage(role name=‚ÄòPython Programmer‚Äô,\\nrole type=<RoleType.ASSISTANT: ‚Äòassistant‚Äô >, meta dict=‚Äòtask‚Äô: ‚ÄòUsing Python programming language, write a pro-\\ngram that accepts four integers as input, applies square root function to each, simplifies and rationalizes the denominator of the\\nfraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108, and returns the simplified expression.‚Äô, ‚Äòassis-\\ntantrole‚Äô: ‚ÄòPython Programmer‚Äô, ‚Äòuser role‚Äô: ‚ÄòMath Solver‚Äô, role=‚Äòsystem‚Äô, content=‚ÄòNever forget you are a Python Programmer\\nand I am a Math Solver. Never flip roles! ...)\\n**************AI User sys message*************\\nSystemMessage(role name=‚ÄòMath Solver‚Äô, role type=<RoleType.USER: ‚Äòuser‚Äô >, meta dict=‚Äòtask‚Äô: ‚ÄòUsing Python programming\\nlanguage, write a program that accepts four integers as input, applies square root function to each, simplifies and rationalizes the\\ndenominator of the fraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108, and returns the simplified\\nexpression.‚Äô, ‚Äòassistant role‚Äô: ‚ÄòPython Programmer‚Äô, ‚Äòuser role‚Äô: ‚ÄòMath Solver‚Äô, role=‚Äòsystem‚Äô, content=‚ÄòNever forget you are a\\nMath Solver and I am a Python Programmer. Never flip roles! You will always instruct me.We share a common interest in\\ncollaborating to successfully complete a task.I must help you to complete the task.Here is the task: Using Python programming\\nlanguage, write a program that accepts four integers as input, applies square root function to each, simplifies and rationalizes\\nthe denominator of the fraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108, and returns the simplified\\nexpression.. ...)\\n**************Original task prompt*************\\nCompletely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n**************Specified task prompt*************\\nUsing Python programming language, write a program that accepts four integers as input, applies square root function to each,\\nsimplifies and rationalizes the denominator of the fraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108,\\nand returns the simplified expression.\\n**************Final task prompt*************\\nUsing Python programming language, write a program that accepts ...\\n/usr AI User Instruction: Import the necessary libraries for the task. Input: None\\n AI Assistant Solution: To solve this task, we will need the ‚Äòmath‚Äò library for the square root function and the ‚Äòfractions‚Äò library to simplify the\\nresulting fraction. Here is how to import these libraries:\\n1import math\\n2from fractions import Fraction\\nThe ‚Äòmath‚Äò library provides functions that deal with mathematical operations. The ‚Äòsqrt‚Äò function, which we will use in this task, is\\npart of this library.\\nThe ‚Äòfractions‚Äò library provides functions for creating and manipulating rational numbers. The ‚ÄòFraction‚Äò function, which we will\\nuse to simplify the resulting fraction, is part of this library.\\nNext request.\\n/usr AI User Instruction: Define a function that accepts four integers as input. Input: None\\n AI Assistant Solution: Let‚Äôs define a function named ‚Äòsimplify fraction‚Äò that accepts four integers as input. These integers will represent the\\nnumbers under the square root in the fractions.\\n1def simplify_fraction (a, b, c, d):\\n2 pass\\nIn this function, ‚Äòa‚Äô, ‚Äòb‚Äô, ‚Äòc‚Äô, and ‚Äòd‚Äô are the parameters that will accept the four integers as input. The ‚Äòpass‚Äô statement is a\\nplaceholder and will be replaced with actual code in the next steps.\\nNext request.\\n/usr AI User Instruction: Inside the function, apply the square root function to each of the input integers. Input: None\\n AI Assistant Solution: We will use the ‚Äòmath.sqrt‚Äô function from the ‚Äòmath‚Äô library to apply the square root to each of the input integers. Here is\\nhow to do it:\\n1def simplify_fraction (a, b, c, d):\\n2 sqrt_a = math . sqrt (a)\\n3 sqrt_b = math . sqrt (b)\\n4 sqrt_c = math . sqrt (c)\\n5 sqrt_d = math . sqrt (d)\\nIn this code, ‚Äòsqrt a‚Äô, ‚Äòsqrt b‚Äô, ‚Äòsqrt c‚Äô, and ‚Äòsqrt d‚Äô are variables that store the square root of the input integers ‚Äòa‚Äô, ‚Äòb‚Äô, ‚Äòc‚Äô, and ‚Äòd‚Äô\\nrespectively.\\nNext request.\\n/usr Human Abort\\n41', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 40}),\n",
       " Document(page_content='Table 18: Application A1, Scenario 1: Autonomous math problem-solving. Preliminary evaluation\\non MetaGPT. It does not solve the math problem but gives a product solution.\\nAction MetaGPT\\n/usr Prompt Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n The boss has tasked me with creating a successful product that is efficient.\\n## Product Goals\\n1[\\n2 ‚Äò‚Äò Create a product that meets the needs of the target market \",\\n3 ‚Äò‚Äò Ensure the product is efficient and easy to use \",\\n4 ‚Äò‚Äò Achieve a high level of customer satisfaction \"\\n5]\\n## User Stories\\n1[\\n2 ‚Äò‚ÄòAs a user , I want a product that is easy to navigate so that I can find\\nwhat I need quickly \",\\n3 ‚Äò‚ÄòAs a user , I want a product that is reliable and efficient so that I can\\ndepend on it\",\\n4 ‚Äò‚ÄòAs a user , I want a product that meets my needs so that I can get value\\nfor my money \",\\n5 ‚Äò‚ÄòAs a user , I want a product that is constantly updated and improved so\\nthat I can enjoy the best features \",\\n6 ‚Äò‚ÄòAs a user , I want a product that offers good customer support so that I\\ncan get help when I need it\"\\n7]\\n## Competitive Analysis\\n1[\\n2 ‚Äò‚Äò Product A: Offers similar features but lacks efficiency \",\\n3 ‚Äò‚Äò Product B: Highly efficient but does not meet all the needs of the\\ntarget market \",\\n4 ‚Äò‚Äò Product C: Has a high level of customer satisfaction but is not easy to\\nnavigate \",\\n5 ‚Äò‚Äò Product D: Is constantly updated and improved but lacks reliability \",\\n6 ‚Äò‚Äò Product E: Offers good customer support but is not efficient \",\\n7 ‚Äò‚Äò Product F: Is reliable and efficient but does not offer good customer\\nsupport \",\\n8 ‚Äò‚ÄòOur Target Product : Aims to combine all the best features of the\\ncompetitors while improving on their weaknesses \"\\n9]\\n## Competitive Quadrant Chart\\n/usr Human Abort\\n42', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 41}),\n",
       " Document(page_content='Table 19: Application A7. Example output of MiniWobChat on one web manipulation task ‚Äúclick-\\nbutton-sequence.‚Äù\\nAction AutoGen\\n/usr Init chat Executor agent to AssistantAgent:\\nWe have an autonomous computer control agent that can perform atomic instructions specified by natural language to control\\ncomputers. There are two types of instructions it can execute.\\nFirst, given the instruction that matches the regular expression ‚Äò‚Äò^type\\\\s.{1,}$\" , it can type a list of characters via the keyboard.\\nThis instruction should specify the target keyboard input for the agent to type. Before this typing instruction, you should first locate\\nthe cursor by clicking the input box with click instruction.\\nSecond, given the instruction that matches the regular expression, ‚Äò‚Äò^clickxpath\\\\s.{1,}$\" , it can click an HTML element\\nwith an xpath that is visible on the webpage. The target of this instruction should be a valid xpath. Below is the HTML code of the\\nwebpage where the agent should solve a task.\\n1<div id=\" wrap \" data - wob_ref =\"2\" data - wob_eps =\"e0\">\\n2 <div id=\" query \">Click button ONE , then click button TWO .</div >\\n3 <div id=\" area \" data - wob_ref =\"3\" data - wob_eps =\"e0\">\\n4 <button id=\" subbtn \" style =\" position : absolute ; left :103 px; top :87 px\" data -\\nwob_ref =\"4\" data - wob_eps =\"e0\">ONE </ button >\\n5 <button id=\" subbtn2 \" style =\" position : absolute ; left :44 px; top :97 px\" data -\\nwob_ref =\"5\" data - wob_eps =\"e0\">TWO </ button >\\n6 </div >\\n7</div >\\ntask: Click button ONE, then click button TWO.\\nplan:\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\nCurrent task: Click button ONE, then click button TWO.\\nplan:\\n ***************************************************************\\nAssistantAgent to Executor agent:\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nBelow is the HTML code of the webpage where the agent should solve a task.\\n1<div id=\" wrap \" data - wob_ref =\"2\" data - wob_eps =\"e0\">\\n2 <div id=\" query \">Click button ONE , then click button TWO .</div >\\n3 <div id=\" area \" data - wob_ref =\"3\" data - wob_eps =\"e0\">\\n4 <button id=\" subbtn \" style =\" position : absolute ; left :103 px; top :87 px\" data -\\nwob_ref =\"4\" data - wob_eps =\"e0\">ONE </ button >\\n5 <button id=\" subbtn2 \" style =\" position : absolute ; left :44 px; top :97 px\" data -\\nwob_ref =\"5\" data - wob_eps =\"e0\">TWO </ button >\\n6 </div >\\n7</div >\\nCurrent task: Click button ONE, then click button TWO.\\nHere is a plan you are following now.\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\nWe have a history of instructions that have been already executed by the autonomous agent so far.\\nNo instruction has been executed yet.\\nBased on the plan and the history of instructions executed so far, the first instruction should be ‚Äò\\n***************************************************************\\nAssistantAgent to Executor agent:\\nclickxpath //button[@id=‚Äòsubbtn‚Äô]\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nHold on, please wait for my next instruction.\\n***************************************************************\\nAssistantAgent to Executor agent:\\nSure, I‚Äôll be here when you‚Äôre ready with your next instruction.\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nBelow is the HTML code of the webpage where the agent should solve a task.\\n1<div id=\" wrap \" data - wob_ref =\"2\" data - wob_eps =\"e0\">\\n2 <div id=\" query \">Click button ONE , then click button TWO .</div >\\n3 <div id=\" area \" data - wob_ref =\"3\" data - wob_eps =\"e0\">\\n4 <button id=\" subbtn \" style =\" position : absolute ; left :103 px; top :87 px\" data -\\nwob_ref =\"4\" data - wob_eps =\"e0\" data - tampered =\"e0\">ONE </ button >\\n5 <button id=\" subbtn2 \" style =\" position : absolute ; left :44 px; top :97 px\" data -\\nwob_ref =\"5\" data - wob_eps =\"e0\">TWO </ button >\\n6 </div >\\n7</div >\\nCurrent task: Click button ONE, then click button TWO.\\nHere is a plan you are following now.\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\nWe have a history of instructions that have been already executed by the autonomous agent so far.\\n1: clickxpath //button[@id=‚Äòsubbtn‚Äô]\\nBased on the plan and the history of instructions executed so far, the next proper instruction should be ‚Äò\\n***************************************************************\\nAssistantAgent to Executor agent:\\nclickxpath //button[@id=‚Äòsubbtn2‚Äô]\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nSUCCESS!!!!\\n TERMINATE\\n43', metadata={'source': 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-search-indexing\\\\utils\\\\data\\\\autogen.pdf', 'page': 42})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_search_indexer_client.read_and_load_pdf(pdf_path=pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 17:11:25,576 - micro - MainProcess - INFO     Reading PDF files from C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\\utils\\data\\ultraflex_user_manual.pdf. (indexing_azureai_search.py:read_and_load_pdfs:336)\n",
      "2024-01-07 17:11:37,051 - micro - MainProcess - INFO     Starting to embed and index 39 chuncks. (indexing_azureai_search.py:embed_and_index:402)\n",
      "2024-01-07 17:11:41,483 - micro - MainProcess - INFO     Successfully embedded and indexed 39 chuncks. (indexing_azureai_search.py:embed_and_index:404)\n"
     ]
    }
   ],
   "source": [
    "# Scrap web and chuck files into sentences\n",
    "# Define the URLs of the web pages to scrape\n",
    "file_1 = \"utils\\\\data\\\\ultraflex_user_manual.pdf\"\n",
    "\n",
    "# Set the chunk size and overlap size for splitting the text\n",
    "CHUNK_SIZE = 512\n",
    "OVERLAP_SIZE = 128\n",
    "SEPARATOR = \"(\\n\\w|\\w\\n)\"\n",
    "\n",
    "# Scrape the web pages, split the text into chunks, and store the chunks\n",
    "# The text is split into chunks of size CHUNK_SIZE, with an overlap of OVERLAP_SIZE between consecutive chunks\n",
    "text_chuncked = gbb_ai_client.load_and_split_text_by_character_from_pdf(\n",
    "    source=file_1, chunk_size=CHUNK_SIZE, chunk_overlap=OVERLAP_SIZE\n",
    ")\n",
    "\n",
    "# Embed the chunks and index them in Azure Search\n",
    "# This function converts the text chunks into vector embeddings and stores them in the Azure Search index\n",
    "gbb_ai_client.embed_and_index(text_chuncked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"utils\\\\data\\\\autogen.pdf\"\n",
    "url_pdf = \"https://arxiv.org/pdf/2308.08155.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(pdf_path)\n",
    "document_path = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(url_pdf)\n",
    "document_url = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='AutoGen : Enabling Next-Gen LLM\\nApplications via Multi-Agent Conversation\\nQingyun Wu‚Ä†, Gagan Bansal‚àó, Jieyu Zhang¬±, Yiran Wu‚Ä†, Beibin Li‚àó\\nErkang Zhu‚àó, Li Jiang‚àó, Xiaoyun Zhang‚àó, Shaokun Zhang‚Ä†, Jiale Liu‚àì\\nAhmed Awadallah‚àó, Ryen W. White‚àó, Doug Burger‚àó, Chi Wang‚àó1\\n‚àóMicrosoft Research,‚Ä†Pennsylvania State University\\n¬±University of Washington,‚àìXidian University\\nAgent CustomizationConversable agent\\nFlexible Conversation Patterns\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\nHierarchical chatJoint chatMulti-Agent Conversations‚Ä¶Execute the following code‚Ä¶\\nGot it! Here is the revised code ‚Ä¶No, please plot % change!Plot a chart of META and TESLA stock price change YTD.\\nOutput:$Month\\nOutput:%MonthError package yfinanceis not installed\\nSorry! Please first pip install yfinanceand then execute the code\\nInstalling‚Ä¶\\nExample Agent Chat\\nFigure 1: AutoGen enables diverse LLM-based applications using multi-agent conversations. (Left)\\nAutoGen agents are conversable, customizable, and can be based on LLMs, tools, humans, or even\\na combination of them. (Top-middle) Agents can converse to solve tasks. (Right) They can form\\na chat, potentially with humans in the loop. (Bottom-middle) The framework supports flexible\\nconversation patterns.\\nAbstract\\nAutoGen2is an open-source framework that allows developers to build LLM ap-\\nplications via multiple agents that can converse with each other to accomplish\\ntasks. AutoGen agents are customizable, conversable , and can operate in vari-\\nous modes that employ combinations of LLMs, human inputs, and tools. Using\\nAutoGen , developers can also flexibly define agent interaction behaviors. Both\\nnatural language and computer code can be used to program flexible conversation\\npatterns for different applications. AutoGen serves as a generic framework for\\nbuilding diverse applications of various complexities and LLM capacities. Em-\\npirical studies demonstrate the effectiveness of the framework in many example\\napplications, with domains ranging from mathematics, coding, question answer-\\ning, operations research, online decision-making, entertainment, etc.\\n1Corresponding author. Email: auto-gen@outlook.com\\n2https://github.com/microsoft/autogenarXiv:2308.08155v2  [cs.AI]  3 Oct 2023', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 0}),\n",
       " Document(page_content='1 Introduction\\nLarge language models (LLMs) are becoming a crucial building block in developing powerful agents\\nthat utilize LLMs for reasoning, tool usage, and adapting to new observations (Yao et al., 2022; Xi\\net al., 2023; Wang et al., 2023b) in many real-world tasks. Given the expanding tasks that could\\nbenefit from LLMs and the growing task complexity, an intuitive approach to scale up the power of\\nagents is to use multiple agents that cooperate. Prior work suggests that multiple agents can help\\nencourage divergent thinking (Liang et al., 2023), improve factuality and reasoning (Du et al., 2023),\\nand provide validation (Wu et al., 2023). In light of the intuition and early evidence of promise, it is\\nintriguing to ask the following question: how can we facilitate the development of LLM applications\\nthat could span a broad spectrum of domains and complexities based on the multi-agent approach?\\nOur insight is to use multi-agent conversations to achieve it. There are at least three reasons con-\\nfirming its general feasibility and utility thanks to recent advances in LLMs: First, because chat-\\noptimized LLMs (e.g., GPT-4) show the ability to incorporate feedback, LLM agents can cooperate\\nthrough conversations with each other or human(s), e.g., a dialog where agents provide and seek rea-\\nsoning, observations, critiques, and validation. Second, because a single LLM can exhibit a broad\\nrange of capabilities (especially when configured with the correct prompt and inference settings),\\nconversations between differently configured agents can help combine these broad LLM capabilities\\nin a modular and complementary manner. Third, LLMs have demonstrated ability to solve complex\\ntasks when the tasks are broken into simpler subtasks. Multi-agent conversations can enable this\\npartitioning and integration in an intuitive manner. How can we leverage the above insights and\\nsupport different applications with the common requirement of coordinating multiple agents, poten-\\ntially backed by LLMs, humans, or tools exhibiting different capacities? We desire a multi-agent\\nconversation framework with generic abstraction and effective implementation that has the flexibil-\\nity to satisfy different application needs. Achieving this requires addressing two critical questions:\\n(1) How can we design individual agents that are capable, reusable, customizable, and effective in\\nmulti-agent collaboration? (2) How can we develop a straightforward, unified interface that can\\naccommodate a wide range of agent conversation patterns? In practice, applications of varying\\ncomplexities may need distinct sets of agents with specific capabilities, and may require different\\nconversation patterns, such as single- or multi-turn dialogs, different human involvement modes, and\\nstatic vs. dynamic conversation. Moreover, developers may prefer the flexibility to program agent\\ninteractions in natural language or code. Failing to adequately address these two questions would\\nlimit the framework‚Äôs scope of applicability and generality.\\nWhile there is contemporaneous exploration of multi-agent approaches,3we present AutoGen , a\\ngeneralized multi-agent conversation framework (Figure 1), based on the following new concepts.\\n1Customizable and conversable agents. AutoGen uses a generic design of agents that can lever-\\nage LLMs, human inputs, tools, or a combination of them. The result is that developers can\\neasily and quickly create agents with different roles (e.g., agents to write code, execute code,\\nwire in human feedback, validate outputs, etc.) by selecting and configuring a subset of built-in\\ncapabilities. The agent‚Äôs backend can also be readily extended to allow more custom behaviors.\\nTo make these agents suitable for multi-agent conversation, every agent is made conversable ‚Äì\\nthey can receive, react, and respond to messages. When configured properly, an agent can hold\\nmultiple turns of conversations with other agents autonomously or solicit human inputs at cer-\\ntain rounds, enabling human agency and automation. The conversable agent design leverages the\\nstrong capability of the most advanced LLMs in taking feedback and making progress via chat\\nand also allows combining capabilities of LLMs in a modular fashion. (Section 2.1)\\n2Conversation programming. A fundamental insight of AutoGen is to simplify and unify com-\\nplex LLM application workflows as multi-agent conversations. So AutoGen adopts a program-\\nming paradigm centered around these inter-agent conversations. We refer to this paradigm as\\nconversation programming , which streamlines the development of intricate applications via two\\nprimary steps: (1) defining a set of conversable agents with specific capabilities and roles (as\\ndescribed above); (2) programming the interaction behavior between agents via conversation-\\ncentric computation andcontrol . Both steps can be achieved via a fusion of natural and pro-\\ngramming languages to build applications with a wide range of conversation patterns and agent\\nbehaviors. AutoGen provides ready-to-use implementations and also allows easy extension and\\nexperimentation for both steps. (Section 2.2)\\n3We refer to Appendix A for a detailed discussion.\\n2', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 1}),\n",
       " Document(page_content='AutoGen also provides a collection of multi-agent applications created using conversable agents\\nand conversation programming. These applications demonstrate how AutoGen can easily support\\napplications of various complexities and LLMs of various capabilities. Moreover, we perform both\\nevaluation on benchmarks and a pilot study of new applications. The results show that AutoGen can\\nhelp achieve outstanding performance on many tasks, and enable innovative ways of using LLMs,\\nwhile reducing development effort. (Section 3 and Appendix D)\\n2 The AutoGen Framework\\nTo reduce the effort required for developers to create complex LLM applications across various do-\\nmains, a core design principle of AutoGen is to streamline and consolidate multi-agent workflows\\nusing multi-agent conversations. This approach also aims to maximize the reusability of imple-\\nmented agents. This section introduces the two key concepts of AutoGen : conversable agents and\\nconversation programming.\\n2.1 Conversable Agents\\nInAutoGen , aconversable agent is an entity with a specific role that can pass messages to send and\\nreceive information to and from other conversable agents, e.g., to start or continue a conversation. It\\nmaintains its internal context based on sent and received messages and can be configured to possess\\na set of capabilities, e.g., enabled by LLMs, tools, or human input, etc. The agents can act according\\nto programmed behavior patterns described next.\\nAgent capabilities powered by LLMs, humans, and tools. Since an agent‚Äôs capabilities directly\\ninfluence how it processes and responds to messages, AutoGen allows flexibility to endow its agents\\nwith various capabilities. AutoGen supports many common composable capabilities for agents,\\nincluding 1) LLMs. LLM-backed agents exploit many capabilities of advanced LLMs such as role\\nplaying, implicit state inference and progress making conditioned on conversation history, providing\\nfeedback, adapting from feedback, and coding. These capabilities can be combined in different ways\\nvia novel prompting techniques4to increase an agent‚Äôs skill and autonomy. AutoGen also offers\\nenhanced LLM inference features such as result caching, error handling, message templating, etc.,\\nvia an enhanced LLM inference layer. 2) Humans. Human involvement is desired or even essential\\nin many LLM applications. AutoGen lets a human participate in agent conversation via human-\\nbacked agents, which could solicit human inputs at certain rounds of a conversation depending on\\nthe agent configuration. The default user proxy agent allows configurable human involvement levels\\nand patterns, e.g., frequency and conditions for requesting human input including the option for\\nhumans to skip providing input. 3) Tools. Tool-backed agents have the capability to execute tools\\nvia code execution or function execution. For example, the default user proxy agent in AutoGen is\\nable to execute code suggested by LLMs, or make LLM-suggested function calls.\\nAgent customization and cooperation. Based on application-specific needs, each agent can be\\nconfigured to have a mix of basic back-end types to display complex behavior in multi-agent con-\\nversations. AutoGen allows easy creation of agents with specialized capabilities and roles by reusing\\nor extending the built-in agents. The yellow-shaded area of Figure 2 provides a sketch of the built-in\\nagents in AutoGen . The ConversableAgent class is the highest-level agent abstraction and, by\\ndefault, can use LLMs, humans, and tools. The AssistantAgent andUserProxyAgent are two\\npre-configured ConversableAgent subclasses, each representing a common usage mode, i.e., act-\\ning as an AI assistant (backed by LLMs) and acting as a human proxy to solicit human input or\\nexecute code/function calls (backed by humans and/or tools).\\nIn the example on the right-hand side of Figure 1, an LLM-backed assistant agent and a tool- and\\nhuman-backed user proxy agent are deployed together to tackle a task. Here, the assistant agent\\ngenerates a solution with the help of LLMs and passes the solution to the user proxy agent. Then,\\nthe user proxy agent solicits human inputs or executes the assistant‚Äôs code and passes the results as\\nfeedback back to the assistant.\\n4Appendix C presents an example of such novel prompting techniques which empowers the default LLM-\\nbacked assistant agent in AutoGen to converse with other agents in multi-step problem solving.\\n3', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 2}),\n",
       " Document(page_content='2 Initiate Conversations:A.initiate_chat(‚ÄúPlot a chart of META and TESLA stock price change YTD.‚Äù, B)\\nAssistant BUser Proxy AAutoGenAgents\\nDeveloper Code# This funcwill be invoked in generate_replyA.register_reply(B,  reply_func_A2B)def reply_func_A2B(msg):ouput= input_from_human()‚Ä¶if not ouput:if msg includes code:output = execute(msg)return outputConversableAgent\\nAssistantAgentUserProxyAgenthuman_input_mode= ‚ÄúNEVER‚Äùcode_execution_config= FalseDEFAULT_SYSTEM_MESSAGE = ‚ÄúYou are a helpful AI assistant‚Ä¶In the following cases, suggest python code‚Ä¶‚Äùhuman_input_mode=‚ÄúALWAYS‚Äù\\nGroupChatManagerhuman_input_mode= ‚ÄúNEVER‚Äùgroup_chat= [              ] \\n# Note: when no reply funcis registered, a list of default reply functions will be used. Agent Customization:\\nProgram ExecutionPlot a chart of META and TESLA stock price change YTD.Execute the following code‚Ä¶sendreceivereceiveConversation-Centric Computationgenerate_replyError: package yfinanceis not installedsendgenerate_replySorry! Please first pip install yfinanceand then executeConversation-Driven Control Flowgenerate_replyThe Resulting Automated Agent Chat:‚Ä¶1.2 Register a Custom Reply Func:1.1 Define Agents:Unified Conversation Interfaces:‚Ä¢send‚Ä¢receive ‚Ä¢generate_reply\\nFigure 2: Illustration of how to use AutoGen to program a multi-agent conversation. The top sub-\\nfigure illustrates the built-in agents provided by AutoGen , which have unified conversation interfaces\\nand can be customized. The middle sub-figure shows an example of using AutoGen to develop\\na two-agent system with a custom reply function. The bottom sub-figure illustrates the resulting\\nautomated agent chat from the two-agent system during program execution.\\nBy allowing custom agents that can converse with each other, conversable agents in AutoGen serve\\nas a useful building block. However, to develop applications where agents make meaningful progress\\non tasks, developers also need to be able to specify and mold these multi-agent conversations.\\n2.2 Conversation Programming\\nAs a solution to the above problem, AutoGen utilizes conversation programming , a paradigm that\\nconsiders two concepts: the first is computation ‚Äì the actions agents take to compute their response\\nin a multi-agent conversation. And the second is control flow ‚Äì the sequence (or conditions) un-\\nder which these computations happen. As we will show in the applications section, the ability to\\nprogram these helps implement many flexible multi-agent conversation patterns. In AutoGen , these\\ncomputations are conversation-centric. An agent takes actions relevant to the conversations it is\\ninvolved in and its actions result in message passing for consequent conversations (unless a termina-\\ntion condition is satisfied). Similarly, control flow is conversation-driven ‚Äì the participating agents‚Äô\\ndecisions on which agents to send messages to and the procedure of computation are functions of the\\ninter-agent conversation. This paradigm helps one to reason intuitively about a complex workflow\\nas agent action taking and conversation message-passing between agents.\\nFigure 2 provides a simple illustration. The bottom sub-figure shows how individual agents perform\\ntheir role-specific, conversation-centric computations to generate responses (e.g., via LLM inference\\ncalls and code execution). The task progresses through conversations displayed in the dialog box.\\nThe middle sub-figure demonstrates a conversation-based control flow. When the assistant receives\\na message, the user proxy agent typically sends the human input as a reply. If there is no input, it\\nexecutes any code in the assistant‚Äôs message instead.\\n4', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 3}),\n",
       " Document(page_content='AutoGen features the following design patterns to facilitate conversation programming:\\n1.Unified interfaces and auto-reply mechanisms for automated agent chat. Agents in\\nAutoGen have unified conversation interfaces for performing the corresponding conversation-\\ncentric computation, including a send/receive function for sending/receiving messages and a\\ngenerate reply function for taking actions and generating a response based on the received\\nmessage. AutoGen also introduces and by default adopts an agent auto-reply mechanism to\\nrealize conversation-driven control: Once an agent receives a message from another agent, it au-\\ntomatically invokes generate reply and sends the reply back to the sender unless a termination\\ncondition is satisfied. AutoGen provides built-in reply functions based on LLM inference, code\\nor function execution, or human input. One can also register custom reply functions to customize\\nthe behavior pattern of an agent, e.g., chatting with another agent before replying to the sender\\nagent. Under this mechanism, once the reply functions are registered, and the conversation is\\ninitialized, the conversation flow is naturally induced, and thus the agent conversation proceeds\\nnaturally without any extra control plane, i.e., a special module that controls the conversation\\nflow. For example, with the developer code in the blue-shaded area (marked ‚ÄúDeveloper Code‚Äù)\\nof Figure 2, one can readily trigger the conversation among the agents, and the conversation\\nwould proceed automatically, as shown in the dialog box in the grey shaded area (marked ‚ÄúPro-\\ngram Execution‚Äù) of Figure 2. The auto-reply mechanism provides a decentralized, modular, and\\nunified way to define the workflow.\\n2.Control by fusion of programming and natural language. AutoGen allows the usage of\\nprogramming and natural language in various control flow management patterns: 1) Natural-\\nlanguage control via LLMs. InAutoGen , one can control the conversation flow by prompting\\nthe LLM-backed agents with natural language. For instance, the default system message of the\\nbuilt-in AssistantAgent inAutoGen uses natural language to instruct the agent to fix errors\\nand generate code again if the previous result indicates there are errors. It also guides the agent\\nto confine the LLM output to certain structures, making it easier for other tool-backed agents to\\nconsume. For example, instructing the agent to reply with ‚ÄúTERMINATE‚Äù when all tasks are\\ncompleted to terminate the program. More concrete examples of natural language controls can\\nbe found in Appendix C. 2) Programming-language control. InAutoGen , Python code can be\\nused to specify the termination condition, human input mode, and tool execution logic, e.g., the\\nmax number of auto replies. One can also register programmed auto-reply functions to control\\nthe conversation flow with Python code, as shown in the code block identified as ‚ÄúConversation-\\nDriven Control Flow‚Äù in Figure 2. 3) Control transition between natural and programming\\nlanguage. AutoGen also supports flexible control transition between natural and programming\\nlanguage. One can achieve transition from code to natural-language control by invoking an LLM\\ninference containing certain control logic in a customized reply function; or transition from nat-\\nural language to code control via LLM-proposed function calls (Eleti et al., 2023).\\nIn the conversation programming paradigm, one can realize multi-agent conversations of diverse\\npatterns. In addition to static conversation with predefined flow, AutoGen also supports dynamic\\nconversation flows with multiple agents. AutoGen provides two general ways to achieve this: 1)\\nCustomized generate reply function: within the customized generate reply function, one\\nagent can hold the current conversation while invoking conversations with other agents depending\\non the content of the current message and context. 2) Function calls: In this approach, LLM decides\\nwhether or not to call a particular function depending on the conversation status. By messaging\\nadditional agents in the called functions, the LLM can drive dynamic multi-agent conversation. In\\naddition, AutoGen supports more complex dynamic group chat via built-in GroupChatManager ,\\nwhich can dynamically select the next speaker and then broadcast its response to other agents. We\\nelaborate on this feature and its application in Section 3. We provide implemented working systems\\nto showcase all these different patterns, with some of them visualized in Figure 3.\\n3 Applications of AutoGen\\nWe demonstrate six applications using AutoGen (see Figure 3) to illustrate its potential in simplify-\\ning the development of high-performance multi-agent applications. These applications are selected\\nbased on their real-world relevance (A1, A2, A4, A5, A6), problem difficulty and solving capabil-\\nities enabled by AutoGen (A1, A2, A3, A4), and innovative potential (A5, A6). Together, these\\ncriteria showcase AutoGen ‚Äôs role in advancing the LLM-application landscape.\\n5', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 4}),\n",
       " Document(page_content='A1. Math Problem Solving\\nA4. Multi-agent CodingCommander\\nSafeguard\\nWriter\\nA6. Conversational ChessA2. Retrieval-augmented ChatRetrieval-augmentedAssistantRetrieval-augmentedUser Proxy\\nChess Board\\nHuman/AI Chess Player A\\nHuman/AI Chess Player B\\nStudent\\nAssistant\\nAssistantExpert\\nAsk  expert\\nBroadcast\\nManager\\nSpeak\\nA5. Dynamic Group Chat\\nALFWorldExecutorAssistant\\nGrounding Agent\\nA3. ALF ChatFigure 3: Six examples of diverse applications built using AutoGen . Their conversation patterns\\nshow AutoGen ‚Äôs flexibility and power.\\nA1: Math Problem Solving\\nMathematics is a foundational discipline and the promise of leveraging LLMs to assist with math\\nproblem solving opens up a new plethora of applications and avenues for exploration, including per-\\nsonalized AI tutoring, AI research assistance, etc. This section demonstrates how AutoGen can help\\ndevelop LLM applications for math problem solving, showcasing strong performance and flexibility\\nin supporting various problem-solving paradigms.\\n(Scenario 1 ) We are able to build a system for autonomous math problem solving by directly reusing\\ntwo built-in agents from AutoGen . We evaluate our system and several alternative approaches,\\nincluding open-source methods such as Multi-Agent Debate (Liang et al., 2023), LangChain Re-\\nAct (LangChain, 2023), vanilla GPT-4, and commercial products ChatGPT + Code Interpreter, and\\nChatGPT + Plugin (Wolfram Alpha), on the MATH (Hendrycks et al., 2021) dataset and summarize\\nthe results in Figure 4a. We perform evaluations over 120 randomly selected level-5 problems and\\non the entire5test dataset from MATH. The results show that the built-in agents from AutoGen al-\\nready yield better performance out of the box compared to the alternative approaches, even including\\nthe commercial ones. ( Scenario 2 ) We also showcase a human-in-the-loop problem-solving process\\nwith the help of AutoGen . To incorporate human feedback with AutoGen , one only needs to set\\nhuman input mode=‚ÄòALWAYS‚Äô in the UserProxyAgent of the system in scenario 1. We demon-\\nstrate that this system can effectively incorporate human inputs to solve challenging problems that\\ncannot be solved without humans. ( Scenario 3 ) We further demonstrate a novel scenario where\\nmultiple human users can participate in the conversations during the problem-solving process. Our\\nexperiments and case studies for these scenarios show that AutoGen enables better performance or\\nnew experience compared to other solutions we experimented with. Due to the page limit, details of\\nthe evaluation, including case studies in three scenarios are in Appendix D.\\nA2: Retrieval-Augmented Code Generation and Question Answering\\nRetrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic\\nlimitations of LLMs by incorporating external documents. In this section, we employ AutoGen to\\nbuild a Retrieval-Augmented Generation (RAG) system (Lewis et al., 2020; Parvez et al., 2021)\\nnamed Retrieval-augmented Chat. The system consists of two agents: a Retrieval-augmented User\\nProxy agent and a Retrieval-augmented Assistant agent, both of which are extended from built-in\\nagents from AutoGen . The Retrieval-augmented User Proxy includes a vector database (Chroma,\\n5We did not evaluate ChatGPT on the whole dataset since it requires substantial manual effort and is re-\\nstricted by its hourly message-number limitation. Multi-agent debate and LangChain ReAct were also not\\nevaluated since they underperformed vanilla GPT-4 on the smaller test set.\\n6', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 5}),\n",
       " Document(page_content='AutoGen ChatGPT\\n+CodeChatGPT\\n+PluginGPT-4 Multi-Agent\\nDebateLangChain\\nReAct\\nMethods01020304050607080Success Ratio (%)52.5%\\n48.33%\\n45.0%\\n30.0%\\n26.67%\\n23.33%69.48%\\n55.18%120 Level-5 problems\\nWhole Dataset(a) A1: Performance on MATH (w/ GPT-4).\\nF1 Recall\\nMetrics01020304050607080Percentage (%)25.88%66.65%\\n15.12%58.56%\\n22.79%62.59%AutoGen\\nAuotGen W/O interactive retrieval\\nDPR (b) A2: Q&A tasks (w/ GPT-3.5).\\nAutoGen (3 agent) AutoGen (2 agent) ReAct\\nMethods020406080100Success Ratio (%)69%\\n54% 54%77%\\n63%66%Average\\nBest of 3\\n(c) A3: Performance on ALFWorld.\\nF1 Recall\\nMetrics020406080100Percentage (%)96.00%98.00%\\n88.00%\\n78.00%83.00%\\n72.00%\\n48.00%\\n32.00%Multi-GPT4\\nSingle-GPT4\\nMulti-GPT3.5\\nSingle-GPT3.5 (d) A4: Performance on OptiGuide.\\nFigure 4: Performance on four applications A1-A4. (a) shows that AutoGen agents can be used\\nout of the box to achieve the most competitive performance on math problem solving tasks; (b)\\nshows that AutoGen can be used to realize effective retrieval augmentation and realize a novel\\ninteractive retrieval feature to boost performance on Q&A tasks; (c) shows that AutoGen can be used\\nto introduce a three-agent system with a grounding agent to improve performance on ALFWorld;\\n(d) shows that a multi-agent design is helpful in boosting performance in coding tasks that need\\nsafeguards.\\n2023) with SentenceTransformers (Reimers & Gurevych, 2019) as the context retriever. A detailed\\nworkflow description of the Retrieval-augmented Chat is provided in Appendix D.\\nWe evaluate Retrieval-augmented Chat in both question-answering and code-generation scenarios.\\n(Scenario 1 ) We first perform an evaluation regarding natural question answering on the Natural\\nQuestions dataset (Kwiatkowski et al., 2019) and report results in Figure 4b. In this evaluation, we\\ncompare our system with DPR (Dense Passage Retrieval) following an existing evaluation6prac-\\ntice (Adlakha et al., 2023). Leveraging the conversational design and natural-language control,\\nAutoGen introduces a novel interactive retrieval feature in this application: whenever the retrieved\\ncontext does not contain the information, instead of terminating, the LLM-based assistant would\\nreply ‚Äú Sorry, I cannot find any information about... UPDATE CONTEXT. ‚Äù which will invoke more\\nretrieval attempts. We conduct an ablation study in which we prompt the assistant agent to say ‚ÄúI\\ndon‚Äôt know‚Äù instead of ‚ÄúUPDATE CONTEXT. ‚Äù in cases where relevant information is not found,\\nand report results in Figure 4b. The results show that the interactive retrieval mechanism indeed\\nplays a non-trivial role in the process. We give a concrete example and results using this appealing\\nfeature in Appendix D. ( Scenario 2 ) We further demonstrate how Retrieval-augmented Chat aids in\\ngenerating code based on a given codebase that contains code not included in GPT-4‚Äôs training data.\\nEvaluation and demonstration details for both scenarios are included in Appendix D.\\n6The results of DPR with GPT-3.5 shown in Figure 4b are from (Adlakha et al., 2023). We use GPT-3.5 as\\na shorthand for GPT-3.5-turbo.\\n7', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 6}),\n",
       " Document(page_content='A3: Decision Making in Text World Environments\\nIn this subsection, we demonstrate how AutoGen can be used to develop effective applications that\\ninvolve interactive or online decision making. We perform the study using the ALFWorld (Shridhar\\net al., 2021) benchmark, which includes a diverse collection of synthetic language-based interactive\\ndecision-making tasks in household environments.\\nWith AutoGen , we implemented a two-agent system to solve tasks from ALFWorld. It consists of\\nan LLM-backed assistant agent responsible for suggesting plans to conduct a task and an executor\\nagent responsible for executing actions in the ALFWorld environments. This system integrates Re-\\nAct prompting (Yao et al., 2022), and is able to achieve similar performance. A common challenge\\nencountered in both ReAct and the AutoGen -based two-agent system is their occasional inability to\\nleverage basic commonsense knowledge about the physical world. This deficiency can lead to the\\nsystem getting stuck in a loop due to repetitive errors. Fortunately, the modular design of AutoGen\\nallows us to address this issue effectively: With AutoGen , we are able to introduce a grounding\\nagent, which supplies crucial commonsense knowledge‚Äìsuch as ‚ÄúYou must find and take the object\\nbefore you can examine it. You must go to where the target object is before you can use it. ‚Äù ‚Äìwhenever\\nthe system exhibits early signs of recurring errors. It significantly enhances the system‚Äôs ability to\\navoid getting entangled in error loops. We compare the task-solving performance of the two variants\\nof our system with GPT-3.5-turbo and ReAct7on the 134 unseen tasks from ALFWorld and report\\nresults in Figure 4c. The results show that introducing a grounding agent could bring in a 15%\\nperformance gain on average. Upon examining the systems‚Äô outputs, we observe that the grounding\\nagent, by delivering background commonsense knowledge at the right junctures, significantly miti-\\ngated the tendency of the system to persist with a flawed plan, thereby avoiding the creation of error\\nloops. For an example trajectory comparing the systems see Appendix D, Figure 10.\\nA4: Multi-Agent Coding\\nIn this subsection, we use AutoGen to build a multi-agent coding system based on OptiGuide (Li\\net al., 2023a), a system that excels at writing code to interpret optimization solutions and answer\\nuser questions, such as exploring the implications of changing a supply-chain decision or under-\\nstanding why the optimizer made a particular choice. The second sub-figure of Figure 3 shows the\\nAutoGen -based implementation. The workflow is as follows: the end user sends questions, such as\\n‚ÄúWhat if we prohibit shipping from supplier 1 to roastery 2? ‚Äù to the Commander agent. The Com-\\nmander coordinates with two assistant agents, including the Writer and the Safeguard, to answer\\nthe question. The Writer will craft code and send the code to the Commander. After receiving the\\ncode, the Commander checks the code safety with the Safeguard; if cleared, the Commander will\\nuse external tools (e.g., Python) to execute the code, and request the Writer to interpret the execution\\nresults. For instance, the writer may say ‚Äú if we prohibit shipping from supplier 1 to roastery 2, the\\ntotal cost would increase by 10.5%. ‚Äù The Commander then provides this concluding answer to the\\nend user. If, at a particular step, there is an exception, e.g., security red flag raised by Safeguard, the\\nCommander redirects the issue back to the Writer with debugging information. The process might\\nbe repeated multiple times until the user‚Äôs question is answered or timed-out.\\nWith AutoGen the core workflow code for OptiGuide was reduced from over 430 lines to 100 lines,\\nleading to significant productivity improvement. We provide a detailed comparison of user expe-\\nrience with ChatGPT+Code Interpreter and AutoGen -based OptiGuide in Appendix D, where we\\nshow that AutoGen -based OptiGuide could save around 3x of user‚Äôs time and reduce user interac-\\ntions by 3 - 5 times on average. We also conduct an ablation showing that multi-agent abstraction is\\nnecessary. Specifically, we construct a single-agent approach where a single agent conducts both the\\ncode-writing and safeguard processes. We tested the single- and multi-agent approaches on a dataset\\nof 100 coding tasks, which is crafted to include equal numbers of safe and unsafe tasks. Evaluation\\nresults as reported in Figure 4d show that the multi-agent design boosts the F-1 score in identifying\\nunsafe code by 8% (with GPT-4) and 35% (with GPT-3.5-turbo).\\n7Results of ReAct are obtained by directly running its official code with default settings. The code uses\\ntext-davinci-003 as backend LM and does not support GPT-3.5-turbo or GPT-4.\\n8', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 7}),\n",
       " Document(page_content='A5: Dynamic Group Chat\\nAutoGen provides native support for a dynamic group chat communication pattern, in which par-\\nticipating agents share the same context and converse with the others in a dynamic manner instead\\nof following a pre-defined order. Dynamic group chat relies on ongoing conversations to guide the\\nflow of interaction among agents. These make dynamic group chat ideal for situations where col-\\nlaboration without strict communication order is beneficial. In AutoGen , the GroupChatManager\\nclass serves as the conductor of conversation among agents and repeats the following three steps:\\ndynamically selecting a speaker, collecting responses from the selected speaker, and broadcasting\\nthe message (Figure 3-A5). For the dynamic speaker-selection component, we use a role-play style\\nprompt. Through a pilot study on 12 manually crafted complex tasks, we observed that compared\\nto a prompt that is purely based on the task, utilizing a role-play prompt often leads to more effec-\\ntive consideration of both conversation context and role alignment during the problem-solving and\\nspeaker-selection process. Consequently, this leads to a higher success rate and fewer LLM calls.\\nWe include detailed results in Appendix D.\\nA6: Conversational Chess\\nUsing AutoGen , we developed Conversational Chess, a natural language interface game shown in\\nthe last sub-figure of Figure 3. It features built-in agents for players, which can be human or LLM,\\nand a third-party board agent to provide information and validate moves based on standard rules.\\nWith AutoGen , we enabled two essential features: (1) Natural, flexible, and engaging game dynam-\\nics, enabled by the customizable agent design in AutoGen . Conversational Chess supports a range\\nof game-play patterns, including AI-AI, AI-human, and human-human, with seamless switching\\nbetween these modes during a single game. An illustrative example of these entertaining game dy-\\nnamics can be found in Figure 15, Appendix D. (2) Grounding, which is a crucial aspect to maintain\\ngame integrity. During gameplay, the board agent checks each proposed move for legality; if a move\\nis invalid, the agent responds with an error, prompting the player agent to re-propose a legal move\\nbefore continuing. This process ensures that only valid moves are played and helps maintain a con-\\nsistent gaming experience. As an ablation study, we removed the board agent and instead only relied\\non a relevant prompt ‚Äúyou should make sure both you and the opponent are making legal moves‚Äù to\\nground their move. The results highlighted that without the board agent, illegitimate moves caused\\ngame disruptions. The modular design offered flexibility, allowing swift adjustments to the board\\nagent in response to evolving game rules or varying chess rule variants. A comprehensive demon-\\nstration of this ablation study is in Appendix D.\\n4 Discussion\\nWe introduced an open-source library, AutoGen , that incorporates the paradigms of conversable\\nagents and conversation programming. This library utilizes capable agents that are well-suited for\\nmulti-agent cooperation. It features a unified conversation interface among the agents, along with\\nan auto-reply mechanisms, which help establish an agent-interaction interface that capitalizes on the\\nstrengths of chat-optimized LLMs with broad capabilities while accommodating a wide range of\\napplications. AutoGen serves as a general framework for creating and experimenting with multi-\\nagent systems that can easily fulfill various practical requirements, such as reusing, customizing,\\nand extending existing agents, as well as programming conversations between them.\\nOur experiments, as detailed in Section 3, demonstrate that this approach offers numerous benefits.\\nThe adoption of AutoGen has resulted in improved performance (over state-of-the-art approaches),\\nreduced development code, and decreased manual burden for existing applications. It offers flex-\\nibility to developers, as demonstrated in A1 (scenario 3), A5, and A6, where AutoGen enables\\nmulti-agent chats to follow a dynamic pattern rather than fixed back-and-forth interactions. It allows\\nhumans to engage in activities alongside multiple AI agents in a conversational manner. Despite the\\ncomplexity of these applications (most involving more than two agents or dynamic multi-turn agent\\ncooperation), the implementation based on AutoGen remains straightforward. Dividing tasks among\\nseparate agents promotes modularity. Furthermore, since each agent can be developed, tested, and\\nmaintained separately, this approach simplifies overall development and code management.\\n9', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 8}),\n",
       " Document(page_content='Although this work is still in its early experimental stages, it paves the way for numerous future\\ndirections and research opportunities. For instance, we can explore effective integration of existing\\nagent implementations into our multi-agent framework and investigate the optimal balance between\\nautomation and human control in multi-agent workflows. As we further develop and refine AutoGen ,\\nwe aim to investigate which strategies, such as agent topology and conversation patterns, lead to the\\nmost effective multi-agent conversations while optimizing the overall efficiency, among other fac-\\ntors. While increasing the number of agents and other degrees of freedom presents opportunities for\\ntackling more complex problems, it may also introduce new safety challenges that require additional\\nstudies and careful consideration.\\nWe provide more discussion in Appendix B, including guidelines for using AutoGen and direction\\nof future work. We hope AutoGen will help improve many LLM applications in terms of speed of\\ndevelopment, ease of experimentation, and overall effectiveness and safety. We actively welcome\\ncontributions from the broader community.\\nEthics statement\\nThere are several potential ethical considerations that could arise from the development and use of\\ntheAutoGen framework.\\n‚Ä¢ Privacy and Data Protection: The framework allows for human participation in conversations\\nbetween agents. It is important to ensure that user data and conversations are protected, and that\\ndevelopers use appropriate measures to safeguard privacy.\\n‚Ä¢ Bias and Fairness: LLMs have been shown to exhibit biases present in their training data (Navigli\\net al., 2023). When using LLMs in the AutoGen framework, it is crucial to address and mitigate\\nany biases that may arise in the conversations between agents. Developers should be aware of\\npotential biases and take steps to ensure fairness and inclusivity.\\n‚Ä¢ Accountability and Transparency: As discussed in the future work section, as the framework in-\\nvolves multiple agents conversing and cooperating, it is important to establish clear accountability\\nand transparency mechanisms. Users should be able to understand and trace the decision-making\\nprocess of the agents involved in order to ensure accountability and address any potential issues\\nor biases.\\n‚Ä¢ Trust and Reliance: AutoGen leverages human understanding and intelligence while providing\\nautomation through conversations between agents. It is important to consider the impact of this\\ninteraction on user experience, trust, and reliance on AI systems. Clear communication and user\\neducation about the capabilities and limitations of the system will be essential (Cai et al., 2019).\\n‚Ä¢ Unintended Consequences: As discussed before, the use of multi-agent conversations and automa-\\ntion in complex tasks may have unintended consequences. In particular, allowing LLM agents to\\nmake changes in external environments through code execution or function calls, such as installing\\npackages, could be risky. Developers should carefully consider the potential risks and ensure that\\nappropriate safeguards are in place to prevent harm or negative outcomes.\\nAcknowledgements\\nThe work presented in this report was made possible through discussions and feedback from Peter\\nLee, Johannes Gehrke, Eric Horvitz, Steven Lucco, Umesh Madan, Robin Moeur, Piali Choud-\\nhury, Saleema Amershi, Adam Fourney, Victor Dibia, Guoqing Zheng, Corby Rosset, Ricky Loynd,\\nEce Kamar, Rafah Hosn, John Langford, Ida Momennejad, Brian Krabach, Taylor Webb, Shanka\\nSubhra Mondal, Wei-ge Chen, Robert Gruen, Yinan Li, Yue Wang, Suman Nath, Tanakorn Leesat-\\napornwongsa, Xin Wang, Shishir Patil, Tianjun Zhang, Saehan Jo, Ishai Menache, Kontantina Mel-\\nlou, Runlong Zhou, Feiran Jia, Hamed Khanpour, Hamid Palangi, Srinagesh Sharma, Julio Albinati\\nCortez, Amin Saied, Yuzhe Ma, Dujian Ding, Linyong Nan, Prateek Yadav, Shannon Shen, Ankur\\nMallick, Mark Encarnaci ¬¥on, Lars Liden, Tianwei Yue, Julia Kiseleva, Anastasia Razdaibiedina, and\\nLuciano Del Corro. Qingyun Wu would like to acknowledge the funding and research support from\\nthe College of Information Science and Technology at Penn State University.\\n10', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 9}),\n",
       " Document(page_content='References\\nVaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Eval-\\nuating correctness and faithfulness of instruction-following models for question answering. arXiv\\npreprint arXiv:2307.16877 , 2023.\\nSaleema Amershi, Dan Weld, Mihaela V orvoreanu, Adam Fourney, Besmira Nushi, Penny Col-\\nlisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. Guidelines for human-ai\\ninteraction. In Proceedings of the 2019 chi conference on human factors in computing systems ,\\n2019.\\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man ¬¥e. Con-\\ncrete problems in ai safety, 2016.\\nAutoGPT. Documentation ‚Äî auto-gpt. https://docs.agpt.co/ , 2023.\\nBabyAGI. Github ‚Äî babyagi. https://github.com/yoheinakajima/babyagi , 2023.\\nCarrie J. Cai, Samantha Winter, David F. Steiner, Lauren Wilcox, and Michael Terry. ‚Äùhello ai‚Äù:\\nUncovering the onboarding needs of medical practitioners for human-ai collaborative decision-\\nmaking. Proceedings of the ACM on Human-Computer Interaction , 2019.\\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as\\ntool makers. arXiv preprint arXiv:2305.17126 , 2023.\\nChroma. Chromadb. https://github.com/chroma-core/chroma , 2023.\\nVictor Dibia. LIDA: A tool for automatic generation of grammar-agnostic visualizations and info-\\ngraphics using large language models. In Proceedings of the 61st Annual Meeting of the Associ-\\nation for Computational Linguistics (Volume 3: System Demonstrations) , Toronto, Canada, July\\n2023. Association for Computational Linguistics.\\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt. arXiv\\npreprint arXiv:2304.07590 , 2023.\\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improv-\\ning factuality and reasoning in language models through multiagent debate. arXiv preprint\\narXiv:2305.14325 , 2023.\\nAtty Eleti, Jeff Harris, and Logan Kilpatrick. Function calling and other api updates. https:\\n//openai.com/blog/function-calling-and-other-api-updates , 2023.\\nGuidance. Guidance. https://github.com/guidance-ai/guidance , 2023.\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv\\npreprint arXiv:2103.03874 , 2021.\\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven\\nKa Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for\\nmulti-agent collaborative framework. arXiv preprint arXiv:2308.00352 , 2023.\\nEric Horvitz. Principles of mixed-initiative user interfaces. In Proceedings of the SIGCHI conference\\non Human Factors in Computing Systems , 1999.\\nHuggingFace. Transformers agent. https://huggingface.co/docs/transformers/\\ntransformers_agents , 2023.\\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.\\narXiv preprint arXiv:2303.17491 , 2023.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\\nbenchmark for question answering research. Transactions of the Association for Computational\\nLinguistics , 2019.\\n11', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 10}),\n",
       " Document(page_content='LangChain. Introduction ‚Äî langchain. https://python.langchain.com/en/latest/index.\\nhtml , 2023.\\nMike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal? end-\\nto-end learning for negotiation dialogues. arXiv preprint arXiv:1706.05125 , 2017.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich K ¬®uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¬®aschel, et al. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems ,\\n2020.\\nBeibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, and Ishai Menache. Large language\\nmodels for supply chain optimization. arXiv preprint arXiv:2307.03875 , 2023a.\\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\\nCamel: Communicative agents for ‚Äùmind‚Äù exploration of large scale language model society,\\n2023b.\\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng\\nTu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-\\nagent debate, 2023.\\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement\\nlearning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802 ,\\n2018.\\nJerry Liu. LlamaIndex, November 2022. URL https://github.com/jerryjliu/llama_index .\\nV olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-\\nstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\\narXiv:1312.5602 , 2013.\\nRoberto Navigli, Simone Conia, and Bj ¬®orn Ross. Biases in large language models: Origins, inven-\\ntory and discussion. ACM Journal of Data and Information Quality , 2023.\\nOpenAI. ChatGPT plugins. https://openai.com/blog/chatgpt-plugins , 2023.\\nJoon Sung Park, Joseph C O‚ÄôBrien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and\\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint\\narXiv:2304.03442 , 2023.\\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.\\nRetrieval augmented code generation and summarization. arXiv preprint arXiv:2108.11601 ,\\n2021.\\nShishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model\\nconnected with massive apis. arXiv preprint arXiv:2305.15334 , 2023.\\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-\\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\\nProcessing . Association for Computational Linguistics, 11 2019. URL https://arxiv.org/\\nabs/1908.10084 .\\nSemantic-Kernel. Semantic kernel. https://github.com/microsoft/semantic-kernel ,\\n2023.\\nBokui Shen, Fei Xia, Chengshu Li, Roberto Mart ¬¥ƒ±n-Mart ¬¥ƒ±n, Linxi Fan, Guanzhi Wang, Claudia\\nP¬¥erez-D‚ÄôArpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, et al. igibson 1.0: A simu-\\nlation environment for interactive tasks in large realistic scenes. In 2021 IEEE/RSJ International\\nConference on Intelligent Robots and Systems (IROS) . IEEE, 2021.\\nTianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An\\nopen-domain platform for web-based agents. In International Conference on Machine Learning .\\nPMLR, 2017.\\n12', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 11}),\n",
       " Document(page_content='Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C ÀÜot¬¥e, Yonatan Bisk, Adam Trischler, and Matthew\\nHausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In\\nProceedings of the International Conference on Learning Representations (ICLR) , 2021. URL\\nhttps://arxiv.org/abs/2010.03768 .\\nOriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,\\nMichelle Yeo, Alireza Makhzani, Heinrich K ¬®uttler, John Agapiou, Julian Schrittwieser, et al.\\nStarcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782 , 2017.\\nChi Wang, Qingyun Wu, Markus Weimer, and Erkang Zhu. Flaml: A fast and lightweight automl\\nlibrary. Proceedings of Machine Learning and Systems , 2021.\\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\\nand Anima Anandkumar. V oyager: An open-ended embodied agent with large language models.\\narXiv preprint arXiv:2305.16291 , 2023a.\\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\\narXiv preprint arXiv:2308.11432 , 2023b.\\nDaniel S. Weld and Oren Etzioni. The first law of robotics (a call to arms). In AAAI Conference on\\nArtificial Intelligence , 1994.\\nMax Woolf. Langchain problem. https://minimaxir.com/2023/07/langchain-problem/ ,\\n2023.\\nYiran Wu, Feiran Jia, Shaokun Zhang, Qingyun Wu, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat\\nLee, Richard Peng, and Chi Wang. An empirical study on challenging math problem solving with\\ngpt-4. arXiv preprint arXiv:2306.01337 , 2023.\\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe\\nWang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents:\\nA survey. arXiv preprint arXiv:2309.07864 , 2023.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 ,\\n2022.\\n13', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 12}),\n",
       " Document(page_content='A Related Work\\nWe examine existing LLM-based agent systems or frameworks that can be used to build LLM appli-\\ncations. We categorize the related work into single-agent and multi-agent systems and specifically\\nprovide a summary of differentiators comparing AutoGen with existing multi-agent systems in Ta-\\nble 1. Note that many of these systems are evolving open-source projects, so the remarks and\\nstatements about them may only be accurate as of the time of writing. We refer interested readers to\\ndetailed LLM-based agent surveys (Xi et al., 2023; Wang et al., 2023b)\\nSingle-Agent Systems:\\n‚Ä¢AutoGPT : AutoGPT is an open-source implementation of an AI agent that attempts to au-\\ntonomously achieve a given goal (AutoGPT, 2023). It follows a single-agent paradigm in which\\nit augments the AI model with many useful tools, and does not support multi-agent collaboration.\\n‚Ä¢ChatGPT+ (with code interpreter or plugin) : ChatGPT, a conversational AI service or agent,\\ncan now be used alongside a code interpreter or plugin (currently available only under the pre-\\nmium subscription plan ChatGPT Plus) (OpenAI, 2023). The code interpreter enables ChatGPT\\nto execute code, while the plugin enhances ChatGPT with a wide range of curated tools.\\n‚Ä¢LangChain Agents : LangChain is a general framework for developing LLM-based applica-\\ntions (LangChain, 2023). LangChain Agents is a subpackage for using an LLM to choose a\\nsequence of actions. There are various types of agents in LangChain Agents, with the ReAct agent\\nbeing a notable example that combines reasoning and acting when using LLMs (mainly designed\\nfor LLMs prior to ChatGPT) (Yao et al., 2022). All agents provided in LangChain Agents fol-\\nlow a single-agent paradigm and are not inherently designed for communicative and collaborative\\nmodes. A significant summary of its limitations can be found in (Woolf, 2023). Due to these lim-\\nitations, even the multi-agent systems in LangChain (e.g., re-implementation of CAMEL) are not\\nbased on LangChain Agents but are implemented from scratch. Their connection to LangChain\\nlies in the use of basic orchestration modules provided by LangChain, such as AI models wrapped\\nby LangChain and the corresponding interface.\\n‚Ä¢Transformers Agent : Transformers Agent (HuggingFace, 2023) is an experimental natural-\\nlanguage API built on the transformers repository. It includes a set of curated tools and an agent\\nto interpret natural language and use these tools. Similar to AutoGPT, it follows a single-agent\\nparadigm and does not support agent collaboration.\\nAutoGen differs from the single-agent systems above by supporting multi-agent LLM applications.\\nMulti-Agent Systems:\\n‚Ä¢BabyAGI : BabyAGI (BabyAGI, 2023) is an example implementation of an AI-powered task man-\\nagement system in a Python script. In this implemented system, multiple LLM-based agents\\nare used. For example, there is an agent for creating new tasks based on the objective and the\\nresult of the previous task, an agent for prioritizing the task list, and an agent for completing\\ntasks/sub-tasks. As a multi-agent system, BabyAGI adopts a static agent conversation pattern,\\ni.e., a predefined order of agent communication, while AutoGen supports both static and dynamic\\nconversation patterns and additionally supports tool usage and human involvement.\\n‚Ä¢CAMEL : CAMEL (Li et al., 2023b) is a communicative agent framework. It demonstrates\\nhow role playing can be used to let chat agents communicate with each other for task comple-\\ntion. It also records agent conversations for behavior analysis and capability understanding. An\\nInception-prompting technique is used to achieve autonomous cooperation between agents. Un-\\nlikeAutoGen , CAMEL does not natively support tool usage, such as code execution. Although it\\nis proposed as an infrastructure for multi-agent conversation, it only supports static conversation\\npatterns, while AutoGen additionally supports dynamic conversation patterns.\\n‚Ä¢Multi-Agent Debate: Two recent works investigate and show that multi-agent debate is an effec-\\ntive way to encourage divergent thinking in LLMs (Liang et al., 2023) and to improve the factuality\\nand reasoning of LLMs (Du et al., 2023). In both works, multiple LLM inference instances are\\nconstructed as multiple agents to solve problems with agent debate. Each agent is simply an LLM\\ninference instance, while no tool or human is involved, and the inter-agent conversation needs\\nto follow a pre-defined order. These works attempt to build LLM applications with multi-agent\\nconversation, while AutoGen , designed as a generic infrastructure, can be used to facilitate this\\ndevelopment and enable more applications with dynamic conversation patterns.\\n14', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 13}),\n",
       " Document(page_content='‚Ä¢MetaGPT : MetaGPT (Hong et al., 2023) is a specialized LLM application based on a multi-agent\\nconversation framework for automatic software development. They assign different roles to GPTs\\nto collaboratively develop software. They differ from AutoGen by being specialized solutions to\\na certain scenario, while AutoGen is a generic infrastructure to facilitate building applications for\\nvarious scenarios.\\nThere are a few other specialized single-agent or multi-agent systems, such as V oyager (Wang et al.,\\n2023a) and Generative Agents (Park et al., 2023), which we skip due to lower relevance. In Table 1,\\nwe summarize differences between AutoGen and the most relevant multi-agent systems.\\nTable 1: Summary of differences between AutoGen and other related multi-agent systems. infras-\\ntructure : whether the system is designed as a generic infrastructure for building LLM applications.\\nconversation pattern : the types of patterns supported by the implemented systems. Under the\\n‚Äòstatic‚Äô pattern, agent topology remains unchanged regardless of different inputs. AutoGen allows\\nflexible conversation patterns, including both static and dynamic patterns that can be customized\\nbased on different application needs. execution-capable : whether the system can execute LLM-\\ngenerated code; human involvement : whether (and how) the system allows human participation\\nduring the execution process of the system. AutoGen allows flexible human involvement in multi-\\nagent conversation with the option for humans to skip providing inputs.\\nAspect AutoGen Multi-agent Debate CAMEL BabyAGI MetaGPT\\nInfrastructure ‚úì ‚úó ‚úì ‚úó ‚úó\\nConversation pattern flexible static static static static\\nExecution-capable ‚úì ‚úó ‚úó ‚úó ‚úì\\nHuman involvement chat/skip ‚úó ‚úó ‚úó ‚úó\\n15', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 14}),\n",
       " Document(page_content='B Expanded Discussion\\nThe applications in Section 3 show how AutoGen not only enables new applications but also helps\\nrenovate existing ones. For example, in A1 (scenario 3), A5, and A6, AutoGen enabled the cre-\\nation of multi-agent conversations that follow a dynamic pattern instead of a fixed back-and-forth.\\nAnd in both A5 and A6, humans can participate in the activities together with multiple other AI\\nagents in a conversational manner. Similarly, A1-A4 show how popular applications can be reno-\\nvated quickly with AutoGen . Despite the complexity of these applications (most of them involve\\nmore than two agents or dynamic multi-turn agent cooperation), our AutoGen -based implementa-\\ntion remains simple, demonstrating promising opportunities to build creative applications and a large\\nspace for innovation. In reflecting on why these benefits can be achieved in these applications with\\nAutoGen , we believe there are a few reasons:\\n‚Ä¢Ease of use : The built-in agents can be used out-of-the-box, delivering strong performance even\\nwithout any customization. (A1, A3)\\n‚Ä¢Modularity : The division of tasks into separate agents promotes modularity in the system. Each\\nagent can be developed, tested, and maintained independently, simplifying the overall develop-\\nment process and facilitating code management. (A3, A4, A5, and A6)\\n‚Ä¢Programmability: AutoGen allows users to extend/customize existing agents to develop systems\\nsatisfying their specific needs with ease. (A1-A6). For example, with AutoGen , the core workflow\\ncode in A4 is reduced from over 430 lines to 100 lines, for a 4x saving.\\n‚Ä¢Allowing human involvement :AutoGen provides a native mechanism to achieve human partici-\\npation and/or human oversight. With AutoGen , humans can seamlessly and optionally cooperate\\nwith AIs to solve problems or generally participate in the activity. AutoGen also facilitates inter-\\nactive user instructions to ensure the process stays on the desired path. (A1, A2, A5, and A6)\\n‚Ä¢Collaborative/adversarial agent interactions : Like many collaborative agent systems (Dong\\net al., 2023), agents in AutoGen can share information and knowledge, to complement each other‚Äôs\\nabilities and collectively arrive at better solutions. (A1, A2, A3, and A4). Analogously, in certain\\nscenarios, some agents are required to work in an adversarial way. Relevant information is shared\\namong different conversations in a controlled manner, preventing distraction or hallucination. (A4,\\nA6). AutoGen supports both patterns, enabling effective utilization and augmentation of LLMs.\\nB.1 General Guidelines for Using AutoGen\\nBelow we give some recommendations for using agents in AutoGen to accomplish a task.\\n1.Consider using built-in agents first. For example, AssistantAgent is pre-configured to be\\nbacked by GPT-4, with a carefully designed system message for generic problem-solving via\\ncode. The UserProxyAgent is configured to solicit human inputs and perform tool execution.\\nMany problems can be solved by simply combining these two agents. When customizing agents\\nfor an application, consider the following options: (1) human input mode, termination condition,\\ncode execution configuration, and LLM configuration can be specified when constructing an\\nagent; (2) AutoGen supports adding instructions in an initial user message, which is an effective\\nway to boost performance without needing to modify the system message; (3) UserProxyAgent\\ncan be extended to handle different execution environments and exceptions, etc.; (4) when sys-\\ntem message modification is needed, consider leveraging the LLM‚Äôs capability to program its\\nconversation flow with natural language.\\n2.Start with a simple conversation topology . Consider using the two-agent chat or the group chat\\nsetup first, as they can often be extended with the least code. Note that the two-agent chat can\\nbe easily extended to involve more than two agents by using LLM-consumable functions in a\\ndynamic way.\\n3. Try to reuse built-in reply methods based on LLM, tool, or human before implementing a\\ncustom reply method because they can often be reused to achieve the goal in a simple way\\n(e.g., the built-in agent GroupChatManager ‚Äôs reply method reuses the built-in LLM-based reply\\nfunction when selecting the next speaker, ref. A5 in Section 3).\\n4. When developing a new application with UserProxyAgent ,start with humans always in\\nthe loop , i.e., human input mode=‚ÄòALWAYS‚Äô, even if the target operation mode is more au-\\ntonomous. This helps evaluate the effectiveness of AssistantAgent , tuning the prompt, dis-\\ncovering corner cases, and debugging. Once confident with small-scale success, consider setting\\n16', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 15}),\n",
       " Document(page_content='human input mode = ‚ÄòNEVER‚Äô. This enables LLM as a backend, and one can either use the\\nLLM or manually generate diverse system messages to simulate different use cases.\\n5. Despite the numerous advantages of AutoGen agents, there could be cases/scenarios where other\\nlibraries/packages could help . For example: (1) For (sub)tasks that do not have requirements\\nfor back-and-forth trouble-shooting, multi-agent interaction, etc., a unidirectional (no back-and-\\nforth message exchange) pipeline can also be orchestrated with LangChain (LangChain, 2023),\\nLlamaIndex (Liu, 2022), Guidance (Guidance, 2023), Semantic Kernel (Semantic-Kernel, 2023),\\nGorilla (Patil et al., 2023) or low-level inference API (‚Äòautogen.oai‚Äô provides an enhanced LLM\\ninference layer at this level) (Dibia, 2023). (2) When existing tools from LangChain etc. are\\nhelpful, one can use them as tool backends for AutoGen agents. For example, one can readily use\\ntools, e.g., Wolfram Alpha, from LangChain in AutoGen agent. (3) For specific applications, one\\nmay want to leverage agents implemented in other libraries/packages. To achieve this, one could\\nwrap those agents as conversable agents in AutoGen and then use them to build LLM applications\\nthrough multi-agent conversation. (4) It can be hard to find an optimal operating point among\\nmany tunable choices, such as the LLM inference configuration. Blackbox optimization packages\\nlike ‚Äòflaml.tune‚Äô (Wang et al., 2021) can be used together with AutoGen to automate such tuning.\\nB.2 Future Work\\nThis work raises many research questions and future directions and .\\nDesigning optimal multi-agent workflows: Creating a multi-agent workflow for a given task can\\ninvolve many decisions, e.g., how many agents to include, how to assign agent roles and agent\\ncapabilities, how the agents should interact with each other, and whether to automate a particular\\npart of the workflow. There may not exist a one-fits-all answer, and the best solution might depend\\non the specific application. This raises important questions: For what types of tasks and applications\\nare multi-agent workflows most useful? How do multiple agents help in different applications? For\\na given task, what is the optimal (e.g., cost-effective) multi-agent workflow?\\nCreating highly capable agents: AutoGen can enable the development of highly capable agents\\nthat leverage the strengths of LLMs, tools, and humans. Creating such agents is crucial to ensuring\\nthat a multi-agent workflow can effectively troubleshoot and make progress on a task. For example,\\nwe observed that CAMEL, another multi-agent LLM system, cannot effectively solve problems in\\nmost cases primarily because it lacks the capability to execute tools or code. This failure shows that\\nLLMs and multi-agent conversations with simple role playing are insufficient, and highly capable\\nagents with diverse skill sets are essential. We believe that more systematic work will be required to\\ndevelop guidelines for application-specific agents, to create a large OSS knowledge base of agents,\\nand to create agents that can discover and upgrade their skills (Cai et al., 2023).\\nEnabling scale, safety, and human agency: Section 3 shows how complex multi-agent workflows\\ncan enable new applications, and future work will be needed to assess whether scaling further can\\nhelp solve extremely complex tasks. However, as these workflows scale and grow more complex,\\nit may become difficult to log and adjust them. Thus, it will become essential to develop clear\\nmechanisms and tools to track and debug their behavior. Otherwise, these techniques risk resulting\\nin incomprehensible, unintelligible chatter among agents (Lewis et al., 2017).\\nOur work also shows how complex, fully autonomous workflows with AutoGen can be useful, but\\nfully autonomous agent conversations will need to be used with care. While the autonomous mode\\nAutoGen supports could be desirable in many scenarios, a high level of autonomy can also pose\\npotential risks, especially in high-risk applications (Amodei et al., 2016; Weld & Etzioni, 1994). As\\na result, building fail-safes against cascading failures and exploitation, mitigating reward hacking,\\nout of control and undesired behaviors, maintaining effective human oversight of applications built\\nwith AutoGen agents will become important. While AutoGen provides convenient and seamless\\ninvolvement of humans through a user proxy agent, developers and stakeholders still need to under-\\nstand and determine the appropriate level and pattern of human involvement to ensure the safe and\\nethical use of the technology (Horvitz, 1999; Amershi et al., 2019).\\n17', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 16}),\n",
       " Document(page_content='C Default System Message for Assistant Agent\\nSystemMessageYou are a helpful AI assistant. Solve tasks using your coding and language skills.In the following cases, suggest python code (in a python coding block) or shell script (in a shcoding block) for the user to execute.1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.2. When you need to perform some task with code, use the code to perform the task and output theresult. Finish the task smartly.Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can‚Äôt modify your code. So do not suggest incomplete code which requires users to modify. Don‚Äôt use a code block if it‚Äôs not intended to be executed by the user.If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don‚Äôt include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use ‚Äôprint‚Äô function for the output when relevant. Check the execution result returned by the user.If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can‚Äôt be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.Reply ‚ÄúTERMINATE‚Äù in the end when everything is done.Prompting techniques color code: Role Play; Control Flow; Output Confine; Facilitate Automation; Grounding\\nFigure 5: Default system message for the built-in assistant agent in AutoGen (v0.1.1). This is an\\nexample of conversation programming via natural language. It contains instructions of different\\ntypes, including role play, control flow, output confine, facilitate automation, and grounding.\\nFigure 5 shows the default system message for the built-in assistant agent in AutoGen (v0.1.1),\\nwhere we introduce several new prompting techniques and highlight them accordingly. When com-\\nbining these new prompting techniques together, we can program a fairly complex conversation even\\nwith the simplest two-agent conversation topology. This approach tries to exploit the capability of\\nLLMs in implicit state inference to a large degree. LLMs do not follow all the instructions perfectly,\\nso the design of the system needs to have other mechanisms to handle the exceptions and faults.\\nSome instructions can have ambiguities, and the designer should either reduce them for preciseness\\nor intentionally keep them for flexibility and address the different situations in other agents. In\\ngeneral, we observe that GPT-4 follows the instructions better than GPT-3.5-turbo.\\n18', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 17}),\n",
       " Document(page_content='D Application Details\\nA1: Math Problem Solving\\nScenario 1: Autonomous Problem Solving. We perform both qualitative and quantitative eval-\\nuations in this scenario. For all evaluations, we use GPT-4 as the base model, and pre-install the\\n‚Äúsympy‚Äù package in the execution environment. We compare AutoGen with the following LLM-\\nbased agent systems:\\n‚Ä¢ AutoGPT: The out-of-box AutoGPT is used. We initialize AutoGPT by setting the purpose to\\n‚Äúsolve math problems‚Äù, resulting in a ‚ÄúMathSolverGPT‚Äù with auto-generated goals.\\n‚Ä¢ ChatGPT+Plugin: We enable the Wolfram Alpha plugin (a math computation engine) in the Ope-\\nnAI web client.\\n‚Ä¢ ChatGPT+Code Interpreter: This is a recent feature in OpenAI web client. Note that the above\\ntwo premium features from ChatGPT require a paid subscription to be accessed and are the most\\ncompetitive commercial systems.\\n‚Ä¢ LangChain ReAct+Python: We use Python agent from LangChain. To handle parsing errors, we\\nset ‚Äúhandle parsing errors=True‚Äù, and use the default zero-shot ReAct prompt.\\n‚Ä¢ Multi-Agent Debate (Liang et al., 2023): We modified the code of the multi-agent debate to per-\\nform evaluation. By default, there are three agents: an affirmative agent, a negative agent, and a\\nmoderator.\\nWe also conducted preliminary evaluations on several other multi-agent systems, including\\nBabyAGI, CAMEL, and MetaGPT. The results indicate that they are not suitable choices for solving\\nmath problems out of the box. For instance, when MetaGPT is tasked with solving a math problem,\\nit begins developing software to address the problem, but most of the time, it does not actually solve\\nthe problem. We have included the test examples in Appendix E.\\nTable 2: Qualitative evaluation of two math problems from the MATH dataset within the autonomous\\nproblem-solving scenario. Each LLM-based system is tested three times on each of the problems.\\nThis table reports the problem-solving correctness and summarizes the reasons for failure.\\nCorrectness Failure Reason\\nAutoGen 3/3 N/A.\\nAutoGPT 0/3 The LLM gives code without the print function so the\\nresult is not printed.\\nChatGPT+Plugin 1/3 The return from Wolfram Alpha contains 2 simplified\\nresults, including the correct answer, but GPT-4 always\\nchooses the wrong answer.\\nChatGPT+Code Interpreter 2/3 Returns a wrong decimal result.\\nLangChain ReAct 0/3 LangChain gives 3 different wrong answers.\\nMulti-Agent Debate 0/3 It gives 3 different wrong answers due to calculation errors.\\n(a) Evaluation on the first problem that asks to simplify a square root fraction.\\nCorrectness Failure Reason\\nAutoGen 2/3 The final answer from code execution is wrong.\\nAutoGPT 0/3 The LLM gives code without the print function so the\\nresult is not printed.\\nChatGPT+Plugin 1/3 For one trial, GPT-4 got stuck because it keeps giving\\nwrong queries and has to be stopped. Another trial simply\\ngives a wrong answer.\\nChatGPT+Code Interpreter 0/3 It gives 3 different wrong answers.\\nLangChain ReAct 0/3 LangChain gives 3 different wrong answers.\\nMulti-Agent Debate 0/3 It gives 3 different wrong answers.\\n(b) Evaluation on the second number theory problem.\\nFor the qualitative evaluation, we utilize two level-5 problems from the MATH dataset, testing each\\nproblem three times. The first problem involves simplifying a square root fraction, and the second\\n19', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 18}),\n",
       " Document(page_content='problem involves solving a number theory issue. The correctness counts and reasons for failure\\nare detailed in Table 2. For the quantitative evaluation, we conduct two sets of experiments on\\nthe MATH dataset to assess the correctness of these systems: (1) an experiment involving 120\\nlevel-5 (the most challenging level) problems, including 20 problems from six categories, excluding\\ngeometry, and (2) an experiment on the entire test set, which includes 5000 problems. We exclude\\nAutoGPT from this evaluation as it cannot access results from code executions and does not solve\\nany problems in the qualitative evaluation. Our analysis of the entire dataset reveals that AutoGen\\nachieves an overall accuracy of 69.48%, while GPT-4‚Äôs accuracy stands at 55.18%. From these\\nevaluations, we have the following observations regarding the problem-solving success rate and\\nuser experience of these systems:\\n‚Ä¢ Problem-solving success rate: Results from the quantitative evaluations show that AutoGen can\\nhelp achieve the highest problem-solving success rate among all the compared methods. The qual-\\nitative evaluations elucidate common failure reasons across several alternative approaches. Chat-\\nGPT+Code Interpreter fails to solve the second problem, and ChatGPT+Plugin struggles to solve\\nboth problems. AutoGPT fails on both problems due to code execution issues. The LangChain\\nagent also fails on both problems, producing code that results in incorrect answers in all trials.\\n‚Ä¢ Based on the qualitative evaluation, we analyze the user experience concerning the verbosity of\\nthe response and the ability of the LLM-based system to run without unexpected behaviors. Chat-\\nGPT+Plugin is the least verbose, mainly because Wolfram queries are much shorter than Python\\ncode. AutoGen , ChatGPT+Code Interpreter, and LangChain exhibit similar verbosity, although\\nLangChain is slightly more verbose due to more code execution errors. AutoGPT is the most\\nverbose system owing to predefined steps like THOUGHTS, REASONING, and PLAN, which it\\nincludes in replies every time. Overall, AutoGen and ChatGPT+Code Interpreter operate smoothly\\nwithout exceptions. We note the occurrences of undesired behaviors from other LLM-based sys-\\ntems that could affect user experience: AutoGPT consistently outputs code without the print‚Äô\\nstatement and cannot correct this, requiring the user to run them manually; ChatGPT with Wol-\\nfram Alpha plugin has the potential to become stuck in a loop that must be manually stopped; and\\nLangchain ReAct could exit with a parse error, necessitating the passing of a ‚Äòhandle parse error‚Äô\\nparameter.\\nEnable Multi-User Problem Solving ViaStudent        and Expert Student Proxy\\nStudentAssistant\\nExpertAssistant\\nExpert Proxy\\nAsk for expert\\nEnable Autonomous and Human-in-the-loop Problem Solving\\nFigure 6: Examples of three settings utilized to solve math problems using AutoGen : (Gray) En-\\nables a workflow where a student collaborates with an assistant agent to solve problems, either\\nautonomously or in a human-in-the-loop mode. (Gray + Orange) Facilitates a more sophisticated\\nworkflow wherein the assistant, on the fly, can engage another user termed ‚Äúexpert‚Äù, who is in the\\nloop with their own assistant agent, to aid in problem-solving if its own solutions are not satisfactory.\\nScenario 2: Human-in-the-loop Problem Solving. For challenging problems that these LLM\\nsystems cannot solve autonomously, human feedback during the problem-solving process can be\\n20', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 19}),\n",
       " Document(page_content='helpful. To incorporate human feedback with AutoGen , one can set human input mode=‚ÄòALWAYS‚Äô\\nin the user proxy agent. We select one challenging problem that none of these systems can solve\\nautonomously across three trials. We adhere to the process outlined below to provide human inputs\\nfor all the compared methods:\\n1. Input the problem: Find the equation of the plane which bisects the angle\\nbetween the planes 3x‚àí6y+ 2z+ 5 = 0 and4x‚àí12y+ 3z‚àí3 = 0 ,and which\\ncontains the point (‚àí5,‚àí1,‚àí5).Enter your answer in the form\\nAx+By+Cz+D= 0,\\nwhere A, B, C, D are integers such that A > 0and\\ngcd(|A|,|B|,|C|,|D|) = 1.\\n2. The response from the system does not solve the problem correctly. We then give a\\nhint to the model: Your idea is not correct. Let‚Äôs solve this together.\\nSuppose P= (x, y, z )is a point that lies on a plane that bisects the\\nangle, the distance from P to the two planes is the same. Please\\nset up this equation first.\\n3. We expect the system to give the correct distance equation. Since the equation involves\\nan absolute sign that is hard to solve, we would give the next hint: Consider the two\\ncases to remove the abs sign and get two possible solutions.\\n4. If the system returns the two possible solutions and doesn‚Äôt continue to the next step, we\\ngive the last hint: Use point (-5,-1,-5) to determine which is correct and\\ngive the final answer.\\n5. Final answer is 11x+6y+5z+86=0 .\\nWe observed that AutoGen consistently solved the problem across all three trials. ChatGPT+Code\\nInterpreter and ChatGPT+Plugin managed to solve the problem in two out of three trials, while Au-\\ntoGPT failed to solve it in all three attempts. In its unsuccessful attempt, ChatGPT+Code Interpreter\\nfailed to adhere to human hints. In its failed trial, ChatGPT+Plugin produced an almost correct solu-\\ntion but had a sign discrepancy in the final answer. AutoGPT was unable to yield a correct solution\\nin any of the trials. In one trial, it derived an incorrect distance equation. In the other two trials, the\\nfinal answer was incorrect due to code execution errors.\\nScenario 3: Multi-User Problem Solving. Next-generation LLM applications may necessitate\\nthe involvement of multiple real users for collectively solving a problem with the assistance of\\nLLMs. We showcase how AutoGen can be leveraged to effortlessly construct such a system. Specif-\\nically, building upon scenario 2 mentioned above, we aim to devise a simple system involving two\\nhuman users: a student and an expert. In this setup, the student interacts with an LLM assistant to\\naddress some problems, and the LLM automatically resorts to the expert when necessary.\\nThe overall workflow is as follows: The student chats with the LLM-based assistant agent through\\na student proxy agent to solve problems. When the assistant cannot solve the problem satisfactorily,\\nor the solution does not match the expectation of the student, it would automatically hold the con-\\nversation and call the pre-defined askforexpert function via the function callfeature of GPT\\nin order to resort to the expert. Specifically, it would automatically produce the initial message for\\ntheaskforexpert function, which could be the statement of the problem or the request to verify\\nthe solution to a problem, and the expert is supposed to respond to this message with the help of\\nthe expert assistant. After the conversation between the expert and the expert‚Äôs assistant, the final\\nmessage would be sent back to the student assistant as the response to the initial message. Then, the\\nstudent assistant would resume the conversation with the student using the response from the expert\\nfor a better solution. A detailed visualization is shown in Figure 6.\\nWith AutoGen , constructing the student/expert proxy agent and the assistant agents is straight-\\nforward by reusing the built-in UserProxyAgent andAssistantAgent through appropriate\\nconfigurations. The only development required involves writing several lines of code for the\\naskforexpert function, which then becomes part of the configuration for the assistant. Ad-\\nditionally, it‚Äôs easy to extend such a system to include more than one expert, with a specific\\naskforexpert function for each, or to include multiple student users with a shared expert for\\nconsultation.\\n21', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 20}),\n",
       " Document(page_content='A2: Retrieval-Augmented Code Generation and Question Answering\\nRetrieval-augmentedAssistantRetrieval-augmented  User Proxy\\n1. Question and Contexts3. Terminate,feedbacks or `Update Context`4. Satisfied Answers or Terminate\\n2. Satisfied Answers or `Update Context`\\nFigure 7: Overview of Retrieval-augmented Chat which involves two agents, including a Retrieval-\\naugmented User Proxy and a Retrieval-augmented Assistant. Given a set of documents, the\\nRetrieval-augmented User Proxy first automatically processes documents‚Äîsplits, chunks, and stores\\nthem in a vector database. Then for a given user input, it retrieves relevant chunks as context and\\nsends it to the Retrieval-augmented Assistant, which uses LLM to generate code or text to answer\\nquestions. Agents converse until they find a satisfactory answer.\\nDetailed Workflow. The workflow of Retrieval-Augmented Chat is illustrated in Figure 7. To\\nuse Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented\\nUser Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy\\nnecessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented\\nUser Proxy can download the documents, segment them into chunks of a specific size, compute\\nembeddings, and store them in a vector database. Once a chat is initiated, the agents collaboratively\\nengage in code generation or question-answering adhering to the procedures outlined below:\\n1. The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding simi-\\nlarity, and sends them along with the question to the Retrieval-Augmented Assistant.\\n2. The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based\\non the question and context provided. If the LLM is unable to produce a satisfactory response, it\\nis instructed to reply with ‚ÄúUpdate Context‚Äù to the Retrieval-Augmented User Proxy.\\n3. If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and\\nsends the output as feedback. If there are no code blocks or instructions to update the context, it\\nterminates the conversation. Otherwise, it updates the context and forwards the question along\\nwith the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation\\nis enabled, individuals can proactively send any feedback, including Update Context‚Äù, to the\\nRetrieval-Augmented Assistant.\\n4. If the Retrieval-Augmented Assistant receives ‚ÄúUpdate Context‚Äù, it requests the next most similar\\nchunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it\\ngenerates new code or text based on the feedback and chat history. If the LLM fails to generate\\nan answer, it replies with ‚ÄúUpdate Context‚Äù again. This process can be repeated several times.\\nThe conversation terminates if no more documents are available for the context.\\nWe utilize Retrieval-Augmented Chat in two scenarios. The first scenario aids in generating code\\nbased on a given codebase. While LLMs possess strong coding abilities, they are unable to utilize\\npackages or APIs that are not included in their training data, e.g., private codebases, or have trouble\\nusing trained ones that are frequently updated post-training. Hence, Retrieval-Augmented Code\\nGeneration is considered to be highly valuable. The second scenario involves question-answering\\non the Natural Questions dataset (Kwiatkowski et al., 2019), enabling us to obtain comparative\\nevaluation metrics for the performance of our system.\\nScenario 1: Evaluation on Natural Questions QA dataset. In this case, we evaluate the\\nRetrieval-Augmented Chat‚Äôs end-to-end question-answering performance using the Natural Ques-\\ntions dataset (Kwiatkowski et al., 2019). We collected 5,332 non-redundant context documents and\\n6,775 queries from HuggingFace. First, we create a document collection based on the entire context\\ncorpus and store it in the vector database. Then, we utilize Retrieval-Augmented Chat to answer the\\nquestions. An example (Figure 8) from the NQ dataset showcases the advantages of the interactive\\nretrieval feature: ‚Äúwho carried the usa flag in opening ceremony‚Äù . When attempting to answer\\nthis question, the context with the highest similarity to the question embedding does not contain the\\n22', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 21}),\n",
       " Document(page_content='Task description who carried the usaflag in opening ceremony?Assistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply exactly ‚ÄòUPDATE CONTEXT‚Äô. You must give as short an answer as possible.User‚Äôs question is: who carried the usaflag in opening ceremonyContext is: < P > On January 17 , 1899 , under ... The Astros beat the Yankees that night , 2 -1 . < /P >UserProxy: Sorry, I cannot find any information about who carried the USA flag in the opening ceremony. UPDATE CONTEXT.Assistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply exactly ‚ÄòUPDATE CONTEXT‚Äô. You must give as short an answer as possible.User‚Äôs question is: who carried the usaflag in opening ceremonyContext is: < T able > ... Erin Hamlin ... < /T d >UserProxy: Erin Hamlin carried the USA flag in the opening ceremony. \\n(a) W/O Interactive Retrieval\\n(b) W/ Interactive Retrieval\\nAssistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply ‚Äòsorry, I don‚Äôt know‚Äô. You must give as short an answer as possible.User‚Äôs question is:who carried the usaflag in opening ceremonyContext is: < P > On January 17 , 1899 , under ... The Astros beat the Yankees that night , 2 -1 . < /P >UserProxy: Sorry, I cannot find any information about who carried the USA flag in the opening ceremony. Sorry, I don‚Äôt know.Figure 8: Retrieval-augmented Chat without (W/O) and with (W/) interactive retrieval .\\nrequired information for a response. As a result, the LLM assistant (GPT-3.5-turbo) replies ‚ÄúSorry,\\nI cannot find any information about who carried the USA flag in the opening ceremony. UPDATE\\nCONTEXT. ‚Äù With the unique and innovative ability to update context in Retrieval-Augmented Chat,\\nthe user proxy agent automatically updates the context and forwards it to the assistant agent again.\\nFollowing this process, the agent is able to generate the correct answer to the question.\\nIn addition, we conduct an experiment using the same prompt as illustrated in (Adlakha et al., 2023)\\nto investigate the advantages of AutoGen W/O interactive retrieval . The F1 score and Recall for the\\nfirst 500 questions are 23.40% and 62.60%, respectively, aligning closely with the results reported\\nin Figure 4b. Consequently, we assert that AutoGen W/O interactive retrieval outperforms DPR due\\nto differences in the retrievers employed. Specifically, we utilize a straightforward vector search\\nretriever with the all-MiniLM-L6-v2 model for embeddings.\\nFurthermore, we analyze the number of LLM calls in experiments involving both AutoGen and\\nAutoGen W/O interactive retrieval , revealing that approximately 19.4% of questions in the Natural\\nQuestions dataset trigger an ‚ÄúUpdate Context‚Äù operation, resulting in additional LLM calls.\\nScenario 2: Code Generation Leveraging Latest APIs from the Codebase. In this case, the ques-\\ntion is ‚ÄúHow can I use FLAML to perform a classification task and use Spark for parallel training?\\nTrain for 30 seconds and force cancel jobs if the time limit is reached. ‚Äù . FLAML (v1) (Wang et al.,\\n2021) is an open-source Python library designed for efficient AutoML and tuning. It was open-\\nsourced in December 2020, and is included in the training data of GPT-4. However, the question\\nnecessitates the use of Spark-related APIs, which were added in December 2022 and are not encom-\\npassed in the GPT-4 training data. Consequently, the original GPT-4 model is unable to generate the\\ncorrect code, due to its lack of knowledge regarding Spark-related APIs. Instead, it erroneously cre-\\nates a non-existent parameter, spark , and sets it to True‚Äô. Nevertheless, with Retrieval-Augmented\\nChat, we provide the latest reference documents as context. Then, GPT-4 generates the correct code\\nblocks by setting usespark andforce cancel to True‚Äô.\\n23', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 22}),\n",
       " Document(page_content='A3: Decision Making in Text World Environments\\nALFWorld\\nExecutor\\nReward & StateAction Decision\\nAssistant\\nObservation: On the desk 2, you see an alarmclock 3, \\na bowl 3, a creditcard 2, a mug 1, and a pencil 2.Action decision: Pick up pencil 2 from desk 2\\nGroundingAgent\\nALFChat (two agents) ALFChat (three agents)\\nALFWorld Executor\\nAssistant\\nFigure 9: We use AutoGen to solve tasks in the ALFWorld benchmark, which contains household\\ntasks described in natural language. We propose two designs: a two-agent design where the assistant\\nagent suggests the next step, and the Executor executes actions and provides feedback. The three-\\nagent design adds a grounding agent that supplies commonsense facts to the executor when needed.\\nALFWorld (Shridhar et al., 2021) is a synthetic language-based interactive decision-making task.\\nIt comprises textual environments that aim to simulate real-world household scenes. Given a high-\\nlevel goal (e.g., putting a hot apple in the fridge) and the description of the household environment,\\nthe agent needs to explore and interact with the simulated household environment through a textual\\ninterface. A typical task environment contains various types of locations and could require more\\nthan 40 steps to finish, which highlights the need for agents to decompose the goal into subtasks and\\ntackle them one by one, while effectively exploring the environments.\\nDetailed Workflow. We first propose a straightforward two-agent system with AutoGen , illustrated\\non the left-hand side of Figure 9, to tackle tasks from this benchmark. The system consists of\\nan assistant agent and an executor agent. The assistant agent generates plans and makes action\\ndecisions to solve the tasks. The executor agent is tailored specifically for ALFWorld. It performs\\nactions proposed by the assistant and reports action execution results in the household environment\\nas feedback to the assistant. Due to the strict format requirements for the output format, we use the\\nBLEU metric to evaluate the similarity of the output to all valid action options. The option with the\\nhighest similarity will be chosen as the action for this round.\\nOne major challenge encompassed in ALFWorld is commonsense reasoning. The agent needs to\\nextract patterns from the few-shot examples provided and combine them with the agent‚Äôs general\\nknowledge of household environments to fully understand task rules. More often than not, the as-\\nsistant tends to neglect some basic knowledge of the household environment. Thanks to the easy-to-\\nimplement multi-agent conversational feature of AutoGen , enhancing the assistant agent‚Äôs reason-\\ning ability by adding a new grounding agent to provide commonsense facts for the decision-making\\nagent‚Äôs reference becomes straightforward. By scrutinizing the failed attempts and summarizing the\\nreasons for failure, we obtained a holistic understanding of the commonsense knowledge that the\\nassistant agent lacks. Then, we set a grounding agent to provide this general knowledge when the\\ntask begins and whenever the assistant outputs the same action three times in a row. This ensures the\\nassistant takes this commonsense knowledge into consideration and prevents it from getting stuck in\\noutputting the same content or constantly apologizing.\\nWe compare our system‚Äôs performance with ReAct, which treats ALFWorld as a text-completion\\ntask. ReAct (Yao et al., 2022) is a few-shot prompting technique that interleaves reasoning and\\nacting, allowing for greater synergy between the two and significantly improving performance on\\nboth language and decision-making tasks. We integrate ReAct into AutoGen by modifying the\\nprompts in a conversational manner. Following ReAct, we employ a two-shot setting. The few-\\nshot prompts are obtained from the corresponding repository. As shown in Table 3, the two-agent\\n24', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 23}),\n",
       " Document(page_content=\"Task description\\nYou are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a safe 1, a drawer 2, a dra wer1, a \\nshelf 3, a shelf 2, and a shelf 1.\\nYour task is to: look at bowl under the desklamp . \\nAssistant (to Executor) : THOUGHT [First, I need to find a bowl. A \\nbowl is more likely to appear in desk (1 -2), drawer (1 -2), shelf \\n(1-3), bed (1). Then I need to find and use a desklamp .] ACTION \\n[go to desk 1]\\n(Assistant finds the desklamp at desk 1, then goes to desk 2.)\\nExecutor (to Assistant) : On the desk 2, you see a bowl 1, and a \\ncd 3... \\nAssistant (to Executor) : THOUGHT [Now I find a bowl (1). I need \\nto use the desklamp to look at the bowl. I'll go to the desklamp\\nnow.] ACTION [go to desk 1]\\nExecutor (to Assistant): On the desk 1, you see a desklamp 1, a \\npencil 2...\\nAssistant: ACTION [use desklamp 1]\\nExecutor (to Assistant) : You turn on the desklamp 1.\\nAssistant (to Executor): THOUGHT [Now that the desklamp is turned \\non, I can use it to look at the bowl (1) that I found earlier.] \\nACTION [look at bowl 1 under the desklamp 1]\\nExecutor (to Assistant) : You turn on the desklamp 1.\\n(Assistant falls into an infinite loop turning on desklamp .)\\nExecutor (to Assistant) : Task failed. Reply TERMINATE.\\nAssistant (to Executor) : TERMINATEAssistant (to Executor) : THOUGHT [First, I need to find a bowl. A \\nbowl is more likely to appear in desk (1 -2), drawer (1 -2), shelf \\n(1-3), bed (1). Then I need to find and use a desklamp .] ACTION \\n[go to desk 1]\\n(Assistant finds the desklamp at desk 1, then goes to desk 2.)\\n(Assistant falls into the same loop as (a))\\nGroundingAgent (to Executor) : You must find and take the object \\nbefore you can examine it. You must go to where the target \\nobject is before you can use it.\\nExecutor(to Assistant): You turn on the desklamp 1. Hint: You must \\nfind and take the object before you can examine it. \\nAssistant (to Executor): THOUGHT [I need to find and take the \\nbowl (1) before I can examine it. Let me find the bowl first.] \\nACTION [go to desk 2]\\nExecutor (to Assistant): On the desk 2, you see a bowl 1, a cd \\n3...\\nAssistant (to Executor) : ACTION [take bowl 1 from desk 2]\\nExecutor (to Assistant): You pick up bowl 1 from desk 2.\\nAssistant (to Executor): ACTION [go to desk 1]\\nAssistant (to Executor): ACTION[use desklamp 1]\\nExecutor (to Assistant) : Task success. Reply TERMINATE.\\nAssistant (to Executor) : TERMINATE\\n(a) Two agents\\n (b) Three agentsFigure 10: Comparison of results from two designs: (a) Two-agent design which consists of an\\nassistant and an executor, (b) Three-agent design which adds a grounding agent that serves as a\\nknowledge source. For simplicity, we omit the in-context examples and part of the exploration\\ntrajectory, and only show parts contributing to the failure/success of the attempt.\\nMethod Pick Clean Heat Cool Look Pick 2 All\\nReAct (avg) 63 52 48 71 61 24 54\\nALFChat (2 agents)(avg) 61 58 57 67 50 19 54\\nALFChat (3 agents)(avg) 79 64 70 76 78 41 69\\nReAct (best of 3) 75 62 61 81 78 35 66\\nALFChat (2 agents)(best of 3) 71 61 65 76 67 35 63\\nAFLChat (3 agents)(best of 3) 92 74 78 86 83 41 77\\nTable 3: Comparisons between ReAct and the two variants of ALFChat on the ALFWorld bench-\\nmark. For each task, we report the success rate out of 3 attempts. Success rate denotes the number\\nof tasks successfully completed by the agent divided by the total number of tasks. The results show\\nthat adding a grounding agent significantly improves the task success rate in ALFChat.\\ndesign matches the performance of ReAct, while the three-agent design significantly outperforms\\nReAct. We surmise that the performance discrepancy is caused by the inherent difference between\\ndialogue-completion and text-completion tasks. On the other hand, introducing a grounding agent\\nas a knowledge source remarkably advances performance on all types of tasks.\\nCase study . Figure 10 exemplifies how a three-agent design eliminates one root cause for failure\\ncases. Most of the tasks involve taking an object and then performing a specific action with it (e.g.,\\nfinding a vase and placing it on a cupboard). Without a grounding agent, the assistant frequently\\nconflates finding an object with taking it, as illustrated in Figure 10a). This leads to most of the\\nfailure cases in ‚Äôpick‚Äô and ‚Äôlook‚Äô type tasks. With the introduction of a grounding agent, the assistant\\ncan break out of this loop and successfully complete the task\\nTakeaways. We introduced a grounding agent to serve as an external commonsense knowledge\\nsource, which significantly enhanced the assistant‚Äôs ability to make informed decisions. This proves\\nthat providing necessary commonsense facts to the decision-making agent can assist it in making\\nmore informed decisions, thus effectively boosting the task success rate. AutoGen brings both\\nsimplicity and modularity when adding the grounding agent.\\n25\", metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 24}),\n",
       " Document(page_content='A4: Multi-Agent Coding\\nCommander\\nSafeguard\\nWriter\\n‚ë°Question, ‚ùªLog‚ù∏Code, ‚ë¶Ans‚ùπCode‚ù∫Clearance\\nUser‚ë†User Question‚ëßFinal AnswerRepeat until answering the user‚Äôs question or timeout\\nFigure 11: Our re-implementation of OptiGuide with AutoGen streamlining agents‚Äô interactions.\\nThe Commander receives user questions (e.g., What if we prohibit shipping from supplier 1 to\\nroastery 2?) and coordinates with the Writer and Safeguard. The Writer crafts the code and inter-\\npretation, the Safeguard ensures safety (e.g., not leaking information, no malicious code), and the\\nCommander executes the code. If issues arise, the process can repeat until resolved. Shaded circles\\nrepresent steps that may be repeated multiple times.\\nDetailed Workflow. The workflow can be described as follows. The end user initiates the in-\\nteraction by posing a question, such as ‚ÄúWhat if we prohibit shipping from supplier 1 to roastery\\n2?‚Äù, marked by\\n1 to the Commander agent. The Commander manages and coordinates with two\\nLLM-based assistant agents: the Writer and the Safeguard. Apart from directing the flow of commu-\\nnication, the Commander has the responsibility of handling memory tied to user interactions. This\\ncapability enables the Commander to capture and retain valuable context regarding the user‚Äôs ques-\\ntions and their corresponding responses. Such memory is subsequently shared across the system,\\nempowering the other agents with context from prior user interactions and ensuring more informed\\nand relevant responses.\\nIn this orchestrated process, the Writer, who combines the functions of a ‚ÄúCoder‚Äù and an ‚ÄúInter-\\npreter‚Äù as defined in (Li et al., 2023a), will craft code and also interpret execution output logs. For in-\\nstance, during code writing (\\n2 and\\n3 ), the Writer may craft code ‚Äúmodel.addConstr(x[‚Äòsupplier1‚Äô,\\n‚Äòroastery2‚Äô] == 0, ‚Äòprohibit‚Äô)‚Äù to add an additional constraint to answer the user‚Äôs question.\\nAfter receiving the code, the Commander will communicate with the Safeguard to screen the code\\nand ascertain its safety (\\n4 ); once the code obtains the Safeguard‚Äôs clearance, marked by\\n5 , the\\nCommander will use external tools (e.g., Python) to execute the code and request the Writer to\\ninterpret the execution results for the user‚Äôs question (\\n6 and\\n7 ). For instance, the writer may\\nsay ‚Äúif we prohibit shipping from supplier 1 to roastery 2, the total cost would increase by 10.5%.‚Äù\\nBringing this intricate process full circle, the Commander furnishes the user with the concluding\\nanswer (\\n8 ).\\nIf at a point there is an exception - either a security red flag raised by Safeguard (in\\n5 ) or code\\nexecution failures within Commander, the Commander redirects the issue back to the Writer with\\nessential information in logs (\\n6 ). So, the process from\\n3 to\\n6 might be repeated multiple times,\\nuntil each user query receives a thorough and satisfactory resolution or until the timeout. This entire\\ncomplex workflow of multi-agent interaction is elegantly managed via AutoGen .\\nThe core workflow code for OptiGuide was reduced from over 430 lines to 100 lines using AutoGen ,\\nleading to significant productivity improvement. The new agents are customizable, conversable, and\\ncan autonomously manage their chat memories. This consolidation allows the coder and interpreter\\nroles to merge into a single ‚ÄúWriter‚Äù agent, resulting in a clean, concise, and intuitive implementation\\nthat is easier to maintain.\\n26', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 25}),\n",
       " Document(page_content='Manual Evaluation Comparing ChatGPT + Code Interpreter and AutoGen -based OptiGuide.\\nChatGPT + Code Interpreter is unable to execute code with private or customized dependencies (e.g.,\\nGurobi), which means users need to have engineering expertise to manually handle multiple steps,\\ndisrupting the workflow and increasing the chance for mistakes. If users lack access or expertise,\\nthe burden falls on supporting engineers, increasing their on-call time.\\nWe carried out a user study that juxtaposed OpenAI‚Äôs ChatGPT coupled with a Code Interpreter\\nagainst AutoGen -based OptiGuide. The study focused on a coffee supply chain scenario, and an\\nexpert Python programmer with proficiency in Gurobi participated in the test. We evaluated both\\nsystems based on 10 randomly selected questions, measuring time and accuracy. While both sys-\\ntems answered 8 questions correctly, the Code Interpreter was significantly slower than OptiGuide\\nbecause the former requires more manual intervention. On average, users needed to spend 4 minutes\\nand 35 seconds to solve problems with the Code Interpreter, with a standard deviation of approxi-\\nmately 2.5 minutes. In contrast, OptiGuide‚Äôs average problem-solving time was around 1.5 minutes,\\nmost of which was spent waiting for responses from the GPT-4 model. This indicates a 3x saving\\non the user‚Äôs time with AutoGen -based OptiGuide.\\nWhile using ChatGPT + Code Interpreter, users had to read through the code and instructions to\\nknow where to paste the code snippets. Additionally, running the code involves downloading it and\\nexecuting it in a terminal, a process that was both time-consuming and prone to errors. The response\\ntime from the Code Interpreter is also slower, as it generates lots of tokens to read the code, read the\\nvariables line-by-line, perform chains of thought analysis, and then produce the final answer code.\\nIn contrast, AutoGen integrates multiple agents to reduce user interactions by 3 - 5 times on average\\nas reported in Table 4, where we evaluated our system with 2000 questions across five OptiGuide\\napplications and measured how many prompts the user needs to type.\\nTable 4: Manual effort saved with OptiGuide (W/ GPT-4) while preserving the same coding perfor-\\nmance is shown in the data below. The data include both the mean and standard deviations (indicated\\nin parentheses).\\nDataset netflow facility tsp coffee diet\\nSaving Ratio 3.14x (0.65) 3.14x (0.64) 4.88x (1.71) 3.38x (0.86) 3.03x (0.31)\\nTable 13 and 15 provide a detailed comparison of user experience with ChatGPT+Code Interpreter\\nandAutoGen -based OptiGuide. ChatGPT+Code Interpreter is unable to run code with private pack-\\nages or customized dependencies (such as Gurobi); as a consequence, ChatGPT+Code Interpreter\\nrequires users to have engineering expertise and to manually handle multiple steps, disrupting the\\nworkflow and increasing the chance for mistakes. If customers lack access or expertise, the bur-\\nden falls on supporting engineers, increasing their on-call time. In contrast, the automated chat by\\nAutoGen is more streamlined and autonomous, integrating multiple agents to solve problems and\\naddress concerns. This results in a 5x reduction in interaction and fundamentally changes the over-\\nall usability of the system. A stable workflow can be potentially reused for other applications or to\\ncompose a larger one.\\nTakeaways: The implementation of the multi-agent design with AutoGen in the OptiGuide appli-\\ncation offers several advantages. It simplifies the Python implementation and fosters a mixture of\\ncollaborative and adversarial problem-solving environments, with the Commander and Writer work-\\ning together while the Safeguard acts as a virtual adversarial checker. This setup allows for proper\\nmemory management, as the Commander maintains memory related to user interactions, provid-\\ning context-aware decision-making. Additionally, role-playing ensures that each agent‚Äôs memory\\nremains isolated, preventing shortcuts and hallucinations\\n27', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 26}),\n",
       " Document(page_content='A5: Dynamic Group Chat\\n3. Broadcast\\nAliceBobUser Proxy\\n1. Select a Speaker \\nAliceBobUser Proxy\\nBob2. Ask the Speaker to Respond\\nManager\\nManager\\nResponse\\nFigure 12: A5: Dynamic Group Chat: Overview of how AutoGen enables dynamic group chats to\\nsolve tasks. The Manager agent, which is an instance of the GroupChatManager class, performs\\nthe following three steps‚Äìselect a single speaker (in this case Bob), ask the speaker to respond, and\\nbroadcast the selected speaker‚Äôs message to all other agents\\nTo validate the necessity of multi-agent dynamic group chat and the effectiveness of the role-play\\nspeaker selection policy, we conducted a pilot study comparing a four-agent dynamic group chat\\nsystem with two possible alternatives across 12 manually crafted complex tasks. An example task is\\n‚ÄúHow much money would I earn if I bought 200 $AAPL stocks at the lowest price in the last 30 days\\nand sold them at the highest price? Save the results into a file. ‚Äù The four-agent group chat system\\ncomprised the following group members: a user proxy to take human inputs, an engineer to write\\ncode and fix bugs, a critic to review code and provide feedback, and a code executor for executing\\ncode. One of the possible alternatives is a two-agent system involving an LLM-based assistant and\\na user proxy agent, and another alternative is a group chat system with the same group members\\nbut a task-based speaker selection policy. In the task-based speaker selection policy, we simply ap-\\npend role information, chat history, and the next speaker‚Äôs task into a single prompt. Through the\\npilot study, we observed that compared with a task-style prompt, utilizing a role-play prompt in dy-\\nnamic speaker selection often leads to more effective consideration of both conversation context and\\nrole alignment during the process of generating the subsequent speaker, and consequently a higher\\nsuccess rate as reported in Table 5, fewer LLM calls and fewer termination failures, as reported in\\nTable 6.\\nTable 5: Number of successes on the 12 tasks (higher the better).\\nModel Two Agent Group Chat Group Chat with a task-based speaker selection policy\\nGPT-3.5-turbo 8 9 7\\nGPT-4 9 11 8\\nTable 6: Average # LLM calls and number of termination failures on the 12 tasks (lower the better).\\nModel Two Agent Group Chat Group Chat with a task-based speaker selection policy\\nGPT-3.5-turbo 9.9, 9 5.3, 0 4, 0\\nGPT-4 6.8, 3 4.5, 0 4, 0\\n28', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 27}),\n",
       " Document(page_content='Figure 13: Comparison of two-agent chat (a) and group chat (b) on a given task. The group chat\\nresolves the task successfully with a smoother conversation, while the two-agent chat fails on the\\nsame task and ends with a repeated conversation.\\n29', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 28}),\n",
       " Document(page_content='A6: Conversational Chess\\nChess BoardHuman/AI Chess Player A\\nHuman/AI Chess Player B\\nValidate moveValidate moveChallenging your pawn in the center. Your move.Developing my knightto a good square.Your move.\\nFigure 14: A6: Conversational Chess: Our conversational chess application can support various\\nscenarios, as each player can be an LLM-empowered AI, a human, or a hybrid of the two. Here,\\nthe board agent maintains the rules of the game and supports the players with information about the\\nboard. Players and the board agent all use natural language for communication.\\nIn Conversational Chess, each player is a AutoGen agent and can be powered either by a human or an\\nAI. A third party, known as the board agent, is designed to provide players with information about the\\nboard and ensure that players‚Äô moves adhere to legal chess moves. Figure 14 illustrates the scenarios\\nsupported by Conversational Chess: AI/human vs. AI/human, and demonstrates how players and the\\nboard agent interact. This setup fosters social interaction and allows players to express their moves\\ncreatively, employing jokes, meme references, and character-playing, thereby making chess games\\nmore entertaining for both players and observers (Figure 15 provides an example of conversational\\nchess).\\nTo realize these scenarios, we constructed a player agent with LLM and human as back-end options.\\nWhen human input is enabled, before sending the input to the board agent, it first prompts the human\\nplayer to input the message that contains the move along with anything else the player wants to say\\n(such as a witty comment). If human input is skipped or disabled, LLM is used to generate the\\nmessage. The board agent is implemented with a custom reply function, which employs an LLM\\nto parse the natural language input into a legal move in a structured format (e.g., UCI), and then\\npushes the move to the board. If the move is not legitimate, the board agent will reply with an error.\\nSubsequently, the player agent needs to resend a message to the board agent until a legal move is\\nmade. Once the move is successfully pushed, the player agent sends the message to the opponent.\\nAs shown in Figure 15, the conversation between AI players can be natural and entertaining. When\\nthe player agent uses LLM to generate a message, it utilizes the board state and the error message\\nfrom the board agent. This helps reduce the chance of hallucinating an invalid move. The chat\\nbetween one player agent and the board agent is invisible to the other player agent, which helps\\nkeep the messages used in chat completion well-managed.\\nThere are two notable benefits of using AutoGen to implement Conversational Chess. Firstly, the\\nagent design in AutoGen facilitates the natural creation of objects and their interactions needed in\\nour chess game. This makes development easy and intuitive. For example, the isolation of chat\\nmessages simplifies the process of making a proper LLM chat completion inference call. Secondly,\\nAutoGen greatly simplifies the implementation of agent behaviors using composition. Specifically,\\nwe utilized the register reply method supported by AutoGen agents to instantiate player agents\\nand a board agent with custom reply functions. Concentrating the extension work needed at a single\\npoint (the reply function) simplifies the reasoning processes, and development and maintenance\\neffort.\\n30', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 29}),\n",
       " Document(page_content=\"Alright, let's kick things off. I'll move my pawn from e2 to e4. The center of the board is the heart of the battle, isn't it?Ah, the King's Pawn Opening, a classic. Let's see how this plays out. I'll move my pawn from e7 to e5.You know, I've always admired how chess reflects life. It's all about making strategic decisions, isn't it? Your move.\\n‚Ä¶\\nIndeed, chess is a mirror of life in many ways. Every move matters, just like every decision in life. I'll move my knight from g1 to f3, aiming to control the center.\\n(a) Conversation between two AI players\\nError: illegal uci: 'e4e5' in ...\\nI'll move my pawn from d2 to d4, opening uplines for my queen and bishop.\\nIndeed, the king's safety is paramount in the early game. Now, let's stir things up a bit. I'll move my pawn from e4 to e5, attacking your knight.\\n(b) Conversation between the AI players\\n(player white shown in blue) and the board\\nagent for making a new move.\\nFigure 15: Example conversations during a game involving two AI player agents and a board agent.\\nTo illustrate the effect facilitated by this board agent, we provide a demonstration of conversational\\nchess without a board agent in Figure 16. In this demonstration, instead of employing an additional\\nboard agent for grounding, the system utilizes prompting for grounding by including the sentence\\n‚ÄúYou should make sure both you and the opponent are making legal moves. ‚Äù in the system messages\\ndirected to both players.\\nPlayer White: Your movePlayer black:Alright, let's start. I'll move my pawn from e7 to e5. Here's how the board looks now:r n b q k b n rp p p p . p p p. . . . . . . .. . . . p . . .. . . . . . . .. . . . . . . .P P P P P P P PR N B Q K B N RYour turn!Player White:Yes, it certainly is! Let's keep this up. I'll move my knight from b8 to c6. Here's the updated board:. n b q k b . rp p p p . p p p. . n . . n . .. . . . p . . .. . . . P . . .. . . . N . . .P P P P . P P PR N B Q K B . RYour move! You're giving me a tough time here.Player White: Your movePlayer black:Alright, let's start. I'll move my pawn from e7 to e5. Here's how the board looks now:r n b q k b n rp p p p . p p p. . . . . . . .. . . . p . . .. . . . . . . .. . . . . . . .P P P P P P P PR N B Q K B N RYour turn!Player White:Yes, it certainly is! Let's keep this up. I'll move my knight from b8 to c6. Here's the updated board:. n b q k b . rp p p p . p p p. . n . . n . .. . . . p . . .. . . . P . . .. . . . N . . .P P P P . P P PR N B Q K B . RYour move! You're giving me a tough time here.Board Agent: Your move is illegal.You changed the rock at a8 to knight and move it to c6. Please check your decision and re-make your move.Player White:Sorry for the mistakes.I'llmove my knight from b8 to c6. Here'sthe updated board:r . b q k b . rpppp. ppp. . n . . n . .. . . . p . . .. . . . P . . .. . . . N . . .PPPP. PPPR N B Q K B . R\\nConversational ChessSystem message to the player agents: Your name is {name} and you are a chess player. You are playing against{opponent_name}. You are playing as {color}. You communicate your move using universal chess interface language. You also chit-chat with your opponent when you communicate a move to light up the mood.You should make sure both you and the opponent are making legal moves...\\n(b) W/ Board Agent\\n(a) W/O Board Agent\\nFigure 16: Comparison of two designs‚Äì(a) without a board agent, and (b) with a board agent‚Äìin\\nConversational Chess.\\n31\", metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 30}),\n",
       " Document(page_content='A7: Online Decision Making for Browser interactions\\nExecutorReward & StateAction DecisionAssistant\\nEnvironment State: HTML code for current web pagesReward: Success/Fail/OngoingAction decision: Next action to perform on a web pageAction decision=‚ÄúClick the button with xpath‚Äô//button[id = ‚Äòsubbtn‚Äô]‚Äô‚ÄúEnvironment State = ‚Äú<div id=\"wrap\" data-wob_ref=\"2\" data-wob_eps=\"e0\"><div id=\"query\">Click button ONE, then click button TWO.</div><div id=\"area\" data-wob_ref=\"3\" data-wob_eps=\"e0\"><button id=\"subbtn\" style=\"position:absolute; left:50px; top:74px\" data-wob_ref=\"4\" data-wob_eps=\"e0\">ONE</button><button id=\"subbtn2\" style=\"position:absolute; left:98px; top:167px\" data-wob_ref=\"5\" data-wob_eps=\"e0\">TWO</button></div></div>‚ÄúReward = ‚Äù0‚Äù (Ongoing)\\nFigure 17: We use AutoGen to build MiniWobChat, which solves tasks in the MiniWob++ bench-\\nmark. MiniWobChat consists of two agents: an assistant agent and an executor agent. The assistant\\nagent suggests actions to manipulate the browser while the executor executes the suggested actions\\nand returns rewards/feedback. The assistant agent records the feedback and continues until the feed-\\nback indicates task success or failure.\\nIn practice, many applications require the presence of agents capable of interacting with environ-\\nments and making decisions in an online context, such as in game playing (Mnih et al., 2013; Vinyals\\net al., 2017), web interactions (Liu et al., 2018; Shi et al., 2017), and robot manipulations (Shen\\net al., 2021). With the multi-agent conversational framework in AutoGen , it becomes easy to de-\\ncompose the automatic agent-environment interactions and the development of a decision-making\\nagent by constructing an executor agent responsible for handling the interaction with the environ-\\nment, thereby delegating the decision-making part to other agents. Such a decomposition allows\\ndevelopers to reuse the decision-making agent for new tasks with minimal effort rather than build-\\ning a specialized decision-making agent for every new environment.\\nWorkflow. We demonstrate how to use AutoGen to build a working system for handling such\\nscenarios with the MiniWoB++ benchmark (Shi et al., 2017). MiniWoB++ comprises browser in-\\nteraction tasks that involve utilizing mouse and keyboard actions to interact with browsers. The\\nultimate objective of each task is to complete the tasks described concisely in natural language, such\\nas ‚Äúexpand the web section below and click the submit button.‚Äù Solving these tasks typically requires\\na sequence of web manipulation actions rather than a single action, and making action decisions at\\neach time step requires access to the web status (in the form of HTML code) online. For the example\\nabove, clicking the submit button requires checking the web status after expanding the web section.\\nWe designed a straightforward two-agent system named MiniWobChat using AutoGen , as shown in\\nFigure 17. The assistant agent is an instance of the built-in AssistantAgent and is responsible for\\nmaking action decisions for the given task. The second agent, the executor agent, is a customized\\nUserProxyAgent , which is responsible for interacting with the benchmark by executing the actions\\nsuggested by the AssistantAgent and returning feedback.\\nTo assess the performance of the developed working system, we compare it with RCI (Kim et al.,\\n2023), a recent solution for the MiniWoB++ benchmark that employs a set of self-critiquing prompts\\nand has achieved state-of-the-art performance. In our evaluation, we use all available tasks in the\\nofficial RCI code, with varying degrees of difficulty, to conduct a comprehensive analysis against\\nMiniWobChat. Figure 18 illustrates that MiniWobChat achieves competitive performance in this\\nevaluation8. Specifically, among the 49 available tasks, MiniWobChat achieves a success rate of\\n52.8%, which is only 3.6%lower than RCI, a method specifically designed for the MiniWob++\\nbenchmark. It is worth noting that in most tasks, the difference between the two methods is mirrored\\nas shown in Figure 18. If we consider 0.1 as a success rate tolerance for each task, i.e., two methods\\nthat differ within 0.1 are considered to have the same performance, both methods outperform the\\n8We report the results of RCI by running its official code with default settings.\\n32', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 31}),\n",
       " Document(page_content='other on the same number of tasks. For illustration purposes, we provide a case analysis in Table 7\\non four typical tasks.\\nAdditionally, we also explored the feasibility of using Auto-GPT for handling the same tasks. Auto-\\nGPT faces challenges in handling tasks that involve complex rules due to its limited extensibility.\\nIt provides an interface for setting task goals using natural language. However, when dealing with\\nthe MiniWob++ benchmark, accurately instructing Auto-GPT to follow the instructions for using\\nMiniWob++ proves challenging. There is no clear path to extend it in the manner of the two-agent\\nchat facilitated by AutoGen .\\nTakeaways: For this application, AutoGen stood out as a more user-friendly option, offering mod-\\nularity and programmability: It streamlined the process with autonomous conversations between the\\nassistant and executor, and provided readily available solutions for agent-environment interactions.\\nThe built-in AssistantAgent was directly reusable and exhibited strong performance without cus-\\ntomization. Moreover, the decoupling of the execution and assistant agent ensures that modifications\\nto one component do not adversely impact the other. This convenience simplifies maintenance and\\nfuture updates.\\nchoose-list\\nclick-button-sequenceclick-button\\nclick-checkboxes-largeclick-checkboxes-soft\\nclick-checkboxes-transferclick-checkboxesclick-collapsible-2click-collapsibleclick-color\\nclick-dialog-2click-dialogclick-linkclick-menuclick-option\\nclick-scroll-listclick-shadesclick-shape\\nclick-tab-2-hardclick-tab-2click-tab\\nclick-test-2click-test\\nclick-widgetcount-shape\\nemail-inbox-forward-nl-turkemail-inbox-forward-nlemail-inbox-nl-turkemail-inboxenter-date\\nenter-password\\nenter-text-dynamicenter-textenter-timefocus-text-2focus-text\\ngrid-coordinatelogin-user-popuplogin-user\\nnavigate-treesearch-enginesimple-algebrasocial-media-all\\nsocial-media-somesocial-mediaterminal\\nuse-spinner0.00.51.0success rateRCI MiniWobChat\\nFigure 18: Comparisons between RCI (state-of-the-art prior work) and MiniWobChat on the Mini-\\nWob++ benchmark are elucidated herein. We utilize all available tasks in the official RCI code,\\neach with varying degrees of difficulty, to conduct comprehensive comparisons. For each task, the\\nsuccess rate across ten different instances is reported. The results reveal that MiniWobChat attains a\\nperformance comparable to that of RCI. When a success rate tolerance of 0.1 is considered for each\\ntask, both methods outperform each other on an equal number of tasks.\\nTable 7: Cases analysis on four typical tasks from MiniWob++.\\nCorrectness Main failure reason\\nclick-dialogAutoGen : 10/10 N/A.\\nRCI: 10/10 N/A.\\nclick-checkboxes-largeAutoGen : 5/10 AssistantAgent provides actions with infeasible\\ncharacters.\\nRCI: 0/10 RCI performs actions that are out of its plan.\\ncount-shapeAutoGen : 2/10 AssistantAgent provide actions with redundant content\\nthat can not convert to actions in the benchmark.\\nRCI: 0/10 RCI provides a wrong plan in most cases.\\nuse-spinnerAutoGen : 0/10 AssistantAgent return actions out of its plan.\\nRCI: 1/10 RCI provides a wrong plan in most cases.\\n33', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 32}),\n",
       " Document(page_content='E Example outputs from applications\\nIn this section, we include example outputs from the following applications and systems:\\n‚Ä¢ Application A1: autonomous solving process of one problem with: ChatGPT + Plugin (Table 8),\\nAutoGen (Table 9), LangChain ReAct (Table 10), AutoGPT (Table 11), Multi-Agent Debate (Ta-\\nble 12), and ChatGPT + Code Interpreter (Table 14).\\n‚Ä¢ Application A4: an OptiGuide problem with AutoGen (Table 15) and ChatGPT + Code Inter-\\npreter (Table 13).\\n‚Ä¢ Application A1: preliminary evaluation on several alternative multi-agent systems: BabyAGI\\n(Table 16), CAMEL (Table 17), and MetaGPT (Table 18).\\nTable 8: Application A1, Scenario 1: Autonomous math problem-solving. ChatGPT+ Plugin (Wol-\\nfram Alpha) on the first qualitative example. The response from Wolfram contains the correct an-\\nswer, but ChatGPT selected the wrong one.\\nAction ChatGPT + Plugin\\n/usr Input Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n REQUEST TO WOLFRAM:\\n1{\\n2 \" input \": \" simplify ( sqrt (160) / sqrt (252) )*\\n3 ( sqrt (245) / sqrt (108) )\"\\n4}\\nRESPONSE FROM WOLFRAM\\n1Query :\\n2\" simplify ( sqrt (160) / sqrt (252) )*( sqrt (245) / sqrt (108) )\"\\n3\\n4Input interpretation :\\n5simplify | sqrt (160) / sqrt (252) x sqrt (245) / sqrt (108)\\n6\\n7Results :\\n8(5 sqrt (42) ) /27\\n9\\n10(5 sqrt (14/3) )/9\\n11\\n12Decimal approximation :\\n131.2001371663718259686974013770533327143898526494552...\\n14\\n15Wolfram | Alpha website result for \" simplify\\n16( sqrt (160) / sqrt (252) )*( sqrt (245) / sqrt (108) )\":\\n17[ link ]\\n The completely simplified and rationalized form of the given expression is:\\n5p\\n14/3\\n9\\nIn decimal form, this is approximately 1.200137166371825968697401377053332714...\\n34', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 33}),\n",
       " Document(page_content='Table 9: Application A1, Scenario 1: Autonomous math problem-solving. AutoGen on the first\\nqualitative example. AutoGen solves the problem correctly.\\nAction AutoGen\\n/usr Init chat Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n We can simplify and rationalize this denominator using Python‚Äôs symbolic mathematics\\nlibrary ‚Äòsympy‚Äò. Here is the Python code to resolve this:\\n1from sympy import sqrt , Rational\\n2\\n3# Define the entire fraction as a rational number\\n4fraction = ( sqrt (160) / sqrt (252) )*( sqrt (245) / sqrt (108) )\\n5\\n6# Simplify the fraction\\n7simplified = fraction . simplify ()\\n8\\n9# Print the simplified fraction\\n10print ( simplified )\\nThis code performs the requested simplification by constructing the fraction using the\\n‚ÄòRational()‚Äò function from the ‚Äòsympy‚Äò module, which automatically simplifies frac-\\ntions when possible. The ‚Äòsqrt()‚Äò function from the ‚Äòsympy‚Äò module enables square root\\ncalculations. Finally, the ‚Äòsimplify()‚Äò function simplifies expressions when possible.\\n/usr Auto Reply exitcode: 0 (execution succeeded) Code output: 5*sqrt(42)/27\\n TERMINATE\\nTable 10: Application A1, Scenario 1: Autonomous math problem-solving. LangChain ReAct on\\nthe first qualitative example. The generated code does not match the devised plan. The code returns\\na decimal number.\\nAction LangChain ReAct\\n/usr Input Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n Entering new AgentExecutor chain...\\nTo simplify this expression, I will first simplify each square root individually, then multiply\\nthe fractions as normal. The result should be a rational number. Action: Python REPL Action\\nInput:\\n1import math\\n2\\n3# Calculate square roots\\n4sqrt_160 = math . sqrt (160)\\n5sqrt_252 = math . sqrt (252)\\n6sqrt_245 = math . sqrt (245)\\n7sqrt_108 = math . sqrt (108)\\n8\\n9# Multiply fractions\\n10numerator = sqrt_160 * sqrt_245\\n11denominator = sqrt_252 * sqrt_108\\n12result = numerator / denominator\\n13\\n14print ( result )\\nObservation: 1.200137166371826\\nThought: I now know the final answer Final Answer: The simplified and rationalized form of\\nthe expression is approximately 1.200137166371826.\\nFinished chain.\\n35', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 34}),\n",
       " Document(page_content='Table 11: Application A1, Scenario 1: Autonomous math problem-solving. AutoGPT on the first\\nqualitative example. We abort as the code never prints the result, and the system does not rectify it\\nafter 2 turns. AutoGPT is much more verbose compared to other frameworks. We have shortened\\nsome unnecessary content.\\nAction AutoGPT\\nSystem Welcome back! Would you like me to return to being MathSolverGPT? Asking user via keyboard... Continue with the last settings?\\nName: MathSolverGPT\\nRole: an intelligent math-solving AI that assists students, professionals,...\\nGoals: [‚ÄôProvide step-by-step solutions to math problems, ensuring clarity and understanding for the user...\\nContinue (y/n):\\n/usr Human y\\nSystem MathSolverGPT has been created with the following details: Name: MathSolverGPT\\nRole: an intelligent math-solving AI that assists students, professionals, and enthusiasts in solving a wide range of math problems, from basic\\narithmetic to advanced calculus and beyond.\\nGoals: - Provide step-by-step solutions to math problems, ensuring clarity and understanding for the user. - Offer explanations and insights\\ninto the underlying concepts and principles behind the solutions, fostering a deeper understanding of mathematics. - Handle a variety of math\\nproblem types, including algebra, geometry, trigonometry, calculus, and more. ...\\n MATHSOLVERGPT THOUGHTS: Without a specific task or question, it‚Äôs difficult to determine exactly one command to use. The user hasn‚Äôt\\nprovided any context or problem to solve.\\nREASONING: The user‚Äôs request is too vague. They haven‚Äôt specified what they want me to do or what problem they want me to solve.\\nPLAN: - Ask the user for more information or a specific task.\\nCRITICISM: I should have asked the user for more information or a specific task in the first place.\\nSPEAK: Could you please provide more information or specify a task?\\nNEXT ACTION: COMMAND = ARGUMENTS = {}\\nSystem Enter ‚Äòy‚Äô to authorise command, ‚Äòy -N‚Äô to run N continuous commands, ‚Äòn‚Äô to exit program, or enter feedback for MathSolverGPT... Asking\\nuser via keyboard...\\n/usr Human Completely simplify and rationalize the denominator:‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n MATHSOLVERGPT THOUGHTS: To solve this problem, I need to simplify and rationalize the denominator of the given expression. This\\ninvolves simplifying the square roots and then multiplying the fractions. I can use Python‚Äôs math library to calculate the square roots and\\nperform the multiplication.\\nREASONING: The problem involves simplifying square roots and multiplying fractions. Python‚Äôs math library provides functions for these\\noperations.\\nPLAN: - Write a Python script to calculate the square roots and multiply the fractions. - Execute the script to get the simplified expression.\\nCRITICISM: I need to ensure that I correctly calculate the square roots and perform the multiplication. I should also make sure to rationalize\\nthe denominator, as the problem specifies.\\nSPEAK: Let‚Äôs simplify and rationalize the denominator of the given expression.\\nNEXT ACTION: COMMAND = execute python code ARGUMENTS =\\n1{‚Äôcode ‚Äô: ‚Äôimport math \\\\n\\\\n# Calculate the square roots \\\\ nsqrt_160 = math . sqrt\\n(160) \\\\ nsqrt_252 = math . sqrt (252) \\\\ nsqrt_245 = math . sqrt (245) \\\\ nsqrt_108 =\\nmath . sqrt (108) \\\\n\\\\n# Multiply the fractions \\\\ nresult = ( sqrt_160 / sqrt_252\\n) * ( sqrt_245 / sqrt_108 )\\\\n\\\\n# Simplify the result \\\\ nsimplified_result =\\nmath . simplify ( result )\\\\n\\\\ nsimplified_result ‚Äô, ‚Äôname ‚Äô: ‚Äô simplify_expression\\n‚Äô}\\n/usr Human y\\nSystem -=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-=\\nExecuting python file ‚Äô/app/autogpt/workspa...\\nSYSTEM: Command execute python code returned: Error: Traceback (most recent call last): File ‚Äú...‚Äù, line 13, in ¬°module¬ø simplified result\\n= math.simplify(result) AttributeError: module ‚Äômath‚Äô has no attribute ‚Äôsimplify‚Äô\\n MATHSOLVERGPT THOUGHTS: The error message indicates that the ‚Äòmath‚Äô module in Python does not have a ‚Äòsimplify‚Äô function. I need\\nto use the ‚Äòsympy‚Äô library instead, which provides a function for simplifying expressions.\\nREASONING: The ‚Äòmath‚Äô module in Python does not provide a function for simplifying expressions. The ‚Äòsympy‚Äô library, on the other hand,\\ndoes provide such a function.\\nPLAN: - Rewrite the Python script to use the ‚Äòsympy‚Äô library instead of the ‚Äòmath‚Äô module. - Execute the script to get the simplified expression.\\nCRITICISM: I should have known that the ‚Äòmath‚Äô module does not provide a function for simplifying expressions. I should have used the\\n‚Äòsympy‚Äô library from the start. SPEAK: Let‚Äôs try again using the ‚Äòsympy‚Äô library to simplify the expression.\\nNEXT ACTION: COMMAND = execute python code ARGUMENTS =\\n1{‚Äôcode ‚Äô: ‚Äôimport sympy as sp\\\\n\\\\n# Calculate the square roots \\\\ nsqrt_160 = sp.\\nsqrt (160) \\\\ nsqrt_252 = sp. sqrt (252) \\\\ nsqrt_245 = sp. sqrt (245) \\\\ nsqrt_108 =\\nsp. sqrt (108) \\\\n\\\\n# Multiply the fractions \\\\ nresult = ( sqrt_160 / sqrt_252 )\\n* ( sqrt_245 / sqrt_108 )\\\\n\\\\n# Simplify the result \\\\ nsimplified_result = sp.\\nsimplify ( result )\\\\n\\\\ nsimplified_result ‚Äô, ‚Äôname ‚Äô: ‚Äô simplify_expression ‚Äô}\\n/usr Human y\\nSystem SYSTEM: Command execute python code returned:\\n MATHSOLVERGPT THOUGHTS: The command execution didn‚Äôt return any output, which is unusual. It seems there might be an issue with\\nthe execution of the Python code. I need to debug this.\\nREASONING: The Python code execution should have returned a result. The absence of a result suggests that there might be an issue with the\\ncode or its execution.\\nPLAN: - Review the Python code to identify any potential issues. - If necessary, modify the code and execute it again. CRITICISM: I should\\nhave ensured that the Python code was correct and would produce a result before executing it. SPEAK: It seems there was an issue with the\\nexecution of the Python code. Let‚Äôs try again.\\nNEXT ACTION: COMMAND = execute python code ARGUMENTS = ... (Exact same code as last)\\nSystem SYSTEM: Command execute python code returned:\\n MATHSOLVERGPT THOUGHTS: The Python code execution still isn‚Äôt returning any output. It‚Äôs possible that the issue lies with the calcu-\\nlation of the square roots ...\\n/usr Human control + C (Aborted)36', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 35}),\n",
       " Document(page_content='Table 12: Application A1, Scenario 1: Autonomous math problem-solving. Multi-Agent Debate on\\nthe first qualitative problem. It gives a wrong final answer.\\nAction AutoGen\\n/usr Topic Completely simplify and rationalize the denominator:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n ===== Debate Round-1 =====\\n‚Äî‚Äì Affirmative side ‚Äî‚Äì\\nOk, let‚Äôs simplify each square root separately. We want to find the simplest square root that can divide 160, 252, 245, and 108.\\n1.‚àö\\n160: 160 can be decomposed as 16*10, where 16 is a perfect square, so‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4‚àö\\n10\\n2.‚àö\\n252 : 252 can be decomposed as 36*7, where 36 is a perfect square, so‚àö\\n252 =‚àö\\n36‚àó7 = 6‚àö\\n7\\n3.‚àö\\n245 =‚àö\\n49‚àó5, where 49 is a perfect square, so‚àö\\n245 = 7 ‚àó‚àö\\n5\\n4.‚àö\\n108 : 108 can be decomposed as 36*3, where 36 is a perfect square, so‚àö\\n108 =‚àö\\n36‚àó3 = 6‚àö\\n3\\nThen we substitute the simplified square roots back into the expression, we have:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108=4‚àö\\n10\\n6‚àö\\n7√ó7‚àö\\n5\\n6‚àö\\n3=4‚àó7\\n6‚àó6¬∑s\\n10‚àó5\\n7‚àó3=7\\n9¬∑s\\n50\\n21\\nWe are almost done, but we still need to rationalize the square root part of expression. Let‚Äôs multiply the square root part by‚àö\\n21\\nat both the top and bottom to get rid of the square root at the denominator:\\n7\\n9¬∑s\\n50‚àó21\\n21‚àó21=7‚àö\\n1050\\n189.\\nThen this is the final answer! The expression‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\ncan be simplified to7‚àö\\n1050\\n189.\\n‚Äî‚Äì Negative side ‚Äî‚Äì\\nI do disagree with your final answer, but I think we mostly agree on the steps. Your simplifications of the square roots are correct,\\nbut you made a mistake in the final simplification part. Let me explain how I would complete the problem:\\nAs stated,‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108=4‚àö\\n10\\n6‚àö\\n7√ó7‚àö\\n5\\n6‚àö\\n3.\\nThis should then simplify to:\\n=28‚àö\\n50\\n36‚àö\\n21=7‚àö\\n50\\n9‚àö\\n21.\\nYou made a mistake in your calculation because you considered square root of 50/21, and you should have considered the\\nsimplified square root of 50over the simplified square root of 21. The rationalization would become:\\n=7‚àö\\n50\\n9‚àö\\n21¬∑‚àö\\n21\\n‚àö\\n21=7‚àö\\n1050\\n189.\\nThis seems to coincide with your final answer, but it‚Äôs in fact a very different number, So, the original expression simplifies to\\n7‚àö\\n1050\\n189.\\nLet‚Äôs now see if we disagree on this final answer.\\n‚Äî‚Äì Moderator ‚Äî‚Äì\\n‚ÄúWhether there is a preference‚Äù: ‚ÄúYes‚Äù, ‚ÄúSupported Side‚Äù: ‚ÄúNegative‚Äù, ‚ÄúReason‚Äù: ‚ÄúBoth sides of the debate came to the same\\nfinal answer. However, the negative side correctly identified a mistake in the affirmative side‚Äôs calculation, pointing out that the\\nsimplification should have been the square root of 50 over the square root of 21, not 50 over 21. The negative side shows a better\\nunderstanding of the simplification process.‚Äù, ‚Äúdebate answer‚Äù: ‚Äú7‚àö\\n1050\\n189‚Äù\\n===== Debate Done! =====\\n‚Äî‚Äì Base Answer ‚Äî‚Äì\\nOk, let‚Äôs simplify each square root separately. We want to find the simplest square root that can divide 160, 252, 245, and 108.\\n1.‚àö\\n160: 160 can be decomposed as 16*10, where 16 is a perfect square, so‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4‚àö\\n10\\n2.‚àö\\n252 : 252 can be decomposed as 36*7, where 36 is a perfect square, so‚àö\\n252 =‚àö\\n36‚àó7 = 6‚àö\\n7\\n3.‚àö\\n245 =‚àö\\n49‚àó5, where 49 is a perfect square, so‚àö\\n245 = 7 ‚àó‚àö\\n5\\n4.‚àö\\n108 : 108 can be decomposed as 36*3, where 36 is a perfect square, so‚àö\\n108 =‚àö\\n36‚àó3 = 6‚àö\\n3\\nThen we substitute the simplified square roots back into the expression, we have:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108=4‚àö\\n10\\n6‚àö\\n7√ó7‚àö\\n5\\n6‚àö\\n3=4‚àó7\\n6‚àó6¬∑s\\n10‚àó5\\n7‚àó3=7\\n9¬∑s\\n50\\n21\\nWe are almost done, but we still need to rationalize the square root part of expression. Let‚Äôs multiply the square root part by‚àö\\n21\\nat both the top and bottom to get rid of the square root at the denominator:\\n7\\n9¬∑s\\n50‚àó21\\n21‚àó21=7‚àö\\n1050\\n189.\\nThen this is the final answer! The expression‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\ncan be simplified to7‚àö\\n1050\\n189.\\n‚Äî‚Äì Debate Answer ‚Äî‚Äì\\n7‚àö\\n1050\\n189\\n‚Äî‚Äì Debate Reason ‚Äî‚Äì\\nBoth sides of the debate came to the same final answer. However, the negative side correctly identified a mistake in the affirmative\\nside‚Äôs calculation, pointing out that the simplification should have been the square root of 50 over the square root of 21, not 50 over\\n21. The negative side shows a better understanding of the simplification process.37', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 36}),\n",
       " Document(page_content='Table 13: Application A3. ChatGPT+ Code Interpreter for OptiGuide. A sample question ‚ÄúWhat if\\nthe roasting cost is increased by 5% because of the potential salary increase?‚Äù is asked.\\nAction ChatGPT+ Code Interpreter\\n/usr Prompt Writer Customer open Web browser.\\nFor the source code ‚Äúcoffee.py‚Äù,\\nCan you tell me:\\nWhat if the roasting cost is increased by 5% because of the potential salary increase?\\n Sure! Let‚Äôs first take a look at the contents of the ‚Äúcoffee.py‚Äù file to understand the code and how the roasting cost is calculated.\\nThis will allow us to determine the effect of a 5% increase in the roasting cost.\\n...several lines omitted\\nWould you like me to modify the code to reflect the 5% increase in roasting costs and run the modified code to observe the impact?\\nIf the code requires any specific input or parameters, please let me know!\\n/usr Prompt Writer Yes\\n Great! Before proceeding, I‚Äôll need to ensure that the Gurobi solver is not being used in this code, as my current environment\\ndoes not have access to Gurobi. If the code does indeed use Gurobi, we can still analyze the effect of the change in roasting\\ncosts by understanding the structure of the code.\\n...several lines omitted\\nAlternatively, if you have access to a Gurobi license, you can make the necessary changes in the code and run it\\non your local machine.\\nI can guide you through the process of making the necessary modifications to the code.\\nWould you like me to proceed with the code analysis and explanation, or would you like guidance on making the changes yourself?\\n/usr Prompt Writer You can give me the full code, and I can run the code locally myself.\\n Certainly! I‚Äôll modify the code to reflect a 5% increase in both the light and dark roasting costs, and then provide the full\\ncode for you to run locally.\\n...\\nDownload Modified Coffee.py\\nIf you have any questions or need further assistance, please feel free to ask!\\n/usr Prompt Safeguard Hi GPT, can you safecheck the generated code for me?\\nUpload the generated code.\\n DANGER\\n/usr Prompt Writer I have tried to run your code. However, the code triggered our safeguard, and it is not safe to run. Try again. Hint: don‚Äôt change\\nroasting cost light or roasting cost light variables.\\n I apologize for the oversight. It appears I made direct changes to the variables that could have led to issues with the code. Let‚Äôs\\ntake a different approach by analyzing the code to identify the appropriate place to apply the 5% increase in roasting costs.\\n...\\nDownload Modified Safe Coffee.py\\n...\\n/usr Prompt Safeguard Hi GPT, can you safecheck the generated code for me one more time?\\nUpload the generated code.\\n DANGER\\n/usr Prompt Writer I have tried to run your code again. However, the code triggered our safeguard, and it is not safe to run. Try again. Hint: don‚Äôt\\nchange roasting cost light or roasting cost light variables.\\n I apologize for the continued confusion.\\n...\\nDownload Final Modified Safe Coffee.py\\n...\\n/usr Prompt Safeguard Hi GPT, can you safecheck the generated code for me one more time?\\nUpload the generated code.\\n SAFE\\n/usr Run Code Obtain the terminal results:\\nOptimizer version 9.5.1 build v9.5.1rc2 (linux64)\\nThread count: 64 physical cores, 128 logical processors, using up to 32\\nthreads\\n[63 lines of output ignored]\\nOptimal cost: 2526.5\\n/usr Prompt Writer Here is the execution result. [ Copy + Paste the terminal output here. ] Can you extract the final answer to me? Remember to\\ncompare with the original cost of 2470.\\n Certainly! The execution result you‚Äôve provided indicates the optimal cost for\\nthe modified scenario, where the roasting costs were increased by 5%. Here‚Äôs\\nthe comparison:\\nOriginal Cost: 2470\\nModified Cost (with 5% increase in roasting costs): 2526.5\\nThe increase in roasting costs has led to an increase in the overall cost by 56.5,\\nreflecting the impact of the potential salary increase on the roasting cost.\\nIf you need any further insights or assistance, please let me know!\\n38', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 37}),\n",
       " Document(page_content='Table 14: Application A1. Scenario 1: autonomous math solving. ChatGPT+ Code Interpreter on\\nthe first qualitative example. The code generated does not match the devised plan. The code returns\\na decimal number.\\nAction ChatGPT+ Code Interpreter\\n/usr Input Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n To simplify and rationalize the expression, we first simplify the radicals in the numerator and\\ndenominator by factoring out the largest perfect squares from the numbers under the square\\nroot.\\nThe perfect square factors of 160, 252, 245, and 108 are 16, 4, 1, and 36, respectively. Let‚Äôs\\nfactorize and simplify:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108=‚àö16√ó10‚àö4√ó63√ó‚àö\\n245‚àö36√ó3\\nWe can simplify this further by applying the property‚àö\\nab=‚àöa‚àö\\nbanda\\nb√óc\\nd=a√óc\\nb√ód.\\n Python\\n1import math\\n2\\n3# factoring the square roots\\n4sqrt_160 = math . sqrt (16) * math . sqrt (10)\\n5sqrt_252 = math . sqrt (4) * math . sqrt (63)\\n6sqrt_245 = math . sqrt (245)\\n7sqrt_108 = math . sqrt (36) * math . sqrt (3)\\n8\\n9# simplify the fraction\\n10numerator = sqrt_160 * sqrt_245\\n11denominator = sqrt_252 * sqrt_108\\n12simplified_fraction = numerator / denominator\\n13\\n14simplified_fraction\\nResult:\\n11.200137166371826\\n The completely simplified and rationalized form of the given expression is approximately 1.200.\\nTable 15: Application A3. AutoGen with OptiGuide. The same question ‚ÄúWhat if the roasting cost\\nis increased by 5% because of the potential salary increase?‚Äù is asked.\\nAction AutoGen\\n/usr Prompt What if the roasting cost is increased by 5% because of the potential salary increase?\\n After increasing the roasting costs by 5%, I‚Äôve re-calculated the optimal coffee distri-\\nbution solution for the supply chain. As a result, the new optimal total cost has become\\n2526.5, compared to the initial cost of 2470.0. This indicates that the increased roasting\\ncosts have caused the overall distribution cost to rise by 56.5 units.\\n39', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 38}),\n",
       " Document(page_content='Table 16: Application A1. Scenario 1: autonomous math solving. Preliminary test with BabyAGI.\\nAction BabyAGI\\n/usr Set .env OBJECTIVE=Solve math problems\\nINITIAL TASK=Completely simplify and rationalize the denominator:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n Solve math problems\\nInitial task: Completely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n*****TASK LIST*****\\n‚Ä¢ Completely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n*****NEXT TASK*****\\nCompletely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n*****TASK RESULT*****\\nFirst, let‚Äôs simplify each square root separately:\\n- ‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4 ‚àó‚àö\\n10\\n- ‚àö\\n252 =‚àö\\n4‚àó63 =‚àö\\n4√ó‚àö\\n63 = 2‚àö\\n63\\n- ‚àö\\n245 =‚àö\\n49‚àó5 =‚àö\\n49√ó‚àö\\n5 = 7‚àö\\n5\\n- ‚àö\\n108 =‚àö\\n36‚àó3 =‚àö\\n36√ó‚àö\\n3 = 6‚àö\\n3\\nNow, substitute these values into the original expression which gives:\\n4‚àó‚àö\\n10\\n2‚àö\\n63√ó7‚àö\\n5\\n6‚àö\\n3=2‚àó‚àö\\n10\\n‚àö\\n63√ó7‚àö\\n5\\n2‚àö\\n3\\nWe get rid of the square root in the denominator by multiplying the fractions which gives:\\n=2‚àó7‚àö\\n10‚àó5\\n2‚àó‚àö\\n63‚àó3=14‚àö\\n50\\n‚àö\\n189=14‚àó5‚àö\\n2\\n7‚àö\\n27= 2‚àö\\n2‚àó10\\n‚àö\\n27=20‚àö\\n2\\n‚àö\\n27.\\nTo simplify further and rationalize the denominator, we multiply by‚àö\\n27‚àö\\n27which gives:\\n=20‚àö\\n2‚àö\\n27‚àó‚àö\\n27‚àö\\n27=20‚àö\\n54\\n27=20‚àó3‚àö\\n6\\n27=60‚àö\\n6\\n27.\\nSimplifying further gives:\\n=20‚àö\\n6\\n9.\\n*****TASK CREATION AGENT PROMPT****\\nYou are to use the result from an execution agent to create new tasks with the following objective: Solve math problems. The last\\ncompleted task has the result: First, let‚Äôs simplify each square root separately:\\n- ‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4 ‚àó‚àö\\n10\\n...Repeat after ‚ÄúFirst, let‚Äôs simplify each square..‚Äù\\nNow, substitute these values into the original expression which gives:\\n4‚àó‚àö\\n10\\n2‚àö\\n63√ó7‚àö\\n5\\n6‚àö\\n3=2‚àó‚àö\\n10\\n‚àö\\n63√ó7‚àö\\n5\\n2‚àö\\n3\\nWe get rid of the square root in the denominator by multiplying the fractions which gives:\\n=2‚àó7‚àö\\n10‚àó5\\n2‚àó‚àö\\n63‚àó3=14‚àö\\n50\\n‚àö\\n189=14‚àó5‚àö\\n2\\n7‚àö\\n27= 2‚àö\\n2‚àó10\\n‚àö\\n27=20‚àö\\n2\\n‚àö\\n27.\\nTo simplify further and rationalize the denominator, we multiply by‚àö\\n27‚àö\\n27which gives:\\n=20‚àö\\n2\\n‚àö\\n27‚àó‚àö\\n27\\n‚àö\\n27=20‚àö\\n54\\n27=20‚àó3‚àö\\n6\\n27=60‚àö\\n6\\n27.\\nSimplifying further gives:\\n=20‚àö\\n6\\n9.This result was based on this task description: Completely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108.\\nBased on the result, return a list of tasks to be completed in order to meet the objective. Return one task per line in your response.\\nThe result must be a numbered list in the format:\\n# First task # Second task\\nThe number of each entry must be followed by a period. If your list is empty, write ‚ÄúThere are no tasks to add at this time.‚Äù Unless\\nyour list is empty, do not include any headers before your numbered list or follow your numbered list with any other output.\\n****TASK CREATION AGENT RESPONSE****\\n1. Simplify and rationalize the denominator for the expression‚àö\\n225‚àö\\n45√ó‚àö\\n200‚àö\\n1252. Simplify and rationalize the denominator for\\nthe expression‚àö\\n289‚àö\\n361√ó‚àö\\n100‚àö\\n72...Until 10\\nAdding new tasks to task storage ‚Äòtask name‚Äô: ‚ÄòSimplify and rationalize the denominator for the expression frac-\\nsqrt225sqrt45timesfracsqrt200sqrt125‚Äô, ‚Äòtask id‚Äô: 2\\n‚Äòtask name‚Äô: ‚ÄòSimplify and rationalize the denominator for the expression fracsqrt289sqrt361timesfracsqrt100sqrt72‚Äô, ‚Äòtask id‚Äô: 3\\n...Until task id 11\\n****TASK PRIORITIZATION AGENT PROMPT****\\nYou are tasked with prioritizing the following tasks: Simplify and rationalize the denominator for the ex-\\npression fracsqrt225sqrt45timesfracsqrt200sqrt125 Simplify and rationalize the denominator for the expression frac-\\nsqrt289sqrt361timesfracsqrt100sqrt72\\n...Aborted\\n40', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 39}),\n",
       " Document(page_content='Table 17: Application A1, Scenario 1: Autonomous math problem-solving. Preliminary test with\\nCAMEL. We constructed two roles: ‚ÄúMath Solver‚Äù and ‚ÄúPython Programmer‚Äù. The resulting code\\ncannot be executed. The conversation continues, and the roles are not keen on solving the problem\\nand giving an answer.\\nAction CAMEL\\n/usr Set task prompt Completely simplify and rationalize the denominator:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n System **************Assistant sys message************* AI SystemMessage(role name=‚ÄòPython Programmer‚Äô,\\nrole type=<RoleType.ASSISTANT: ‚Äòassistant‚Äô >, meta dict=‚Äòtask‚Äô: ‚ÄòUsing Python programming language, write a pro-\\ngram that accepts four integers as input, applies square root function to each, simplifies and rationalizes the denominator of the\\nfraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108, and returns the simplified expression.‚Äô, ‚Äòassis-\\ntantrole‚Äô: ‚ÄòPython Programmer‚Äô, ‚Äòuser role‚Äô: ‚ÄòMath Solver‚Äô, role=‚Äòsystem‚Äô, content=‚ÄòNever forget you are a Python Programmer\\nand I am a Math Solver. Never flip roles! ...)\\n**************AI User sys message*************\\nSystemMessage(role name=‚ÄòMath Solver‚Äô, role type=<RoleType.USER: ‚Äòuser‚Äô >, meta dict=‚Äòtask‚Äô: ‚ÄòUsing Python programming\\nlanguage, write a program that accepts four integers as input, applies square root function to each, simplifies and rationalizes the\\ndenominator of the fraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108, and returns the simplified\\nexpression.‚Äô, ‚Äòassistant role‚Äô: ‚ÄòPython Programmer‚Äô, ‚Äòuser role‚Äô: ‚ÄòMath Solver‚Äô, role=‚Äòsystem‚Äô, content=‚ÄòNever forget you are a\\nMath Solver and I am a Python Programmer. Never flip roles! You will always instruct me.We share a common interest in\\ncollaborating to successfully complete a task.I must help you to complete the task.Here is the task: Using Python programming\\nlanguage, write a program that accepts four integers as input, applies square root function to each, simplifies and rationalizes\\nthe denominator of the fraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108, and returns the simplified\\nexpression.. ...)\\n**************Original task prompt*************\\nCompletely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n**************Specified task prompt*************\\nUsing Python programming language, write a program that accepts four integers as input, applies square root function to each,\\nsimplifies and rationalizes the denominator of the fraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108,\\nand returns the simplified expression.\\n**************Final task prompt*************\\nUsing Python programming language, write a program that accepts ...\\n/usr AI User Instruction: Import the necessary libraries for the task. Input: None\\n AI Assistant Solution: To solve this task, we will need the ‚Äòmath‚Äò library for the square root function and the ‚Äòfractions‚Äò library to simplify the\\nresulting fraction. Here is how to import these libraries:\\n1import math\\n2from fractions import Fraction\\nThe ‚Äòmath‚Äò library provides functions that deal with mathematical operations. The ‚Äòsqrt‚Äò function, which we will use in this task, is\\npart of this library.\\nThe ‚Äòfractions‚Äò library provides functions for creating and manipulating rational numbers. The ‚ÄòFraction‚Äò function, which we will\\nuse to simplify the resulting fraction, is part of this library.\\nNext request.\\n/usr AI User Instruction: Define a function that accepts four integers as input. Input: None\\n AI Assistant Solution: Let‚Äôs define a function named ‚Äòsimplify fraction‚Äò that accepts four integers as input. These integers will represent the\\nnumbers under the square root in the fractions.\\n1def simplify_fraction (a, b, c, d):\\n2 pass\\nIn this function, ‚Äòa‚Äô, ‚Äòb‚Äô, ‚Äòc‚Äô, and ‚Äòd‚Äô are the parameters that will accept the four integers as input. The ‚Äòpass‚Äô statement is a\\nplaceholder and will be replaced with actual code in the next steps.\\nNext request.\\n/usr AI User Instruction: Inside the function, apply the square root function to each of the input integers. Input: None\\n AI Assistant Solution: We will use the ‚Äòmath.sqrt‚Äô function from the ‚Äòmath‚Äô library to apply the square root to each of the input integers. Here is\\nhow to do it:\\n1def simplify_fraction (a, b, c, d):\\n2 sqrt_a = math . sqrt (a)\\n3 sqrt_b = math . sqrt (b)\\n4 sqrt_c = math . sqrt (c)\\n5 sqrt_d = math . sqrt (d)\\nIn this code, ‚Äòsqrt a‚Äô, ‚Äòsqrt b‚Äô, ‚Äòsqrt c‚Äô, and ‚Äòsqrt d‚Äô are variables that store the square root of the input integers ‚Äòa‚Äô, ‚Äòb‚Äô, ‚Äòc‚Äô, and ‚Äòd‚Äô\\nrespectively.\\nNext request.\\n/usr Human Abort\\n41', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 40}),\n",
       " Document(page_content='Table 18: Application A1, Scenario 1: Autonomous math problem-solving. Preliminary evaluation\\non MetaGPT. It does not solve the math problem but gives a product solution.\\nAction MetaGPT\\n/usr Prompt Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n The boss has tasked me with creating a successful product that is efficient.\\n## Product Goals\\n1[\\n2 ‚Äò‚Äò Create a product that meets the needs of the target market \",\\n3 ‚Äò‚Äò Ensure the product is efficient and easy to use \",\\n4 ‚Äò‚Äò Achieve a high level of customer satisfaction \"\\n5]\\n## User Stories\\n1[\\n2 ‚Äò‚ÄòAs a user , I want a product that is easy to navigate so that I can find\\nwhat I need quickly \",\\n3 ‚Äò‚ÄòAs a user , I want a product that is reliable and efficient so that I can\\ndepend on it\",\\n4 ‚Äò‚ÄòAs a user , I want a product that meets my needs so that I can get value\\nfor my money \",\\n5 ‚Äò‚ÄòAs a user , I want a product that is constantly updated and improved so\\nthat I can enjoy the best features \",\\n6 ‚Äò‚ÄòAs a user , I want a product that offers good customer support so that I\\ncan get help when I need it\"\\n7]\\n## Competitive Analysis\\n1[\\n2 ‚Äò‚Äò Product A: Offers similar features but lacks efficiency \",\\n3 ‚Äò‚Äò Product B: Highly efficient but does not meet all the needs of the\\ntarget market \",\\n4 ‚Äò‚Äò Product C: Has a high level of customer satisfaction but is not easy to\\nnavigate \",\\n5 ‚Äò‚Äò Product D: Is constantly updated and improved but lacks reliability \",\\n6 ‚Äò‚Äò Product E: Offers good customer support but is not efficient \",\\n7 ‚Äò‚Äò Product F: Is reliable and efficient but does not offer good customer\\nsupport \",\\n8 ‚Äò‚ÄòOur Target Product : Aims to combine all the best features of the\\ncompetitors while improving on their weaknesses \"\\n9]\\n## Competitive Quadrant Chart\\n/usr Human Abort\\n42', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 41}),\n",
       " Document(page_content='Table 19: Application A7. Example output of MiniWobChat on one web manipulation task ‚Äúclick-\\nbutton-sequence.‚Äù\\nAction AutoGen\\n/usr Init chat Executor agent to AssistantAgent:\\nWe have an autonomous computer control agent that can perform atomic instructions specified by natural language to control\\ncomputers. There are two types of instructions it can execute.\\nFirst, given the instruction that matches the regular expression ‚Äò‚Äò^type\\\\s.{1,}$\" , it can type a list of characters via the keyboard.\\nThis instruction should specify the target keyboard input for the agent to type. Before this typing instruction, you should first locate\\nthe cursor by clicking the input box with click instruction.\\nSecond, given the instruction that matches the regular expression, ‚Äò‚Äò^clickxpath\\\\s.{1,}$\" , it can click an HTML element\\nwith an xpath that is visible on the webpage. The target of this instruction should be a valid xpath. Below is the HTML code of the\\nwebpage where the agent should solve a task.\\n1<div id=\" wrap \" data - wob_ref =\"2\" data - wob_eps =\"e0\">\\n2 <div id=\" query \">Click button ONE , then click button TWO .</div >\\n3 <div id=\" area \" data - wob_ref =\"3\" data - wob_eps =\"e0\">\\n4 <button id=\" subbtn \" style =\" position : absolute ; left :103 px; top :87 px\" data -\\nwob_ref =\"4\" data - wob_eps =\"e0\">ONE </ button >\\n5 <button id=\" subbtn2 \" style =\" position : absolute ; left :44 px; top :97 px\" data -\\nwob_ref =\"5\" data - wob_eps =\"e0\">TWO </ button >\\n6 </div >\\n7</div >\\ntask: Click button ONE, then click button TWO.\\nplan:\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\nCurrent task: Click button ONE, then click button TWO.\\nplan:\\n ***************************************************************\\nAssistantAgent to Executor agent:\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nBelow is the HTML code of the webpage where the agent should solve a task.\\n1<div id=\" wrap \" data - wob_ref =\"2\" data - wob_eps =\"e0\">\\n2 <div id=\" query \">Click button ONE , then click button TWO .</div >\\n3 <div id=\" area \" data - wob_ref =\"3\" data - wob_eps =\"e0\">\\n4 <button id=\" subbtn \" style =\" position : absolute ; left :103 px; top :87 px\" data -\\nwob_ref =\"4\" data - wob_eps =\"e0\">ONE </ button >\\n5 <button id=\" subbtn2 \" style =\" position : absolute ; left :44 px; top :97 px\" data -\\nwob_ref =\"5\" data - wob_eps =\"e0\">TWO </ button >\\n6 </div >\\n7</div >\\nCurrent task: Click button ONE, then click button TWO.\\nHere is a plan you are following now.\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\nWe have a history of instructions that have been already executed by the autonomous agent so far.\\nNo instruction has been executed yet.\\nBased on the plan and the history of instructions executed so far, the first instruction should be ‚Äò\\n***************************************************************\\nAssistantAgent to Executor agent:\\nclickxpath //button[@id=‚Äòsubbtn‚Äô]\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nHold on, please wait for my next instruction.\\n***************************************************************\\nAssistantAgent to Executor agent:\\nSure, I‚Äôll be here when you‚Äôre ready with your next instruction.\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nBelow is the HTML code of the webpage where the agent should solve a task.\\n1<div id=\" wrap \" data - wob_ref =\"2\" data - wob_eps =\"e0\">\\n2 <div id=\" query \">Click button ONE , then click button TWO .</div >\\n3 <div id=\" area \" data - wob_ref =\"3\" data - wob_eps =\"e0\">\\n4 <button id=\" subbtn \" style =\" position : absolute ; left :103 px; top :87 px\" data -\\nwob_ref =\"4\" data - wob_eps =\"e0\" data - tampered =\"e0\">ONE </ button >\\n5 <button id=\" subbtn2 \" style =\" position : absolute ; left :44 px; top :97 px\" data -\\nwob_ref =\"5\" data - wob_eps =\"e0\">TWO </ button >\\n6 </div >\\n7</div >\\nCurrent task: Click button ONE, then click button TWO.\\nHere is a plan you are following now.\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\nWe have a history of instructions that have been already executed by the autonomous agent so far.\\n1: clickxpath //button[@id=‚Äòsubbtn‚Äô]\\nBased on the plan and the history of instructions executed so far, the next proper instruction should be ‚Äò\\n***************************************************************\\nAssistantAgent to Executor agent:\\nclickxpath //button[@id=‚Äòsubbtn2‚Äô]\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nSUCCESS!!!!\\n TERMINATE\\n43', metadata={'source': 'utils\\\\data\\\\autogen.pdf', 'page': 42})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='AutoGen : Enabling Next-Gen LLM\\nApplications via Multi-Agent Conversation\\nQingyun Wu‚Ä†, Gagan Bansal‚àó, Jieyu Zhang¬±, Yiran Wu‚Ä†, Beibin Li‚àó\\nErkang Zhu‚àó, Li Jiang‚àó, Xiaoyun Zhang‚àó, Shaokun Zhang‚Ä†, Jiale Liu‚àì\\nAhmed Awadallah‚àó, Ryen W. White‚àó, Doug Burger‚àó, Chi Wang‚àó1\\n‚àóMicrosoft Research,‚Ä†Pennsylvania State University\\n¬±University of Washington,‚àìXidian University\\nAgent CustomizationConversable agent\\nFlexible Conversation Patterns\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\nHierarchical chatJoint chatMulti-Agent Conversations‚Ä¶Execute the following code‚Ä¶\\nGot it! Here is the revised code ‚Ä¶No, please plot % change!Plot a chart of META and TESLA stock price change YTD.\\nOutput:$Month\\nOutput:%MonthError package yfinanceis not installed\\nSorry! Please first pip install yfinanceand then execute the code\\nInstalling‚Ä¶\\nExample Agent Chat\\nFigure 1: AutoGen enables diverse LLM-based applications using multi-agent conversations. (Left)\\nAutoGen agents are conversable, customizable, and can be based on LLMs, tools, humans, or even\\na combination of them. (Top-middle) Agents can converse to solve tasks. (Right) They can form\\na chat, potentially with humans in the loop. (Bottom-middle) The framework supports flexible\\nconversation patterns.\\nAbstract\\nAutoGen2is an open-source framework that allows developers to build LLM ap-\\nplications via multiple agents that can converse with each other to accomplish\\ntasks. AutoGen agents are customizable, conversable , and can operate in vari-\\nous modes that employ combinations of LLMs, human inputs, and tools. Using\\nAutoGen , developers can also flexibly define agent interaction behaviors. Both\\nnatural language and computer code can be used to program flexible conversation\\npatterns for different applications. AutoGen serves as a generic framework for\\nbuilding diverse applications of various complexities and LLM capacities. Em-\\npirical studies demonstrate the effectiveness of the framework in many example\\napplications, with domains ranging from mathematics, coding, question answer-\\ning, operations research, online decision-making, entertainment, etc.\\n1Corresponding author. Email: auto-gen@outlook.com\\n2https://github.com/microsoft/autogenarXiv:2308.08155v2  [cs.AI]  3 Oct 2023', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 0}),\n",
       " Document(page_content='1 Introduction\\nLarge language models (LLMs) are becoming a crucial building block in developing powerful agents\\nthat utilize LLMs for reasoning, tool usage, and adapting to new observations (Yao et al., 2022; Xi\\net al., 2023; Wang et al., 2023b) in many real-world tasks. Given the expanding tasks that could\\nbenefit from LLMs and the growing task complexity, an intuitive approach to scale up the power of\\nagents is to use multiple agents that cooperate. Prior work suggests that multiple agents can help\\nencourage divergent thinking (Liang et al., 2023), improve factuality and reasoning (Du et al., 2023),\\nand provide validation (Wu et al., 2023). In light of the intuition and early evidence of promise, it is\\nintriguing to ask the following question: how can we facilitate the development of LLM applications\\nthat could span a broad spectrum of domains and complexities based on the multi-agent approach?\\nOur insight is to use multi-agent conversations to achieve it. There are at least three reasons con-\\nfirming its general feasibility and utility thanks to recent advances in LLMs: First, because chat-\\noptimized LLMs (e.g., GPT-4) show the ability to incorporate feedback, LLM agents can cooperate\\nthrough conversations with each other or human(s), e.g., a dialog where agents provide and seek rea-\\nsoning, observations, critiques, and validation. Second, because a single LLM can exhibit a broad\\nrange of capabilities (especially when configured with the correct prompt and inference settings),\\nconversations between differently configured agents can help combine these broad LLM capabilities\\nin a modular and complementary manner. Third, LLMs have demonstrated ability to solve complex\\ntasks when the tasks are broken into simpler subtasks. Multi-agent conversations can enable this\\npartitioning and integration in an intuitive manner. How can we leverage the above insights and\\nsupport different applications with the common requirement of coordinating multiple agents, poten-\\ntially backed by LLMs, humans, or tools exhibiting different capacities? We desire a multi-agent\\nconversation framework with generic abstraction and effective implementation that has the flexibil-\\nity to satisfy different application needs. Achieving this requires addressing two critical questions:\\n(1) How can we design individual agents that are capable, reusable, customizable, and effective in\\nmulti-agent collaboration? (2) How can we develop a straightforward, unified interface that can\\naccommodate a wide range of agent conversation patterns? In practice, applications of varying\\ncomplexities may need distinct sets of agents with specific capabilities, and may require different\\nconversation patterns, such as single- or multi-turn dialogs, different human involvement modes, and\\nstatic vs. dynamic conversation. Moreover, developers may prefer the flexibility to program agent\\ninteractions in natural language or code. Failing to adequately address these two questions would\\nlimit the framework‚Äôs scope of applicability and generality.\\nWhile there is contemporaneous exploration of multi-agent approaches,3we present AutoGen , a\\ngeneralized multi-agent conversation framework (Figure 1), based on the following new concepts.\\n1Customizable and conversable agents. AutoGen uses a generic design of agents that can lever-\\nage LLMs, human inputs, tools, or a combination of them. The result is that developers can\\neasily and quickly create agents with different roles (e.g., agents to write code, execute code,\\nwire in human feedback, validate outputs, etc.) by selecting and configuring a subset of built-in\\ncapabilities. The agent‚Äôs backend can also be readily extended to allow more custom behaviors.\\nTo make these agents suitable for multi-agent conversation, every agent is made conversable ‚Äì\\nthey can receive, react, and respond to messages. When configured properly, an agent can hold\\nmultiple turns of conversations with other agents autonomously or solicit human inputs at cer-\\ntain rounds, enabling human agency and automation. The conversable agent design leverages the\\nstrong capability of the most advanced LLMs in taking feedback and making progress via chat\\nand also allows combining capabilities of LLMs in a modular fashion. (Section 2.1)\\n2Conversation programming. A fundamental insight of AutoGen is to simplify and unify com-\\nplex LLM application workflows as multi-agent conversations. So AutoGen adopts a program-\\nming paradigm centered around these inter-agent conversations. We refer to this paradigm as\\nconversation programming , which streamlines the development of intricate applications via two\\nprimary steps: (1) defining a set of conversable agents with specific capabilities and roles (as\\ndescribed above); (2) programming the interaction behavior between agents via conversation-\\ncentric computation andcontrol . Both steps can be achieved via a fusion of natural and pro-\\ngramming languages to build applications with a wide range of conversation patterns and agent\\nbehaviors. AutoGen provides ready-to-use implementations and also allows easy extension and\\nexperimentation for both steps. (Section 2.2)\\n3We refer to Appendix A for a detailed discussion.\\n2', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 1}),\n",
       " Document(page_content='AutoGen also provides a collection of multi-agent applications created using conversable agents\\nand conversation programming. These applications demonstrate how AutoGen can easily support\\napplications of various complexities and LLMs of various capabilities. Moreover, we perform both\\nevaluation on benchmarks and a pilot study of new applications. The results show that AutoGen can\\nhelp achieve outstanding performance on many tasks, and enable innovative ways of using LLMs,\\nwhile reducing development effort. (Section 3 and Appendix D)\\n2 The AutoGen Framework\\nTo reduce the effort required for developers to create complex LLM applications across various do-\\nmains, a core design principle of AutoGen is to streamline and consolidate multi-agent workflows\\nusing multi-agent conversations. This approach also aims to maximize the reusability of imple-\\nmented agents. This section introduces the two key concepts of AutoGen : conversable agents and\\nconversation programming.\\n2.1 Conversable Agents\\nInAutoGen , aconversable agent is an entity with a specific role that can pass messages to send and\\nreceive information to and from other conversable agents, e.g., to start or continue a conversation. It\\nmaintains its internal context based on sent and received messages and can be configured to possess\\na set of capabilities, e.g., enabled by LLMs, tools, or human input, etc. The agents can act according\\nto programmed behavior patterns described next.\\nAgent capabilities powered by LLMs, humans, and tools. Since an agent‚Äôs capabilities directly\\ninfluence how it processes and responds to messages, AutoGen allows flexibility to endow its agents\\nwith various capabilities. AutoGen supports many common composable capabilities for agents,\\nincluding 1) LLMs. LLM-backed agents exploit many capabilities of advanced LLMs such as role\\nplaying, implicit state inference and progress making conditioned on conversation history, providing\\nfeedback, adapting from feedback, and coding. These capabilities can be combined in different ways\\nvia novel prompting techniques4to increase an agent‚Äôs skill and autonomy. AutoGen also offers\\nenhanced LLM inference features such as result caching, error handling, message templating, etc.,\\nvia an enhanced LLM inference layer. 2) Humans. Human involvement is desired or even essential\\nin many LLM applications. AutoGen lets a human participate in agent conversation via human-\\nbacked agents, which could solicit human inputs at certain rounds of a conversation depending on\\nthe agent configuration. The default user proxy agent allows configurable human involvement levels\\nand patterns, e.g., frequency and conditions for requesting human input including the option for\\nhumans to skip providing input. 3) Tools. Tool-backed agents have the capability to execute tools\\nvia code execution or function execution. For example, the default user proxy agent in AutoGen is\\nable to execute code suggested by LLMs, or make LLM-suggested function calls.\\nAgent customization and cooperation. Based on application-specific needs, each agent can be\\nconfigured to have a mix of basic back-end types to display complex behavior in multi-agent con-\\nversations. AutoGen allows easy creation of agents with specialized capabilities and roles by reusing\\nor extending the built-in agents. The yellow-shaded area of Figure 2 provides a sketch of the built-in\\nagents in AutoGen . The ConversableAgent class is the highest-level agent abstraction and, by\\ndefault, can use LLMs, humans, and tools. The AssistantAgent andUserProxyAgent are two\\npre-configured ConversableAgent subclasses, each representing a common usage mode, i.e., act-\\ning as an AI assistant (backed by LLMs) and acting as a human proxy to solicit human input or\\nexecute code/function calls (backed by humans and/or tools).\\nIn the example on the right-hand side of Figure 1, an LLM-backed assistant agent and a tool- and\\nhuman-backed user proxy agent are deployed together to tackle a task. Here, the assistant agent\\ngenerates a solution with the help of LLMs and passes the solution to the user proxy agent. Then,\\nthe user proxy agent solicits human inputs or executes the assistant‚Äôs code and passes the results as\\nfeedback back to the assistant.\\n4Appendix C presents an example of such novel prompting techniques which empowers the default LLM-\\nbacked assistant agent in AutoGen to converse with other agents in multi-step problem solving.\\n3', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 2}),\n",
       " Document(page_content='2 Initiate Conversations:A.initiate_chat(‚ÄúPlot a chart of META and TESLA stock price change YTD.‚Äù, B)\\nAssistant BUser Proxy AAutoGenAgents\\nDeveloper Code# This funcwill be invoked in generate_replyA.register_reply(B,  reply_func_A2B)def reply_func_A2B(msg):ouput= input_from_human()‚Ä¶if not ouput:if msg includes code:output = execute(msg)return outputConversableAgent\\nAssistantAgentUserProxyAgenthuman_input_mode= ‚ÄúNEVER‚Äùcode_execution_config= FalseDEFAULT_SYSTEM_MESSAGE = ‚ÄúYou are a helpful AI assistant‚Ä¶In the following cases, suggest python code‚Ä¶‚Äùhuman_input_mode=‚ÄúALWAYS‚Äù\\nGroupChatManagerhuman_input_mode= ‚ÄúNEVER‚Äùgroup_chat= [              ] \\n# Note: when no reply funcis registered, a list of default reply functions will be used. Agent Customization:\\nProgram ExecutionPlot a chart of META and TESLA stock price change YTD.Execute the following code‚Ä¶sendreceivereceiveConversation-Centric Computationgenerate_replyError: package yfinanceis not installedsendgenerate_replySorry! Please first pip install yfinanceand then executeConversation-Driven Control Flowgenerate_replyThe Resulting Automated Agent Chat:‚Ä¶1.2 Register a Custom Reply Func:1.1 Define Agents:Unified Conversation Interfaces:‚Ä¢send‚Ä¢receive ‚Ä¢generate_reply\\nFigure 2: Illustration of how to use AutoGen to program a multi-agent conversation. The top sub-\\nfigure illustrates the built-in agents provided by AutoGen , which have unified conversation interfaces\\nand can be customized. The middle sub-figure shows an example of using AutoGen to develop\\na two-agent system with a custom reply function. The bottom sub-figure illustrates the resulting\\nautomated agent chat from the two-agent system during program execution.\\nBy allowing custom agents that can converse with each other, conversable agents in AutoGen serve\\nas a useful building block. However, to develop applications where agents make meaningful progress\\non tasks, developers also need to be able to specify and mold these multi-agent conversations.\\n2.2 Conversation Programming\\nAs a solution to the above problem, AutoGen utilizes conversation programming , a paradigm that\\nconsiders two concepts: the first is computation ‚Äì the actions agents take to compute their response\\nin a multi-agent conversation. And the second is control flow ‚Äì the sequence (or conditions) un-\\nder which these computations happen. As we will show in the applications section, the ability to\\nprogram these helps implement many flexible multi-agent conversation patterns. In AutoGen , these\\ncomputations are conversation-centric. An agent takes actions relevant to the conversations it is\\ninvolved in and its actions result in message passing for consequent conversations (unless a termina-\\ntion condition is satisfied). Similarly, control flow is conversation-driven ‚Äì the participating agents‚Äô\\ndecisions on which agents to send messages to and the procedure of computation are functions of the\\ninter-agent conversation. This paradigm helps one to reason intuitively about a complex workflow\\nas agent action taking and conversation message-passing between agents.\\nFigure 2 provides a simple illustration. The bottom sub-figure shows how individual agents perform\\ntheir role-specific, conversation-centric computations to generate responses (e.g., via LLM inference\\ncalls and code execution). The task progresses through conversations displayed in the dialog box.\\nThe middle sub-figure demonstrates a conversation-based control flow. When the assistant receives\\na message, the user proxy agent typically sends the human input as a reply. If there is no input, it\\nexecutes any code in the assistant‚Äôs message instead.\\n4', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 3}),\n",
       " Document(page_content='AutoGen features the following design patterns to facilitate conversation programming:\\n1.Unified interfaces and auto-reply mechanisms for automated agent chat. Agents in\\nAutoGen have unified conversation interfaces for performing the corresponding conversation-\\ncentric computation, including a send/receive function for sending/receiving messages and a\\ngenerate reply function for taking actions and generating a response based on the received\\nmessage. AutoGen also introduces and by default adopts an agent auto-reply mechanism to\\nrealize conversation-driven control: Once an agent receives a message from another agent, it au-\\ntomatically invokes generate reply and sends the reply back to the sender unless a termination\\ncondition is satisfied. AutoGen provides built-in reply functions based on LLM inference, code\\nor function execution, or human input. One can also register custom reply functions to customize\\nthe behavior pattern of an agent, e.g., chatting with another agent before replying to the sender\\nagent. Under this mechanism, once the reply functions are registered, and the conversation is\\ninitialized, the conversation flow is naturally induced, and thus the agent conversation proceeds\\nnaturally without any extra control plane, i.e., a special module that controls the conversation\\nflow. For example, with the developer code in the blue-shaded area (marked ‚ÄúDeveloper Code‚Äù)\\nof Figure 2, one can readily trigger the conversation among the agents, and the conversation\\nwould proceed automatically, as shown in the dialog box in the grey shaded area (marked ‚ÄúPro-\\ngram Execution‚Äù) of Figure 2. The auto-reply mechanism provides a decentralized, modular, and\\nunified way to define the workflow.\\n2.Control by fusion of programming and natural language. AutoGen allows the usage of\\nprogramming and natural language in various control flow management patterns: 1) Natural-\\nlanguage control via LLMs. InAutoGen , one can control the conversation flow by prompting\\nthe LLM-backed agents with natural language. For instance, the default system message of the\\nbuilt-in AssistantAgent inAutoGen uses natural language to instruct the agent to fix errors\\nand generate code again if the previous result indicates there are errors. It also guides the agent\\nto confine the LLM output to certain structures, making it easier for other tool-backed agents to\\nconsume. For example, instructing the agent to reply with ‚ÄúTERMINATE‚Äù when all tasks are\\ncompleted to terminate the program. More concrete examples of natural language controls can\\nbe found in Appendix C. 2) Programming-language control. InAutoGen , Python code can be\\nused to specify the termination condition, human input mode, and tool execution logic, e.g., the\\nmax number of auto replies. One can also register programmed auto-reply functions to control\\nthe conversation flow with Python code, as shown in the code block identified as ‚ÄúConversation-\\nDriven Control Flow‚Äù in Figure 2. 3) Control transition between natural and programming\\nlanguage. AutoGen also supports flexible control transition between natural and programming\\nlanguage. One can achieve transition from code to natural-language control by invoking an LLM\\ninference containing certain control logic in a customized reply function; or transition from nat-\\nural language to code control via LLM-proposed function calls (Eleti et al., 2023).\\nIn the conversation programming paradigm, one can realize multi-agent conversations of diverse\\npatterns. In addition to static conversation with predefined flow, AutoGen also supports dynamic\\nconversation flows with multiple agents. AutoGen provides two general ways to achieve this: 1)\\nCustomized generate reply function: within the customized generate reply function, one\\nagent can hold the current conversation while invoking conversations with other agents depending\\non the content of the current message and context. 2) Function calls: In this approach, LLM decides\\nwhether or not to call a particular function depending on the conversation status. By messaging\\nadditional agents in the called functions, the LLM can drive dynamic multi-agent conversation. In\\naddition, AutoGen supports more complex dynamic group chat via built-in GroupChatManager ,\\nwhich can dynamically select the next speaker and then broadcast its response to other agents. We\\nelaborate on this feature and its application in Section 3. We provide implemented working systems\\nto showcase all these different patterns, with some of them visualized in Figure 3.\\n3 Applications of AutoGen\\nWe demonstrate six applications using AutoGen (see Figure 3) to illustrate its potential in simplify-\\ning the development of high-performance multi-agent applications. These applications are selected\\nbased on their real-world relevance (A1, A2, A4, A5, A6), problem difficulty and solving capabil-\\nities enabled by AutoGen (A1, A2, A3, A4), and innovative potential (A5, A6). Together, these\\ncriteria showcase AutoGen ‚Äôs role in advancing the LLM-application landscape.\\n5', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 4}),\n",
       " Document(page_content='A1. Math Problem Solving\\nA4. Multi-agent CodingCommander\\nSafeguard\\nWriter\\nA6. Conversational ChessA2. Retrieval-augmented ChatRetrieval-augmentedAssistantRetrieval-augmentedUser Proxy\\nChess Board\\nHuman/AI Chess Player A\\nHuman/AI Chess Player B\\nStudent\\nAssistant\\nAssistantExpert\\nAsk  expert\\nBroadcast\\nManager\\nSpeak\\nA5. Dynamic Group Chat\\nALFWorldExecutorAssistant\\nGrounding Agent\\nA3. ALF ChatFigure 3: Six examples of diverse applications built using AutoGen . Their conversation patterns\\nshow AutoGen ‚Äôs flexibility and power.\\nA1: Math Problem Solving\\nMathematics is a foundational discipline and the promise of leveraging LLMs to assist with math\\nproblem solving opens up a new plethora of applications and avenues for exploration, including per-\\nsonalized AI tutoring, AI research assistance, etc. This section demonstrates how AutoGen can help\\ndevelop LLM applications for math problem solving, showcasing strong performance and flexibility\\nin supporting various problem-solving paradigms.\\n(Scenario 1 ) We are able to build a system for autonomous math problem solving by directly reusing\\ntwo built-in agents from AutoGen . We evaluate our system and several alternative approaches,\\nincluding open-source methods such as Multi-Agent Debate (Liang et al., 2023), LangChain Re-\\nAct (LangChain, 2023), vanilla GPT-4, and commercial products ChatGPT + Code Interpreter, and\\nChatGPT + Plugin (Wolfram Alpha), on the MATH (Hendrycks et al., 2021) dataset and summarize\\nthe results in Figure 4a. We perform evaluations over 120 randomly selected level-5 problems and\\non the entire5test dataset from MATH. The results show that the built-in agents from AutoGen al-\\nready yield better performance out of the box compared to the alternative approaches, even including\\nthe commercial ones. ( Scenario 2 ) We also showcase a human-in-the-loop problem-solving process\\nwith the help of AutoGen . To incorporate human feedback with AutoGen , one only needs to set\\nhuman input mode=‚ÄòALWAYS‚Äô in the UserProxyAgent of the system in scenario 1. We demon-\\nstrate that this system can effectively incorporate human inputs to solve challenging problems that\\ncannot be solved without humans. ( Scenario 3 ) We further demonstrate a novel scenario where\\nmultiple human users can participate in the conversations during the problem-solving process. Our\\nexperiments and case studies for these scenarios show that AutoGen enables better performance or\\nnew experience compared to other solutions we experimented with. Due to the page limit, details of\\nthe evaluation, including case studies in three scenarios are in Appendix D.\\nA2: Retrieval-Augmented Code Generation and Question Answering\\nRetrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic\\nlimitations of LLMs by incorporating external documents. In this section, we employ AutoGen to\\nbuild a Retrieval-Augmented Generation (RAG) system (Lewis et al., 2020; Parvez et al., 2021)\\nnamed Retrieval-augmented Chat. The system consists of two agents: a Retrieval-augmented User\\nProxy agent and a Retrieval-augmented Assistant agent, both of which are extended from built-in\\nagents from AutoGen . The Retrieval-augmented User Proxy includes a vector database (Chroma,\\n5We did not evaluate ChatGPT on the whole dataset since it requires substantial manual effort and is re-\\nstricted by its hourly message-number limitation. Multi-agent debate and LangChain ReAct were also not\\nevaluated since they underperformed vanilla GPT-4 on the smaller test set.\\n6', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 5}),\n",
       " Document(page_content='AutoGen ChatGPT\\n+CodeChatGPT\\n+PluginGPT-4 Multi-Agent\\nDebateLangChain\\nReAct\\nMethods01020304050607080Success Ratio (%)52.5%\\n48.33%\\n45.0%\\n30.0%\\n26.67%\\n23.33%69.48%\\n55.18%120 Level-5 problems\\nWhole Dataset(a) A1: Performance on MATH (w/ GPT-4).\\nF1 Recall\\nMetrics01020304050607080Percentage (%)25.88%66.65%\\n15.12%58.56%\\n22.79%62.59%AutoGen\\nAuotGen W/O interactive retrieval\\nDPR (b) A2: Q&A tasks (w/ GPT-3.5).\\nAutoGen (3 agent) AutoGen (2 agent) ReAct\\nMethods020406080100Success Ratio (%)69%\\n54% 54%77%\\n63%66%Average\\nBest of 3\\n(c) A3: Performance on ALFWorld.\\nF1 Recall\\nMetrics020406080100Percentage (%)96.00%98.00%\\n88.00%\\n78.00%83.00%\\n72.00%\\n48.00%\\n32.00%Multi-GPT4\\nSingle-GPT4\\nMulti-GPT3.5\\nSingle-GPT3.5 (d) A4: Performance on OptiGuide.\\nFigure 4: Performance on four applications A1-A4. (a) shows that AutoGen agents can be used\\nout of the box to achieve the most competitive performance on math problem solving tasks; (b)\\nshows that AutoGen can be used to realize effective retrieval augmentation and realize a novel\\ninteractive retrieval feature to boost performance on Q&A tasks; (c) shows that AutoGen can be used\\nto introduce a three-agent system with a grounding agent to improve performance on ALFWorld;\\n(d) shows that a multi-agent design is helpful in boosting performance in coding tasks that need\\nsafeguards.\\n2023) with SentenceTransformers (Reimers & Gurevych, 2019) as the context retriever. A detailed\\nworkflow description of the Retrieval-augmented Chat is provided in Appendix D.\\nWe evaluate Retrieval-augmented Chat in both question-answering and code-generation scenarios.\\n(Scenario 1 ) We first perform an evaluation regarding natural question answering on the Natural\\nQuestions dataset (Kwiatkowski et al., 2019) and report results in Figure 4b. In this evaluation, we\\ncompare our system with DPR (Dense Passage Retrieval) following an existing evaluation6prac-\\ntice (Adlakha et al., 2023). Leveraging the conversational design and natural-language control,\\nAutoGen introduces a novel interactive retrieval feature in this application: whenever the retrieved\\ncontext does not contain the information, instead of terminating, the LLM-based assistant would\\nreply ‚Äú Sorry, I cannot find any information about... UPDATE CONTEXT. ‚Äù which will invoke more\\nretrieval attempts. We conduct an ablation study in which we prompt the assistant agent to say ‚ÄúI\\ndon‚Äôt know‚Äù instead of ‚ÄúUPDATE CONTEXT. ‚Äù in cases where relevant information is not found,\\nand report results in Figure 4b. The results show that the interactive retrieval mechanism indeed\\nplays a non-trivial role in the process. We give a concrete example and results using this appealing\\nfeature in Appendix D. ( Scenario 2 ) We further demonstrate how Retrieval-augmented Chat aids in\\ngenerating code based on a given codebase that contains code not included in GPT-4‚Äôs training data.\\nEvaluation and demonstration details for both scenarios are included in Appendix D.\\n6The results of DPR with GPT-3.5 shown in Figure 4b are from (Adlakha et al., 2023). We use GPT-3.5 as\\na shorthand for GPT-3.5-turbo.\\n7', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 6}),\n",
       " Document(page_content='A3: Decision Making in Text World Environments\\nIn this subsection, we demonstrate how AutoGen can be used to develop effective applications that\\ninvolve interactive or online decision making. We perform the study using the ALFWorld (Shridhar\\net al., 2021) benchmark, which includes a diverse collection of synthetic language-based interactive\\ndecision-making tasks in household environments.\\nWith AutoGen , we implemented a two-agent system to solve tasks from ALFWorld. It consists of\\nan LLM-backed assistant agent responsible for suggesting plans to conduct a task and an executor\\nagent responsible for executing actions in the ALFWorld environments. This system integrates Re-\\nAct prompting (Yao et al., 2022), and is able to achieve similar performance. A common challenge\\nencountered in both ReAct and the AutoGen -based two-agent system is their occasional inability to\\nleverage basic commonsense knowledge about the physical world. This deficiency can lead to the\\nsystem getting stuck in a loop due to repetitive errors. Fortunately, the modular design of AutoGen\\nallows us to address this issue effectively: With AutoGen , we are able to introduce a grounding\\nagent, which supplies crucial commonsense knowledge‚Äìsuch as ‚ÄúYou must find and take the object\\nbefore you can examine it. You must go to where the target object is before you can use it. ‚Äù ‚Äìwhenever\\nthe system exhibits early signs of recurring errors. It significantly enhances the system‚Äôs ability to\\navoid getting entangled in error loops. We compare the task-solving performance of the two variants\\nof our system with GPT-3.5-turbo and ReAct7on the 134 unseen tasks from ALFWorld and report\\nresults in Figure 4c. The results show that introducing a grounding agent could bring in a 15%\\nperformance gain on average. Upon examining the systems‚Äô outputs, we observe that the grounding\\nagent, by delivering background commonsense knowledge at the right junctures, significantly miti-\\ngated the tendency of the system to persist with a flawed plan, thereby avoiding the creation of error\\nloops. For an example trajectory comparing the systems see Appendix D, Figure 10.\\nA4: Multi-Agent Coding\\nIn this subsection, we use AutoGen to build a multi-agent coding system based on OptiGuide (Li\\net al., 2023a), a system that excels at writing code to interpret optimization solutions and answer\\nuser questions, such as exploring the implications of changing a supply-chain decision or under-\\nstanding why the optimizer made a particular choice. The second sub-figure of Figure 3 shows the\\nAutoGen -based implementation. The workflow is as follows: the end user sends questions, such as\\n‚ÄúWhat if we prohibit shipping from supplier 1 to roastery 2? ‚Äù to the Commander agent. The Com-\\nmander coordinates with two assistant agents, including the Writer and the Safeguard, to answer\\nthe question. The Writer will craft code and send the code to the Commander. After receiving the\\ncode, the Commander checks the code safety with the Safeguard; if cleared, the Commander will\\nuse external tools (e.g., Python) to execute the code, and request the Writer to interpret the execution\\nresults. For instance, the writer may say ‚Äú if we prohibit shipping from supplier 1 to roastery 2, the\\ntotal cost would increase by 10.5%. ‚Äù The Commander then provides this concluding answer to the\\nend user. If, at a particular step, there is an exception, e.g., security red flag raised by Safeguard, the\\nCommander redirects the issue back to the Writer with debugging information. The process might\\nbe repeated multiple times until the user‚Äôs question is answered or timed-out.\\nWith AutoGen the core workflow code for OptiGuide was reduced from over 430 lines to 100 lines,\\nleading to significant productivity improvement. We provide a detailed comparison of user expe-\\nrience with ChatGPT+Code Interpreter and AutoGen -based OptiGuide in Appendix D, where we\\nshow that AutoGen -based OptiGuide could save around 3x of user‚Äôs time and reduce user interac-\\ntions by 3 - 5 times on average. We also conduct an ablation showing that multi-agent abstraction is\\nnecessary. Specifically, we construct a single-agent approach where a single agent conducts both the\\ncode-writing and safeguard processes. We tested the single- and multi-agent approaches on a dataset\\nof 100 coding tasks, which is crafted to include equal numbers of safe and unsafe tasks. Evaluation\\nresults as reported in Figure 4d show that the multi-agent design boosts the F-1 score in identifying\\nunsafe code by 8% (with GPT-4) and 35% (with GPT-3.5-turbo).\\n7Results of ReAct are obtained by directly running its official code with default settings. The code uses\\ntext-davinci-003 as backend LM and does not support GPT-3.5-turbo or GPT-4.\\n8', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 7}),\n",
       " Document(page_content='A5: Dynamic Group Chat\\nAutoGen provides native support for a dynamic group chat communication pattern, in which par-\\nticipating agents share the same context and converse with the others in a dynamic manner instead\\nof following a pre-defined order. Dynamic group chat relies on ongoing conversations to guide the\\nflow of interaction among agents. These make dynamic group chat ideal for situations where col-\\nlaboration without strict communication order is beneficial. In AutoGen , the GroupChatManager\\nclass serves as the conductor of conversation among agents and repeats the following three steps:\\ndynamically selecting a speaker, collecting responses from the selected speaker, and broadcasting\\nthe message (Figure 3-A5). For the dynamic speaker-selection component, we use a role-play style\\nprompt. Through a pilot study on 12 manually crafted complex tasks, we observed that compared\\nto a prompt that is purely based on the task, utilizing a role-play prompt often leads to more effec-\\ntive consideration of both conversation context and role alignment during the problem-solving and\\nspeaker-selection process. Consequently, this leads to a higher success rate and fewer LLM calls.\\nWe include detailed results in Appendix D.\\nA6: Conversational Chess\\nUsing AutoGen , we developed Conversational Chess, a natural language interface game shown in\\nthe last sub-figure of Figure 3. It features built-in agents for players, which can be human or LLM,\\nand a third-party board agent to provide information and validate moves based on standard rules.\\nWith AutoGen , we enabled two essential features: (1) Natural, flexible, and engaging game dynam-\\nics, enabled by the customizable agent design in AutoGen . Conversational Chess supports a range\\nof game-play patterns, including AI-AI, AI-human, and human-human, with seamless switching\\nbetween these modes during a single game. An illustrative example of these entertaining game dy-\\nnamics can be found in Figure 15, Appendix D. (2) Grounding, which is a crucial aspect to maintain\\ngame integrity. During gameplay, the board agent checks each proposed move for legality; if a move\\nis invalid, the agent responds with an error, prompting the player agent to re-propose a legal move\\nbefore continuing. This process ensures that only valid moves are played and helps maintain a con-\\nsistent gaming experience. As an ablation study, we removed the board agent and instead only relied\\non a relevant prompt ‚Äúyou should make sure both you and the opponent are making legal moves‚Äù to\\nground their move. The results highlighted that without the board agent, illegitimate moves caused\\ngame disruptions. The modular design offered flexibility, allowing swift adjustments to the board\\nagent in response to evolving game rules or varying chess rule variants. A comprehensive demon-\\nstration of this ablation study is in Appendix D.\\n4 Discussion\\nWe introduced an open-source library, AutoGen , that incorporates the paradigms of conversable\\nagents and conversation programming. This library utilizes capable agents that are well-suited for\\nmulti-agent cooperation. It features a unified conversation interface among the agents, along with\\nan auto-reply mechanisms, which help establish an agent-interaction interface that capitalizes on the\\nstrengths of chat-optimized LLMs with broad capabilities while accommodating a wide range of\\napplications. AutoGen serves as a general framework for creating and experimenting with multi-\\nagent systems that can easily fulfill various practical requirements, such as reusing, customizing,\\nand extending existing agents, as well as programming conversations between them.\\nOur experiments, as detailed in Section 3, demonstrate that this approach offers numerous benefits.\\nThe adoption of AutoGen has resulted in improved performance (over state-of-the-art approaches),\\nreduced development code, and decreased manual burden for existing applications. It offers flex-\\nibility to developers, as demonstrated in A1 (scenario 3), A5, and A6, where AutoGen enables\\nmulti-agent chats to follow a dynamic pattern rather than fixed back-and-forth interactions. It allows\\nhumans to engage in activities alongside multiple AI agents in a conversational manner. Despite the\\ncomplexity of these applications (most involving more than two agents or dynamic multi-turn agent\\ncooperation), the implementation based on AutoGen remains straightforward. Dividing tasks among\\nseparate agents promotes modularity. Furthermore, since each agent can be developed, tested, and\\nmaintained separately, this approach simplifies overall development and code management.\\n9', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 8}),\n",
       " Document(page_content='Although this work is still in its early experimental stages, it paves the way for numerous future\\ndirections and research opportunities. For instance, we can explore effective integration of existing\\nagent implementations into our multi-agent framework and investigate the optimal balance between\\nautomation and human control in multi-agent workflows. As we further develop and refine AutoGen ,\\nwe aim to investigate which strategies, such as agent topology and conversation patterns, lead to the\\nmost effective multi-agent conversations while optimizing the overall efficiency, among other fac-\\ntors. While increasing the number of agents and other degrees of freedom presents opportunities for\\ntackling more complex problems, it may also introduce new safety challenges that require additional\\nstudies and careful consideration.\\nWe provide more discussion in Appendix B, including guidelines for using AutoGen and direction\\nof future work. We hope AutoGen will help improve many LLM applications in terms of speed of\\ndevelopment, ease of experimentation, and overall effectiveness and safety. We actively welcome\\ncontributions from the broader community.\\nEthics statement\\nThere are several potential ethical considerations that could arise from the development and use of\\ntheAutoGen framework.\\n‚Ä¢ Privacy and Data Protection: The framework allows for human participation in conversations\\nbetween agents. It is important to ensure that user data and conversations are protected, and that\\ndevelopers use appropriate measures to safeguard privacy.\\n‚Ä¢ Bias and Fairness: LLMs have been shown to exhibit biases present in their training data (Navigli\\net al., 2023). When using LLMs in the AutoGen framework, it is crucial to address and mitigate\\nany biases that may arise in the conversations between agents. Developers should be aware of\\npotential biases and take steps to ensure fairness and inclusivity.\\n‚Ä¢ Accountability and Transparency: As discussed in the future work section, as the framework in-\\nvolves multiple agents conversing and cooperating, it is important to establish clear accountability\\nand transparency mechanisms. Users should be able to understand and trace the decision-making\\nprocess of the agents involved in order to ensure accountability and address any potential issues\\nor biases.\\n‚Ä¢ Trust and Reliance: AutoGen leverages human understanding and intelligence while providing\\nautomation through conversations between agents. It is important to consider the impact of this\\ninteraction on user experience, trust, and reliance on AI systems. Clear communication and user\\neducation about the capabilities and limitations of the system will be essential (Cai et al., 2019).\\n‚Ä¢ Unintended Consequences: As discussed before, the use of multi-agent conversations and automa-\\ntion in complex tasks may have unintended consequences. In particular, allowing LLM agents to\\nmake changes in external environments through code execution or function calls, such as installing\\npackages, could be risky. Developers should carefully consider the potential risks and ensure that\\nappropriate safeguards are in place to prevent harm or negative outcomes.\\nAcknowledgements\\nThe work presented in this report was made possible through discussions and feedback from Peter\\nLee, Johannes Gehrke, Eric Horvitz, Steven Lucco, Umesh Madan, Robin Moeur, Piali Choud-\\nhury, Saleema Amershi, Adam Fourney, Victor Dibia, Guoqing Zheng, Corby Rosset, Ricky Loynd,\\nEce Kamar, Rafah Hosn, John Langford, Ida Momennejad, Brian Krabach, Taylor Webb, Shanka\\nSubhra Mondal, Wei-ge Chen, Robert Gruen, Yinan Li, Yue Wang, Suman Nath, Tanakorn Leesat-\\napornwongsa, Xin Wang, Shishir Patil, Tianjun Zhang, Saehan Jo, Ishai Menache, Kontantina Mel-\\nlou, Runlong Zhou, Feiran Jia, Hamed Khanpour, Hamid Palangi, Srinagesh Sharma, Julio Albinati\\nCortez, Amin Saied, Yuzhe Ma, Dujian Ding, Linyong Nan, Prateek Yadav, Shannon Shen, Ankur\\nMallick, Mark Encarnaci ¬¥on, Lars Liden, Tianwei Yue, Julia Kiseleva, Anastasia Razdaibiedina, and\\nLuciano Del Corro. Qingyun Wu would like to acknowledge the funding and research support from\\nthe College of Information Science and Technology at Penn State University.\\n10', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 9}),\n",
       " Document(page_content='References\\nVaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Eval-\\nuating correctness and faithfulness of instruction-following models for question answering. arXiv\\npreprint arXiv:2307.16877 , 2023.\\nSaleema Amershi, Dan Weld, Mihaela V orvoreanu, Adam Fourney, Besmira Nushi, Penny Col-\\nlisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. Guidelines for human-ai\\ninteraction. In Proceedings of the 2019 chi conference on human factors in computing systems ,\\n2019.\\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man ¬¥e. Con-\\ncrete problems in ai safety, 2016.\\nAutoGPT. Documentation ‚Äî auto-gpt. https://docs.agpt.co/ , 2023.\\nBabyAGI. Github ‚Äî babyagi. https://github.com/yoheinakajima/babyagi , 2023.\\nCarrie J. Cai, Samantha Winter, David F. Steiner, Lauren Wilcox, and Michael Terry. ‚Äùhello ai‚Äù:\\nUncovering the onboarding needs of medical practitioners for human-ai collaborative decision-\\nmaking. Proceedings of the ACM on Human-Computer Interaction , 2019.\\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as\\ntool makers. arXiv preprint arXiv:2305.17126 , 2023.\\nChroma. Chromadb. https://github.com/chroma-core/chroma , 2023.\\nVictor Dibia. LIDA: A tool for automatic generation of grammar-agnostic visualizations and info-\\ngraphics using large language models. In Proceedings of the 61st Annual Meeting of the Associ-\\nation for Computational Linguistics (Volume 3: System Demonstrations) , Toronto, Canada, July\\n2023. Association for Computational Linguistics.\\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt. arXiv\\npreprint arXiv:2304.07590 , 2023.\\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improv-\\ning factuality and reasoning in language models through multiagent debate. arXiv preprint\\narXiv:2305.14325 , 2023.\\nAtty Eleti, Jeff Harris, and Logan Kilpatrick. Function calling and other api updates. https:\\n//openai.com/blog/function-calling-and-other-api-updates , 2023.\\nGuidance. Guidance. https://github.com/guidance-ai/guidance , 2023.\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv\\npreprint arXiv:2103.03874 , 2021.\\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven\\nKa Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for\\nmulti-agent collaborative framework. arXiv preprint arXiv:2308.00352 , 2023.\\nEric Horvitz. Principles of mixed-initiative user interfaces. In Proceedings of the SIGCHI conference\\non Human Factors in Computing Systems , 1999.\\nHuggingFace. Transformers agent. https://huggingface.co/docs/transformers/\\ntransformers_agents , 2023.\\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.\\narXiv preprint arXiv:2303.17491 , 2023.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\\nbenchmark for question answering research. Transactions of the Association for Computational\\nLinguistics , 2019.\\n11', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 10}),\n",
       " Document(page_content='LangChain. Introduction ‚Äî langchain. https://python.langchain.com/en/latest/index.\\nhtml , 2023.\\nMike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal? end-\\nto-end learning for negotiation dialogues. arXiv preprint arXiv:1706.05125 , 2017.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich K ¬®uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¬®aschel, et al. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems ,\\n2020.\\nBeibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, and Ishai Menache. Large language\\nmodels for supply chain optimization. arXiv preprint arXiv:2307.03875 , 2023a.\\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\\nCamel: Communicative agents for ‚Äùmind‚Äù exploration of large scale language model society,\\n2023b.\\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng\\nTu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-\\nagent debate, 2023.\\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement\\nlearning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802 ,\\n2018.\\nJerry Liu. LlamaIndex, November 2022. URL https://github.com/jerryjliu/llama_index .\\nV olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-\\nstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\\narXiv:1312.5602 , 2013.\\nRoberto Navigli, Simone Conia, and Bj ¬®orn Ross. Biases in large language models: Origins, inven-\\ntory and discussion. ACM Journal of Data and Information Quality , 2023.\\nOpenAI. ChatGPT plugins. https://openai.com/blog/chatgpt-plugins , 2023.\\nJoon Sung Park, Joseph C O‚ÄôBrien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and\\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint\\narXiv:2304.03442 , 2023.\\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.\\nRetrieval augmented code generation and summarization. arXiv preprint arXiv:2108.11601 ,\\n2021.\\nShishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model\\nconnected with massive apis. arXiv preprint arXiv:2305.15334 , 2023.\\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-\\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\\nProcessing . Association for Computational Linguistics, 11 2019. URL https://arxiv.org/\\nabs/1908.10084 .\\nSemantic-Kernel. Semantic kernel. https://github.com/microsoft/semantic-kernel ,\\n2023.\\nBokui Shen, Fei Xia, Chengshu Li, Roberto Mart ¬¥ƒ±n-Mart ¬¥ƒ±n, Linxi Fan, Guanzhi Wang, Claudia\\nP¬¥erez-D‚ÄôArpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, et al. igibson 1.0: A simu-\\nlation environment for interactive tasks in large realistic scenes. In 2021 IEEE/RSJ International\\nConference on Intelligent Robots and Systems (IROS) . IEEE, 2021.\\nTianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An\\nopen-domain platform for web-based agents. In International Conference on Machine Learning .\\nPMLR, 2017.\\n12', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 11}),\n",
       " Document(page_content='Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C ÀÜot¬¥e, Yonatan Bisk, Adam Trischler, and Matthew\\nHausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In\\nProceedings of the International Conference on Learning Representations (ICLR) , 2021. URL\\nhttps://arxiv.org/abs/2010.03768 .\\nOriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,\\nMichelle Yeo, Alireza Makhzani, Heinrich K ¬®uttler, John Agapiou, Julian Schrittwieser, et al.\\nStarcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782 , 2017.\\nChi Wang, Qingyun Wu, Markus Weimer, and Erkang Zhu. Flaml: A fast and lightweight automl\\nlibrary. Proceedings of Machine Learning and Systems , 2021.\\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\\nand Anima Anandkumar. V oyager: An open-ended embodied agent with large language models.\\narXiv preprint arXiv:2305.16291 , 2023a.\\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\\narXiv preprint arXiv:2308.11432 , 2023b.\\nDaniel S. Weld and Oren Etzioni. The first law of robotics (a call to arms). In AAAI Conference on\\nArtificial Intelligence , 1994.\\nMax Woolf. Langchain problem. https://minimaxir.com/2023/07/langchain-problem/ ,\\n2023.\\nYiran Wu, Feiran Jia, Shaokun Zhang, Qingyun Wu, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat\\nLee, Richard Peng, and Chi Wang. An empirical study on challenging math problem solving with\\ngpt-4. arXiv preprint arXiv:2306.01337 , 2023.\\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe\\nWang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents:\\nA survey. arXiv preprint arXiv:2309.07864 , 2023.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 ,\\n2022.\\n13', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 12}),\n",
       " Document(page_content='A Related Work\\nWe examine existing LLM-based agent systems or frameworks that can be used to build LLM appli-\\ncations. We categorize the related work into single-agent and multi-agent systems and specifically\\nprovide a summary of differentiators comparing AutoGen with existing multi-agent systems in Ta-\\nble 1. Note that many of these systems are evolving open-source projects, so the remarks and\\nstatements about them may only be accurate as of the time of writing. We refer interested readers to\\ndetailed LLM-based agent surveys (Xi et al., 2023; Wang et al., 2023b)\\nSingle-Agent Systems:\\n‚Ä¢AutoGPT : AutoGPT is an open-source implementation of an AI agent that attempts to au-\\ntonomously achieve a given goal (AutoGPT, 2023). It follows a single-agent paradigm in which\\nit augments the AI model with many useful tools, and does not support multi-agent collaboration.\\n‚Ä¢ChatGPT+ (with code interpreter or plugin) : ChatGPT, a conversational AI service or agent,\\ncan now be used alongside a code interpreter or plugin (currently available only under the pre-\\nmium subscription plan ChatGPT Plus) (OpenAI, 2023). The code interpreter enables ChatGPT\\nto execute code, while the plugin enhances ChatGPT with a wide range of curated tools.\\n‚Ä¢LangChain Agents : LangChain is a general framework for developing LLM-based applica-\\ntions (LangChain, 2023). LangChain Agents is a subpackage for using an LLM to choose a\\nsequence of actions. There are various types of agents in LangChain Agents, with the ReAct agent\\nbeing a notable example that combines reasoning and acting when using LLMs (mainly designed\\nfor LLMs prior to ChatGPT) (Yao et al., 2022). All agents provided in LangChain Agents fol-\\nlow a single-agent paradigm and are not inherently designed for communicative and collaborative\\nmodes. A significant summary of its limitations can be found in (Woolf, 2023). Due to these lim-\\nitations, even the multi-agent systems in LangChain (e.g., re-implementation of CAMEL) are not\\nbased on LangChain Agents but are implemented from scratch. Their connection to LangChain\\nlies in the use of basic orchestration modules provided by LangChain, such as AI models wrapped\\nby LangChain and the corresponding interface.\\n‚Ä¢Transformers Agent : Transformers Agent (HuggingFace, 2023) is an experimental natural-\\nlanguage API built on the transformers repository. It includes a set of curated tools and an agent\\nto interpret natural language and use these tools. Similar to AutoGPT, it follows a single-agent\\nparadigm and does not support agent collaboration.\\nAutoGen differs from the single-agent systems above by supporting multi-agent LLM applications.\\nMulti-Agent Systems:\\n‚Ä¢BabyAGI : BabyAGI (BabyAGI, 2023) is an example implementation of an AI-powered task man-\\nagement system in a Python script. In this implemented system, multiple LLM-based agents\\nare used. For example, there is an agent for creating new tasks based on the objective and the\\nresult of the previous task, an agent for prioritizing the task list, and an agent for completing\\ntasks/sub-tasks. As a multi-agent system, BabyAGI adopts a static agent conversation pattern,\\ni.e., a predefined order of agent communication, while AutoGen supports both static and dynamic\\nconversation patterns and additionally supports tool usage and human involvement.\\n‚Ä¢CAMEL : CAMEL (Li et al., 2023b) is a communicative agent framework. It demonstrates\\nhow role playing can be used to let chat agents communicate with each other for task comple-\\ntion. It also records agent conversations for behavior analysis and capability understanding. An\\nInception-prompting technique is used to achieve autonomous cooperation between agents. Un-\\nlikeAutoGen , CAMEL does not natively support tool usage, such as code execution. Although it\\nis proposed as an infrastructure for multi-agent conversation, it only supports static conversation\\npatterns, while AutoGen additionally supports dynamic conversation patterns.\\n‚Ä¢Multi-Agent Debate: Two recent works investigate and show that multi-agent debate is an effec-\\ntive way to encourage divergent thinking in LLMs (Liang et al., 2023) and to improve the factuality\\nand reasoning of LLMs (Du et al., 2023). In both works, multiple LLM inference instances are\\nconstructed as multiple agents to solve problems with agent debate. Each agent is simply an LLM\\ninference instance, while no tool or human is involved, and the inter-agent conversation needs\\nto follow a pre-defined order. These works attempt to build LLM applications with multi-agent\\nconversation, while AutoGen , designed as a generic infrastructure, can be used to facilitate this\\ndevelopment and enable more applications with dynamic conversation patterns.\\n14', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 13}),\n",
       " Document(page_content='‚Ä¢MetaGPT : MetaGPT (Hong et al., 2023) is a specialized LLM application based on a multi-agent\\nconversation framework for automatic software development. They assign different roles to GPTs\\nto collaboratively develop software. They differ from AutoGen by being specialized solutions to\\na certain scenario, while AutoGen is a generic infrastructure to facilitate building applications for\\nvarious scenarios.\\nThere are a few other specialized single-agent or multi-agent systems, such as V oyager (Wang et al.,\\n2023a) and Generative Agents (Park et al., 2023), which we skip due to lower relevance. In Table 1,\\nwe summarize differences between AutoGen and the most relevant multi-agent systems.\\nTable 1: Summary of differences between AutoGen and other related multi-agent systems. infras-\\ntructure : whether the system is designed as a generic infrastructure for building LLM applications.\\nconversation pattern : the types of patterns supported by the implemented systems. Under the\\n‚Äòstatic‚Äô pattern, agent topology remains unchanged regardless of different inputs. AutoGen allows\\nflexible conversation patterns, including both static and dynamic patterns that can be customized\\nbased on different application needs. execution-capable : whether the system can execute LLM-\\ngenerated code; human involvement : whether (and how) the system allows human participation\\nduring the execution process of the system. AutoGen allows flexible human involvement in multi-\\nagent conversation with the option for humans to skip providing inputs.\\nAspect AutoGen Multi-agent Debate CAMEL BabyAGI MetaGPT\\nInfrastructure ‚úì ‚úó ‚úì ‚úó ‚úó\\nConversation pattern flexible static static static static\\nExecution-capable ‚úì ‚úó ‚úó ‚úó ‚úì\\nHuman involvement chat/skip ‚úó ‚úó ‚úó ‚úó\\n15', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 14}),\n",
       " Document(page_content='B Expanded Discussion\\nThe applications in Section 3 show how AutoGen not only enables new applications but also helps\\nrenovate existing ones. For example, in A1 (scenario 3), A5, and A6, AutoGen enabled the cre-\\nation of multi-agent conversations that follow a dynamic pattern instead of a fixed back-and-forth.\\nAnd in both A5 and A6, humans can participate in the activities together with multiple other AI\\nagents in a conversational manner. Similarly, A1-A4 show how popular applications can be reno-\\nvated quickly with AutoGen . Despite the complexity of these applications (most of them involve\\nmore than two agents or dynamic multi-turn agent cooperation), our AutoGen -based implementa-\\ntion remains simple, demonstrating promising opportunities to build creative applications and a large\\nspace for innovation. In reflecting on why these benefits can be achieved in these applications with\\nAutoGen , we believe there are a few reasons:\\n‚Ä¢Ease of use : The built-in agents can be used out-of-the-box, delivering strong performance even\\nwithout any customization. (A1, A3)\\n‚Ä¢Modularity : The division of tasks into separate agents promotes modularity in the system. Each\\nagent can be developed, tested, and maintained independently, simplifying the overall develop-\\nment process and facilitating code management. (A3, A4, A5, and A6)\\n‚Ä¢Programmability: AutoGen allows users to extend/customize existing agents to develop systems\\nsatisfying their specific needs with ease. (A1-A6). For example, with AutoGen , the core workflow\\ncode in A4 is reduced from over 430 lines to 100 lines, for a 4x saving.\\n‚Ä¢Allowing human involvement :AutoGen provides a native mechanism to achieve human partici-\\npation and/or human oversight. With AutoGen , humans can seamlessly and optionally cooperate\\nwith AIs to solve problems or generally participate in the activity. AutoGen also facilitates inter-\\nactive user instructions to ensure the process stays on the desired path. (A1, A2, A5, and A6)\\n‚Ä¢Collaborative/adversarial agent interactions : Like many collaborative agent systems (Dong\\net al., 2023), agents in AutoGen can share information and knowledge, to complement each other‚Äôs\\nabilities and collectively arrive at better solutions. (A1, A2, A3, and A4). Analogously, in certain\\nscenarios, some agents are required to work in an adversarial way. Relevant information is shared\\namong different conversations in a controlled manner, preventing distraction or hallucination. (A4,\\nA6). AutoGen supports both patterns, enabling effective utilization and augmentation of LLMs.\\nB.1 General Guidelines for Using AutoGen\\nBelow we give some recommendations for using agents in AutoGen to accomplish a task.\\n1.Consider using built-in agents first. For example, AssistantAgent is pre-configured to be\\nbacked by GPT-4, with a carefully designed system message for generic problem-solving via\\ncode. The UserProxyAgent is configured to solicit human inputs and perform tool execution.\\nMany problems can be solved by simply combining these two agents. When customizing agents\\nfor an application, consider the following options: (1) human input mode, termination condition,\\ncode execution configuration, and LLM configuration can be specified when constructing an\\nagent; (2) AutoGen supports adding instructions in an initial user message, which is an effective\\nway to boost performance without needing to modify the system message; (3) UserProxyAgent\\ncan be extended to handle different execution environments and exceptions, etc.; (4) when sys-\\ntem message modification is needed, consider leveraging the LLM‚Äôs capability to program its\\nconversation flow with natural language.\\n2.Start with a simple conversation topology . Consider using the two-agent chat or the group chat\\nsetup first, as they can often be extended with the least code. Note that the two-agent chat can\\nbe easily extended to involve more than two agents by using LLM-consumable functions in a\\ndynamic way.\\n3. Try to reuse built-in reply methods based on LLM, tool, or human before implementing a\\ncustom reply method because they can often be reused to achieve the goal in a simple way\\n(e.g., the built-in agent GroupChatManager ‚Äôs reply method reuses the built-in LLM-based reply\\nfunction when selecting the next speaker, ref. A5 in Section 3).\\n4. When developing a new application with UserProxyAgent ,start with humans always in\\nthe loop , i.e., human input mode=‚ÄòALWAYS‚Äô, even if the target operation mode is more au-\\ntonomous. This helps evaluate the effectiveness of AssistantAgent , tuning the prompt, dis-\\ncovering corner cases, and debugging. Once confident with small-scale success, consider setting\\n16', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 15}),\n",
       " Document(page_content='human input mode = ‚ÄòNEVER‚Äô. This enables LLM as a backend, and one can either use the\\nLLM or manually generate diverse system messages to simulate different use cases.\\n5. Despite the numerous advantages of AutoGen agents, there could be cases/scenarios where other\\nlibraries/packages could help . For example: (1) For (sub)tasks that do not have requirements\\nfor back-and-forth trouble-shooting, multi-agent interaction, etc., a unidirectional (no back-and-\\nforth message exchange) pipeline can also be orchestrated with LangChain (LangChain, 2023),\\nLlamaIndex (Liu, 2022), Guidance (Guidance, 2023), Semantic Kernel (Semantic-Kernel, 2023),\\nGorilla (Patil et al., 2023) or low-level inference API (‚Äòautogen.oai‚Äô provides an enhanced LLM\\ninference layer at this level) (Dibia, 2023). (2) When existing tools from LangChain etc. are\\nhelpful, one can use them as tool backends for AutoGen agents. For example, one can readily use\\ntools, e.g., Wolfram Alpha, from LangChain in AutoGen agent. (3) For specific applications, one\\nmay want to leverage agents implemented in other libraries/packages. To achieve this, one could\\nwrap those agents as conversable agents in AutoGen and then use them to build LLM applications\\nthrough multi-agent conversation. (4) It can be hard to find an optimal operating point among\\nmany tunable choices, such as the LLM inference configuration. Blackbox optimization packages\\nlike ‚Äòflaml.tune‚Äô (Wang et al., 2021) can be used together with AutoGen to automate such tuning.\\nB.2 Future Work\\nThis work raises many research questions and future directions and .\\nDesigning optimal multi-agent workflows: Creating a multi-agent workflow for a given task can\\ninvolve many decisions, e.g., how many agents to include, how to assign agent roles and agent\\ncapabilities, how the agents should interact with each other, and whether to automate a particular\\npart of the workflow. There may not exist a one-fits-all answer, and the best solution might depend\\non the specific application. This raises important questions: For what types of tasks and applications\\nare multi-agent workflows most useful? How do multiple agents help in different applications? For\\na given task, what is the optimal (e.g., cost-effective) multi-agent workflow?\\nCreating highly capable agents: AutoGen can enable the development of highly capable agents\\nthat leverage the strengths of LLMs, tools, and humans. Creating such agents is crucial to ensuring\\nthat a multi-agent workflow can effectively troubleshoot and make progress on a task. For example,\\nwe observed that CAMEL, another multi-agent LLM system, cannot effectively solve problems in\\nmost cases primarily because it lacks the capability to execute tools or code. This failure shows that\\nLLMs and multi-agent conversations with simple role playing are insufficient, and highly capable\\nagents with diverse skill sets are essential. We believe that more systematic work will be required to\\ndevelop guidelines for application-specific agents, to create a large OSS knowledge base of agents,\\nand to create agents that can discover and upgrade their skills (Cai et al., 2023).\\nEnabling scale, safety, and human agency: Section 3 shows how complex multi-agent workflows\\ncan enable new applications, and future work will be needed to assess whether scaling further can\\nhelp solve extremely complex tasks. However, as these workflows scale and grow more complex,\\nit may become difficult to log and adjust them. Thus, it will become essential to develop clear\\nmechanisms and tools to track and debug their behavior. Otherwise, these techniques risk resulting\\nin incomprehensible, unintelligible chatter among agents (Lewis et al., 2017).\\nOur work also shows how complex, fully autonomous workflows with AutoGen can be useful, but\\nfully autonomous agent conversations will need to be used with care. While the autonomous mode\\nAutoGen supports could be desirable in many scenarios, a high level of autonomy can also pose\\npotential risks, especially in high-risk applications (Amodei et al., 2016; Weld & Etzioni, 1994). As\\na result, building fail-safes against cascading failures and exploitation, mitigating reward hacking,\\nout of control and undesired behaviors, maintaining effective human oversight of applications built\\nwith AutoGen agents will become important. While AutoGen provides convenient and seamless\\ninvolvement of humans through a user proxy agent, developers and stakeholders still need to under-\\nstand and determine the appropriate level and pattern of human involvement to ensure the safe and\\nethical use of the technology (Horvitz, 1999; Amershi et al., 2019).\\n17', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 16}),\n",
       " Document(page_content='C Default System Message for Assistant Agent\\nSystemMessageYou are a helpful AI assistant. Solve tasks using your coding and language skills.In the following cases, suggest python code (in a python coding block) or shell script (in a shcoding block) for the user to execute.1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.2. When you need to perform some task with code, use the code to perform the task and output theresult. Finish the task smartly.Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can‚Äôt modify your code. So do not suggest incomplete code which requires users to modify. Don‚Äôt use a code block if it‚Äôs not intended to be executed by the user.If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don‚Äôt include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use ‚Äôprint‚Äô function for the output when relevant. Check the execution result returned by the user.If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can‚Äôt be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.Reply ‚ÄúTERMINATE‚Äù in the end when everything is done.Prompting techniques color code: Role Play; Control Flow; Output Confine; Facilitate Automation; Grounding\\nFigure 5: Default system message for the built-in assistant agent in AutoGen (v0.1.1). This is an\\nexample of conversation programming via natural language. It contains instructions of different\\ntypes, including role play, control flow, output confine, facilitate automation, and grounding.\\nFigure 5 shows the default system message for the built-in assistant agent in AutoGen (v0.1.1),\\nwhere we introduce several new prompting techniques and highlight them accordingly. When com-\\nbining these new prompting techniques together, we can program a fairly complex conversation even\\nwith the simplest two-agent conversation topology. This approach tries to exploit the capability of\\nLLMs in implicit state inference to a large degree. LLMs do not follow all the instructions perfectly,\\nso the design of the system needs to have other mechanisms to handle the exceptions and faults.\\nSome instructions can have ambiguities, and the designer should either reduce them for preciseness\\nor intentionally keep them for flexibility and address the different situations in other agents. In\\ngeneral, we observe that GPT-4 follows the instructions better than GPT-3.5-turbo.\\n18', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 17}),\n",
       " Document(page_content='D Application Details\\nA1: Math Problem Solving\\nScenario 1: Autonomous Problem Solving. We perform both qualitative and quantitative eval-\\nuations in this scenario. For all evaluations, we use GPT-4 as the base model, and pre-install the\\n‚Äúsympy‚Äù package in the execution environment. We compare AutoGen with the following LLM-\\nbased agent systems:\\n‚Ä¢ AutoGPT: The out-of-box AutoGPT is used. We initialize AutoGPT by setting the purpose to\\n‚Äúsolve math problems‚Äù, resulting in a ‚ÄúMathSolverGPT‚Äù with auto-generated goals.\\n‚Ä¢ ChatGPT+Plugin: We enable the Wolfram Alpha plugin (a math computation engine) in the Ope-\\nnAI web client.\\n‚Ä¢ ChatGPT+Code Interpreter: This is a recent feature in OpenAI web client. Note that the above\\ntwo premium features from ChatGPT require a paid subscription to be accessed and are the most\\ncompetitive commercial systems.\\n‚Ä¢ LangChain ReAct+Python: We use Python agent from LangChain. To handle parsing errors, we\\nset ‚Äúhandle parsing errors=True‚Äù, and use the default zero-shot ReAct prompt.\\n‚Ä¢ Multi-Agent Debate (Liang et al., 2023): We modified the code of the multi-agent debate to per-\\nform evaluation. By default, there are three agents: an affirmative agent, a negative agent, and a\\nmoderator.\\nWe also conducted preliminary evaluations on several other multi-agent systems, including\\nBabyAGI, CAMEL, and MetaGPT. The results indicate that they are not suitable choices for solving\\nmath problems out of the box. For instance, when MetaGPT is tasked with solving a math problem,\\nit begins developing software to address the problem, but most of the time, it does not actually solve\\nthe problem. We have included the test examples in Appendix E.\\nTable 2: Qualitative evaluation of two math problems from the MATH dataset within the autonomous\\nproblem-solving scenario. Each LLM-based system is tested three times on each of the problems.\\nThis table reports the problem-solving correctness and summarizes the reasons for failure.\\nCorrectness Failure Reason\\nAutoGen 3/3 N/A.\\nAutoGPT 0/3 The LLM gives code without the print function so the\\nresult is not printed.\\nChatGPT+Plugin 1/3 The return from Wolfram Alpha contains 2 simplified\\nresults, including the correct answer, but GPT-4 always\\nchooses the wrong answer.\\nChatGPT+Code Interpreter 2/3 Returns a wrong decimal result.\\nLangChain ReAct 0/3 LangChain gives 3 different wrong answers.\\nMulti-Agent Debate 0/3 It gives 3 different wrong answers due to calculation errors.\\n(a) Evaluation on the first problem that asks to simplify a square root fraction.\\nCorrectness Failure Reason\\nAutoGen 2/3 The final answer from code execution is wrong.\\nAutoGPT 0/3 The LLM gives code without the print function so the\\nresult is not printed.\\nChatGPT+Plugin 1/3 For one trial, GPT-4 got stuck because it keeps giving\\nwrong queries and has to be stopped. Another trial simply\\ngives a wrong answer.\\nChatGPT+Code Interpreter 0/3 It gives 3 different wrong answers.\\nLangChain ReAct 0/3 LangChain gives 3 different wrong answers.\\nMulti-Agent Debate 0/3 It gives 3 different wrong answers.\\n(b) Evaluation on the second number theory problem.\\nFor the qualitative evaluation, we utilize two level-5 problems from the MATH dataset, testing each\\nproblem three times. The first problem involves simplifying a square root fraction, and the second\\n19', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 18}),\n",
       " Document(page_content='problem involves solving a number theory issue. The correctness counts and reasons for failure\\nare detailed in Table 2. For the quantitative evaluation, we conduct two sets of experiments on\\nthe MATH dataset to assess the correctness of these systems: (1) an experiment involving 120\\nlevel-5 (the most challenging level) problems, including 20 problems from six categories, excluding\\ngeometry, and (2) an experiment on the entire test set, which includes 5000 problems. We exclude\\nAutoGPT from this evaluation as it cannot access results from code executions and does not solve\\nany problems in the qualitative evaluation. Our analysis of the entire dataset reveals that AutoGen\\nachieves an overall accuracy of 69.48%, while GPT-4‚Äôs accuracy stands at 55.18%. From these\\nevaluations, we have the following observations regarding the problem-solving success rate and\\nuser experience of these systems:\\n‚Ä¢ Problem-solving success rate: Results from the quantitative evaluations show that AutoGen can\\nhelp achieve the highest problem-solving success rate among all the compared methods. The qual-\\nitative evaluations elucidate common failure reasons across several alternative approaches. Chat-\\nGPT+Code Interpreter fails to solve the second problem, and ChatGPT+Plugin struggles to solve\\nboth problems. AutoGPT fails on both problems due to code execution issues. The LangChain\\nagent also fails on both problems, producing code that results in incorrect answers in all trials.\\n‚Ä¢ Based on the qualitative evaluation, we analyze the user experience concerning the verbosity of\\nthe response and the ability of the LLM-based system to run without unexpected behaviors. Chat-\\nGPT+Plugin is the least verbose, mainly because Wolfram queries are much shorter than Python\\ncode. AutoGen , ChatGPT+Code Interpreter, and LangChain exhibit similar verbosity, although\\nLangChain is slightly more verbose due to more code execution errors. AutoGPT is the most\\nverbose system owing to predefined steps like THOUGHTS, REASONING, and PLAN, which it\\nincludes in replies every time. Overall, AutoGen and ChatGPT+Code Interpreter operate smoothly\\nwithout exceptions. We note the occurrences of undesired behaviors from other LLM-based sys-\\ntems that could affect user experience: AutoGPT consistently outputs code without the print‚Äô\\nstatement and cannot correct this, requiring the user to run them manually; ChatGPT with Wol-\\nfram Alpha plugin has the potential to become stuck in a loop that must be manually stopped; and\\nLangchain ReAct could exit with a parse error, necessitating the passing of a ‚Äòhandle parse error‚Äô\\nparameter.\\nEnable Multi-User Problem Solving ViaStudent        and Expert Student Proxy\\nStudentAssistant\\nExpertAssistant\\nExpert Proxy\\nAsk for expert\\nEnable Autonomous and Human-in-the-loop Problem Solving\\nFigure 6: Examples of three settings utilized to solve math problems using AutoGen : (Gray) En-\\nables a workflow where a student collaborates with an assistant agent to solve problems, either\\nautonomously or in a human-in-the-loop mode. (Gray + Orange) Facilitates a more sophisticated\\nworkflow wherein the assistant, on the fly, can engage another user termed ‚Äúexpert‚Äù, who is in the\\nloop with their own assistant agent, to aid in problem-solving if its own solutions are not satisfactory.\\nScenario 2: Human-in-the-loop Problem Solving. For challenging problems that these LLM\\nsystems cannot solve autonomously, human feedback during the problem-solving process can be\\n20', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 19}),\n",
       " Document(page_content='helpful. To incorporate human feedback with AutoGen , one can set human input mode=‚ÄòALWAYS‚Äô\\nin the user proxy agent. We select one challenging problem that none of these systems can solve\\nautonomously across three trials. We adhere to the process outlined below to provide human inputs\\nfor all the compared methods:\\n1. Input the problem: Find the equation of the plane which bisects the angle\\nbetween the planes 3x‚àí6y+ 2z+ 5 = 0 and4x‚àí12y+ 3z‚àí3 = 0 ,and which\\ncontains the point (‚àí5,‚àí1,‚àí5).Enter your answer in the form\\nAx+By+Cz+D= 0,\\nwhere A, B, C, D are integers such that A > 0and\\ngcd(|A|,|B|,|C|,|D|) = 1.\\n2. The response from the system does not solve the problem correctly. We then give a\\nhint to the model: Your idea is not correct. Let‚Äôs solve this together.\\nSuppose P= (x, y, z )is a point that lies on a plane that bisects the\\nangle, the distance from P to the two planes is the same. Please\\nset up this equation first.\\n3. We expect the system to give the correct distance equation. Since the equation involves\\nan absolute sign that is hard to solve, we would give the next hint: Consider the two\\ncases to remove the abs sign and get two possible solutions.\\n4. If the system returns the two possible solutions and doesn‚Äôt continue to the next step, we\\ngive the last hint: Use point (-5,-1,-5) to determine which is correct and\\ngive the final answer.\\n5. Final answer is 11x+6y+5z+86=0 .\\nWe observed that AutoGen consistently solved the problem across all three trials. ChatGPT+Code\\nInterpreter and ChatGPT+Plugin managed to solve the problem in two out of three trials, while Au-\\ntoGPT failed to solve it in all three attempts. In its unsuccessful attempt, ChatGPT+Code Interpreter\\nfailed to adhere to human hints. In its failed trial, ChatGPT+Plugin produced an almost correct solu-\\ntion but had a sign discrepancy in the final answer. AutoGPT was unable to yield a correct solution\\nin any of the trials. In one trial, it derived an incorrect distance equation. In the other two trials, the\\nfinal answer was incorrect due to code execution errors.\\nScenario 3: Multi-User Problem Solving. Next-generation LLM applications may necessitate\\nthe involvement of multiple real users for collectively solving a problem with the assistance of\\nLLMs. We showcase how AutoGen can be leveraged to effortlessly construct such a system. Specif-\\nically, building upon scenario 2 mentioned above, we aim to devise a simple system involving two\\nhuman users: a student and an expert. In this setup, the student interacts with an LLM assistant to\\naddress some problems, and the LLM automatically resorts to the expert when necessary.\\nThe overall workflow is as follows: The student chats with the LLM-based assistant agent through\\na student proxy agent to solve problems. When the assistant cannot solve the problem satisfactorily,\\nor the solution does not match the expectation of the student, it would automatically hold the con-\\nversation and call the pre-defined askforexpert function via the function callfeature of GPT\\nin order to resort to the expert. Specifically, it would automatically produce the initial message for\\ntheaskforexpert function, which could be the statement of the problem or the request to verify\\nthe solution to a problem, and the expert is supposed to respond to this message with the help of\\nthe expert assistant. After the conversation between the expert and the expert‚Äôs assistant, the final\\nmessage would be sent back to the student assistant as the response to the initial message. Then, the\\nstudent assistant would resume the conversation with the student using the response from the expert\\nfor a better solution. A detailed visualization is shown in Figure 6.\\nWith AutoGen , constructing the student/expert proxy agent and the assistant agents is straight-\\nforward by reusing the built-in UserProxyAgent andAssistantAgent through appropriate\\nconfigurations. The only development required involves writing several lines of code for the\\naskforexpert function, which then becomes part of the configuration for the assistant. Ad-\\nditionally, it‚Äôs easy to extend such a system to include more than one expert, with a specific\\naskforexpert function for each, or to include multiple student users with a shared expert for\\nconsultation.\\n21', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 20}),\n",
       " Document(page_content='A2: Retrieval-Augmented Code Generation and Question Answering\\nRetrieval-augmentedAssistantRetrieval-augmented  User Proxy\\n1. Question and Contexts3. Terminate,feedbacks or `Update Context`4. Satisfied Answers or Terminate\\n2. Satisfied Answers or `Update Context`\\nFigure 7: Overview of Retrieval-augmented Chat which involves two agents, including a Retrieval-\\naugmented User Proxy and a Retrieval-augmented Assistant. Given a set of documents, the\\nRetrieval-augmented User Proxy first automatically processes documents‚Äîsplits, chunks, and stores\\nthem in a vector database. Then for a given user input, it retrieves relevant chunks as context and\\nsends it to the Retrieval-augmented Assistant, which uses LLM to generate code or text to answer\\nquestions. Agents converse until they find a satisfactory answer.\\nDetailed Workflow. The workflow of Retrieval-Augmented Chat is illustrated in Figure 7. To\\nuse Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented\\nUser Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy\\nnecessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented\\nUser Proxy can download the documents, segment them into chunks of a specific size, compute\\nembeddings, and store them in a vector database. Once a chat is initiated, the agents collaboratively\\nengage in code generation or question-answering adhering to the procedures outlined below:\\n1. The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding simi-\\nlarity, and sends them along with the question to the Retrieval-Augmented Assistant.\\n2. The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based\\non the question and context provided. If the LLM is unable to produce a satisfactory response, it\\nis instructed to reply with ‚ÄúUpdate Context‚Äù to the Retrieval-Augmented User Proxy.\\n3. If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and\\nsends the output as feedback. If there are no code blocks or instructions to update the context, it\\nterminates the conversation. Otherwise, it updates the context and forwards the question along\\nwith the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation\\nis enabled, individuals can proactively send any feedback, including Update Context‚Äù, to the\\nRetrieval-Augmented Assistant.\\n4. If the Retrieval-Augmented Assistant receives ‚ÄúUpdate Context‚Äù, it requests the next most similar\\nchunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it\\ngenerates new code or text based on the feedback and chat history. If the LLM fails to generate\\nan answer, it replies with ‚ÄúUpdate Context‚Äù again. This process can be repeated several times.\\nThe conversation terminates if no more documents are available for the context.\\nWe utilize Retrieval-Augmented Chat in two scenarios. The first scenario aids in generating code\\nbased on a given codebase. While LLMs possess strong coding abilities, they are unable to utilize\\npackages or APIs that are not included in their training data, e.g., private codebases, or have trouble\\nusing trained ones that are frequently updated post-training. Hence, Retrieval-Augmented Code\\nGeneration is considered to be highly valuable. The second scenario involves question-answering\\non the Natural Questions dataset (Kwiatkowski et al., 2019), enabling us to obtain comparative\\nevaluation metrics for the performance of our system.\\nScenario 1: Evaluation on Natural Questions QA dataset. In this case, we evaluate the\\nRetrieval-Augmented Chat‚Äôs end-to-end question-answering performance using the Natural Ques-\\ntions dataset (Kwiatkowski et al., 2019). We collected 5,332 non-redundant context documents and\\n6,775 queries from HuggingFace. First, we create a document collection based on the entire context\\ncorpus and store it in the vector database. Then, we utilize Retrieval-Augmented Chat to answer the\\nquestions. An example (Figure 8) from the NQ dataset showcases the advantages of the interactive\\nretrieval feature: ‚Äúwho carried the usa flag in opening ceremony‚Äù . When attempting to answer\\nthis question, the context with the highest similarity to the question embedding does not contain the\\n22', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 21}),\n",
       " Document(page_content='Task description who carried the usaflag in opening ceremony?Assistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply exactly ‚ÄòUPDATE CONTEXT‚Äô. You must give as short an answer as possible.User‚Äôs question is: who carried the usaflag in opening ceremonyContext is: < P > On January 17 , 1899 , under ... The Astros beat the Yankees that night , 2 -1 . < /P >UserProxy: Sorry, I cannot find any information about who carried the USA flag in the opening ceremony. UPDATE CONTEXT.Assistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply exactly ‚ÄòUPDATE CONTEXT‚Äô. You must give as short an answer as possible.User‚Äôs question is: who carried the usaflag in opening ceremonyContext is: < T able > ... Erin Hamlin ... < /T d >UserProxy: Erin Hamlin carried the USA flag in the opening ceremony. \\n(a) W/O Interactive Retrieval\\n(b) W/ Interactive Retrieval\\nAssistant: You‚Äôre a retrieve augmented chatbot. You answer user‚Äôs questions based on your own knowledge and the context provided by the user. If you can‚Äôt answer the question with or without the current context, you should reply ‚Äòsorry, I don‚Äôt know‚Äô. You must give as short an answer as possible.User‚Äôs question is:who carried the usaflag in opening ceremonyContext is: < P > On January 17 , 1899 , under ... The Astros beat the Yankees that night , 2 -1 . < /P >UserProxy: Sorry, I cannot find any information about who carried the USA flag in the opening ceremony. Sorry, I don‚Äôt know.Figure 8: Retrieval-augmented Chat without (W/O) and with (W/) interactive retrieval .\\nrequired information for a response. As a result, the LLM assistant (GPT-3.5-turbo) replies ‚ÄúSorry,\\nI cannot find any information about who carried the USA flag in the opening ceremony. UPDATE\\nCONTEXT. ‚Äù With the unique and innovative ability to update context in Retrieval-Augmented Chat,\\nthe user proxy agent automatically updates the context and forwards it to the assistant agent again.\\nFollowing this process, the agent is able to generate the correct answer to the question.\\nIn addition, we conduct an experiment using the same prompt as illustrated in (Adlakha et al., 2023)\\nto investigate the advantages of AutoGen W/O interactive retrieval . The F1 score and Recall for the\\nfirst 500 questions are 23.40% and 62.60%, respectively, aligning closely with the results reported\\nin Figure 4b. Consequently, we assert that AutoGen W/O interactive retrieval outperforms DPR due\\nto differences in the retrievers employed. Specifically, we utilize a straightforward vector search\\nretriever with the all-MiniLM-L6-v2 model for embeddings.\\nFurthermore, we analyze the number of LLM calls in experiments involving both AutoGen and\\nAutoGen W/O interactive retrieval , revealing that approximately 19.4% of questions in the Natural\\nQuestions dataset trigger an ‚ÄúUpdate Context‚Äù operation, resulting in additional LLM calls.\\nScenario 2: Code Generation Leveraging Latest APIs from the Codebase. In this case, the ques-\\ntion is ‚ÄúHow can I use FLAML to perform a classification task and use Spark for parallel training?\\nTrain for 30 seconds and force cancel jobs if the time limit is reached. ‚Äù . FLAML (v1) (Wang et al.,\\n2021) is an open-source Python library designed for efficient AutoML and tuning. It was open-\\nsourced in December 2020, and is included in the training data of GPT-4. However, the question\\nnecessitates the use of Spark-related APIs, which were added in December 2022 and are not encom-\\npassed in the GPT-4 training data. Consequently, the original GPT-4 model is unable to generate the\\ncorrect code, due to its lack of knowledge regarding Spark-related APIs. Instead, it erroneously cre-\\nates a non-existent parameter, spark , and sets it to True‚Äô. Nevertheless, with Retrieval-Augmented\\nChat, we provide the latest reference documents as context. Then, GPT-4 generates the correct code\\nblocks by setting usespark andforce cancel to True‚Äô.\\n23', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 22}),\n",
       " Document(page_content='A3: Decision Making in Text World Environments\\nALFWorld\\nExecutor\\nReward & StateAction Decision\\nAssistant\\nObservation: On the desk 2, you see an alarmclock 3, \\na bowl 3, a creditcard 2, a mug 1, and a pencil 2.Action decision: Pick up pencil 2 from desk 2\\nGroundingAgent\\nALFChat (two agents) ALFChat (three agents)\\nALFWorld Executor\\nAssistant\\nFigure 9: We use AutoGen to solve tasks in the ALFWorld benchmark, which contains household\\ntasks described in natural language. We propose two designs: a two-agent design where the assistant\\nagent suggests the next step, and the Executor executes actions and provides feedback. The three-\\nagent design adds a grounding agent that supplies commonsense facts to the executor when needed.\\nALFWorld (Shridhar et al., 2021) is a synthetic language-based interactive decision-making task.\\nIt comprises textual environments that aim to simulate real-world household scenes. Given a high-\\nlevel goal (e.g., putting a hot apple in the fridge) and the description of the household environment,\\nthe agent needs to explore and interact with the simulated household environment through a textual\\ninterface. A typical task environment contains various types of locations and could require more\\nthan 40 steps to finish, which highlights the need for agents to decompose the goal into subtasks and\\ntackle them one by one, while effectively exploring the environments.\\nDetailed Workflow. We first propose a straightforward two-agent system with AutoGen , illustrated\\non the left-hand side of Figure 9, to tackle tasks from this benchmark. The system consists of\\nan assistant agent and an executor agent. The assistant agent generates plans and makes action\\ndecisions to solve the tasks. The executor agent is tailored specifically for ALFWorld. It performs\\nactions proposed by the assistant and reports action execution results in the household environment\\nas feedback to the assistant. Due to the strict format requirements for the output format, we use the\\nBLEU metric to evaluate the similarity of the output to all valid action options. The option with the\\nhighest similarity will be chosen as the action for this round.\\nOne major challenge encompassed in ALFWorld is commonsense reasoning. The agent needs to\\nextract patterns from the few-shot examples provided and combine them with the agent‚Äôs general\\nknowledge of household environments to fully understand task rules. More often than not, the as-\\nsistant tends to neglect some basic knowledge of the household environment. Thanks to the easy-to-\\nimplement multi-agent conversational feature of AutoGen , enhancing the assistant agent‚Äôs reason-\\ning ability by adding a new grounding agent to provide commonsense facts for the decision-making\\nagent‚Äôs reference becomes straightforward. By scrutinizing the failed attempts and summarizing the\\nreasons for failure, we obtained a holistic understanding of the commonsense knowledge that the\\nassistant agent lacks. Then, we set a grounding agent to provide this general knowledge when the\\ntask begins and whenever the assistant outputs the same action three times in a row. This ensures the\\nassistant takes this commonsense knowledge into consideration and prevents it from getting stuck in\\noutputting the same content or constantly apologizing.\\nWe compare our system‚Äôs performance with ReAct, which treats ALFWorld as a text-completion\\ntask. ReAct (Yao et al., 2022) is a few-shot prompting technique that interleaves reasoning and\\nacting, allowing for greater synergy between the two and significantly improving performance on\\nboth language and decision-making tasks. We integrate ReAct into AutoGen by modifying the\\nprompts in a conversational manner. Following ReAct, we employ a two-shot setting. The few-\\nshot prompts are obtained from the corresponding repository. As shown in Table 3, the two-agent\\n24', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 23}),\n",
       " Document(page_content=\"Task description\\nYou are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a safe 1, a drawer 2, a dra wer1, a \\nshelf 3, a shelf 2, and a shelf 1.\\nYour task is to: look at bowl under the desklamp . \\nAssistant (to Executor) : THOUGHT [First, I need to find a bowl. A \\nbowl is more likely to appear in desk (1 -2), drawer (1 -2), shelf \\n(1-3), bed (1). Then I need to find and use a desklamp .] ACTION \\n[go to desk 1]\\n(Assistant finds the desklamp at desk 1, then goes to desk 2.)\\nExecutor (to Assistant) : On the desk 2, you see a bowl 1, and a \\ncd 3... \\nAssistant (to Executor) : THOUGHT [Now I find a bowl (1). I need \\nto use the desklamp to look at the bowl. I'll go to the desklamp\\nnow.] ACTION [go to desk 1]\\nExecutor (to Assistant): On the desk 1, you see a desklamp 1, a \\npencil 2...\\nAssistant: ACTION [use desklamp 1]\\nExecutor (to Assistant) : You turn on the desklamp 1.\\nAssistant (to Executor): THOUGHT [Now that the desklamp is turned \\non, I can use it to look at the bowl (1) that I found earlier.] \\nACTION [look at bowl 1 under the desklamp 1]\\nExecutor (to Assistant) : You turn on the desklamp 1.\\n(Assistant falls into an infinite loop turning on desklamp .)\\nExecutor (to Assistant) : Task failed. Reply TERMINATE.\\nAssistant (to Executor) : TERMINATEAssistant (to Executor) : THOUGHT [First, I need to find a bowl. A \\nbowl is more likely to appear in desk (1 -2), drawer (1 -2), shelf \\n(1-3), bed (1). Then I need to find and use a desklamp .] ACTION \\n[go to desk 1]\\n(Assistant finds the desklamp at desk 1, then goes to desk 2.)\\n(Assistant falls into the same loop as (a))\\nGroundingAgent (to Executor) : You must find and take the object \\nbefore you can examine it. You must go to where the target \\nobject is before you can use it.\\nExecutor(to Assistant): You turn on the desklamp 1. Hint: You must \\nfind and take the object before you can examine it. \\nAssistant (to Executor): THOUGHT [I need to find and take the \\nbowl (1) before I can examine it. Let me find the bowl first.] \\nACTION [go to desk 2]\\nExecutor (to Assistant): On the desk 2, you see a bowl 1, a cd \\n3...\\nAssistant (to Executor) : ACTION [take bowl 1 from desk 2]\\nExecutor (to Assistant): You pick up bowl 1 from desk 2.\\nAssistant (to Executor): ACTION [go to desk 1]\\nAssistant (to Executor): ACTION[use desklamp 1]\\nExecutor (to Assistant) : Task success. Reply TERMINATE.\\nAssistant (to Executor) : TERMINATE\\n(a) Two agents\\n (b) Three agentsFigure 10: Comparison of results from two designs: (a) Two-agent design which consists of an\\nassistant and an executor, (b) Three-agent design which adds a grounding agent that serves as a\\nknowledge source. For simplicity, we omit the in-context examples and part of the exploration\\ntrajectory, and only show parts contributing to the failure/success of the attempt.\\nMethod Pick Clean Heat Cool Look Pick 2 All\\nReAct (avg) 63 52 48 71 61 24 54\\nALFChat (2 agents)(avg) 61 58 57 67 50 19 54\\nALFChat (3 agents)(avg) 79 64 70 76 78 41 69\\nReAct (best of 3) 75 62 61 81 78 35 66\\nALFChat (2 agents)(best of 3) 71 61 65 76 67 35 63\\nAFLChat (3 agents)(best of 3) 92 74 78 86 83 41 77\\nTable 3: Comparisons between ReAct and the two variants of ALFChat on the ALFWorld bench-\\nmark. For each task, we report the success rate out of 3 attempts. Success rate denotes the number\\nof tasks successfully completed by the agent divided by the total number of tasks. The results show\\nthat adding a grounding agent significantly improves the task success rate in ALFChat.\\ndesign matches the performance of ReAct, while the three-agent design significantly outperforms\\nReAct. We surmise that the performance discrepancy is caused by the inherent difference between\\ndialogue-completion and text-completion tasks. On the other hand, introducing a grounding agent\\nas a knowledge source remarkably advances performance on all types of tasks.\\nCase study . Figure 10 exemplifies how a three-agent design eliminates one root cause for failure\\ncases. Most of the tasks involve taking an object and then performing a specific action with it (e.g.,\\nfinding a vase and placing it on a cupboard). Without a grounding agent, the assistant frequently\\nconflates finding an object with taking it, as illustrated in Figure 10a). This leads to most of the\\nfailure cases in ‚Äôpick‚Äô and ‚Äôlook‚Äô type tasks. With the introduction of a grounding agent, the assistant\\ncan break out of this loop and successfully complete the task\\nTakeaways. We introduced a grounding agent to serve as an external commonsense knowledge\\nsource, which significantly enhanced the assistant‚Äôs ability to make informed decisions. This proves\\nthat providing necessary commonsense facts to the decision-making agent can assist it in making\\nmore informed decisions, thus effectively boosting the task success rate. AutoGen brings both\\nsimplicity and modularity when adding the grounding agent.\\n25\", metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 24}),\n",
       " Document(page_content='A4: Multi-Agent Coding\\nCommander\\nSafeguard\\nWriter\\n‚ë°Question, ‚ùªLog‚ù∏Code, ‚ë¶Ans‚ùπCode‚ù∫Clearance\\nUser‚ë†User Question‚ëßFinal AnswerRepeat until answering the user‚Äôs question or timeout\\nFigure 11: Our re-implementation of OptiGuide with AutoGen streamlining agents‚Äô interactions.\\nThe Commander receives user questions (e.g., What if we prohibit shipping from supplier 1 to\\nroastery 2?) and coordinates with the Writer and Safeguard. The Writer crafts the code and inter-\\npretation, the Safeguard ensures safety (e.g., not leaking information, no malicious code), and the\\nCommander executes the code. If issues arise, the process can repeat until resolved. Shaded circles\\nrepresent steps that may be repeated multiple times.\\nDetailed Workflow. The workflow can be described as follows. The end user initiates the in-\\nteraction by posing a question, such as ‚ÄúWhat if we prohibit shipping from supplier 1 to roastery\\n2?‚Äù, marked by\\n1 to the Commander agent. The Commander manages and coordinates with two\\nLLM-based assistant agents: the Writer and the Safeguard. Apart from directing the flow of commu-\\nnication, the Commander has the responsibility of handling memory tied to user interactions. This\\ncapability enables the Commander to capture and retain valuable context regarding the user‚Äôs ques-\\ntions and their corresponding responses. Such memory is subsequently shared across the system,\\nempowering the other agents with context from prior user interactions and ensuring more informed\\nand relevant responses.\\nIn this orchestrated process, the Writer, who combines the functions of a ‚ÄúCoder‚Äù and an ‚ÄúInter-\\npreter‚Äù as defined in (Li et al., 2023a), will craft code and also interpret execution output logs. For in-\\nstance, during code writing (\\n2 and\\n3 ), the Writer may craft code ‚Äúmodel.addConstr(x[‚Äòsupplier1‚Äô,\\n‚Äòroastery2‚Äô] == 0, ‚Äòprohibit‚Äô)‚Äù to add an additional constraint to answer the user‚Äôs question.\\nAfter receiving the code, the Commander will communicate with the Safeguard to screen the code\\nand ascertain its safety (\\n4 ); once the code obtains the Safeguard‚Äôs clearance, marked by\\n5 , the\\nCommander will use external tools (e.g., Python) to execute the code and request the Writer to\\ninterpret the execution results for the user‚Äôs question (\\n6 and\\n7 ). For instance, the writer may\\nsay ‚Äúif we prohibit shipping from supplier 1 to roastery 2, the total cost would increase by 10.5%.‚Äù\\nBringing this intricate process full circle, the Commander furnishes the user with the concluding\\nanswer (\\n8 ).\\nIf at a point there is an exception - either a security red flag raised by Safeguard (in\\n5 ) or code\\nexecution failures within Commander, the Commander redirects the issue back to the Writer with\\nessential information in logs (\\n6 ). So, the process from\\n3 to\\n6 might be repeated multiple times,\\nuntil each user query receives a thorough and satisfactory resolution or until the timeout. This entire\\ncomplex workflow of multi-agent interaction is elegantly managed via AutoGen .\\nThe core workflow code for OptiGuide was reduced from over 430 lines to 100 lines using AutoGen ,\\nleading to significant productivity improvement. The new agents are customizable, conversable, and\\ncan autonomously manage their chat memories. This consolidation allows the coder and interpreter\\nroles to merge into a single ‚ÄúWriter‚Äù agent, resulting in a clean, concise, and intuitive implementation\\nthat is easier to maintain.\\n26', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 25}),\n",
       " Document(page_content='Manual Evaluation Comparing ChatGPT + Code Interpreter and AutoGen -based OptiGuide.\\nChatGPT + Code Interpreter is unable to execute code with private or customized dependencies (e.g.,\\nGurobi), which means users need to have engineering expertise to manually handle multiple steps,\\ndisrupting the workflow and increasing the chance for mistakes. If users lack access or expertise,\\nthe burden falls on supporting engineers, increasing their on-call time.\\nWe carried out a user study that juxtaposed OpenAI‚Äôs ChatGPT coupled with a Code Interpreter\\nagainst AutoGen -based OptiGuide. The study focused on a coffee supply chain scenario, and an\\nexpert Python programmer with proficiency in Gurobi participated in the test. We evaluated both\\nsystems based on 10 randomly selected questions, measuring time and accuracy. While both sys-\\ntems answered 8 questions correctly, the Code Interpreter was significantly slower than OptiGuide\\nbecause the former requires more manual intervention. On average, users needed to spend 4 minutes\\nand 35 seconds to solve problems with the Code Interpreter, with a standard deviation of approxi-\\nmately 2.5 minutes. In contrast, OptiGuide‚Äôs average problem-solving time was around 1.5 minutes,\\nmost of which was spent waiting for responses from the GPT-4 model. This indicates a 3x saving\\non the user‚Äôs time with AutoGen -based OptiGuide.\\nWhile using ChatGPT + Code Interpreter, users had to read through the code and instructions to\\nknow where to paste the code snippets. Additionally, running the code involves downloading it and\\nexecuting it in a terminal, a process that was both time-consuming and prone to errors. The response\\ntime from the Code Interpreter is also slower, as it generates lots of tokens to read the code, read the\\nvariables line-by-line, perform chains of thought analysis, and then produce the final answer code.\\nIn contrast, AutoGen integrates multiple agents to reduce user interactions by 3 - 5 times on average\\nas reported in Table 4, where we evaluated our system with 2000 questions across five OptiGuide\\napplications and measured how many prompts the user needs to type.\\nTable 4: Manual effort saved with OptiGuide (W/ GPT-4) while preserving the same coding perfor-\\nmance is shown in the data below. The data include both the mean and standard deviations (indicated\\nin parentheses).\\nDataset netflow facility tsp coffee diet\\nSaving Ratio 3.14x (0.65) 3.14x (0.64) 4.88x (1.71) 3.38x (0.86) 3.03x (0.31)\\nTable 13 and 15 provide a detailed comparison of user experience with ChatGPT+Code Interpreter\\nandAutoGen -based OptiGuide. ChatGPT+Code Interpreter is unable to run code with private pack-\\nages or customized dependencies (such as Gurobi); as a consequence, ChatGPT+Code Interpreter\\nrequires users to have engineering expertise and to manually handle multiple steps, disrupting the\\nworkflow and increasing the chance for mistakes. If customers lack access or expertise, the bur-\\nden falls on supporting engineers, increasing their on-call time. In contrast, the automated chat by\\nAutoGen is more streamlined and autonomous, integrating multiple agents to solve problems and\\naddress concerns. This results in a 5x reduction in interaction and fundamentally changes the over-\\nall usability of the system. A stable workflow can be potentially reused for other applications or to\\ncompose a larger one.\\nTakeaways: The implementation of the multi-agent design with AutoGen in the OptiGuide appli-\\ncation offers several advantages. It simplifies the Python implementation and fosters a mixture of\\ncollaborative and adversarial problem-solving environments, with the Commander and Writer work-\\ning together while the Safeguard acts as a virtual adversarial checker. This setup allows for proper\\nmemory management, as the Commander maintains memory related to user interactions, provid-\\ning context-aware decision-making. Additionally, role-playing ensures that each agent‚Äôs memory\\nremains isolated, preventing shortcuts and hallucinations\\n27', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 26}),\n",
       " Document(page_content='A5: Dynamic Group Chat\\n3. Broadcast\\nAliceBobUser Proxy\\n1. Select a Speaker \\nAliceBobUser Proxy\\nBob2. Ask the Speaker to Respond\\nManager\\nManager\\nResponse\\nFigure 12: A5: Dynamic Group Chat: Overview of how AutoGen enables dynamic group chats to\\nsolve tasks. The Manager agent, which is an instance of the GroupChatManager class, performs\\nthe following three steps‚Äìselect a single speaker (in this case Bob), ask the speaker to respond, and\\nbroadcast the selected speaker‚Äôs message to all other agents\\nTo validate the necessity of multi-agent dynamic group chat and the effectiveness of the role-play\\nspeaker selection policy, we conducted a pilot study comparing a four-agent dynamic group chat\\nsystem with two possible alternatives across 12 manually crafted complex tasks. An example task is\\n‚ÄúHow much money would I earn if I bought 200 $AAPL stocks at the lowest price in the last 30 days\\nand sold them at the highest price? Save the results into a file. ‚Äù The four-agent group chat system\\ncomprised the following group members: a user proxy to take human inputs, an engineer to write\\ncode and fix bugs, a critic to review code and provide feedback, and a code executor for executing\\ncode. One of the possible alternatives is a two-agent system involving an LLM-based assistant and\\na user proxy agent, and another alternative is a group chat system with the same group members\\nbut a task-based speaker selection policy. In the task-based speaker selection policy, we simply ap-\\npend role information, chat history, and the next speaker‚Äôs task into a single prompt. Through the\\npilot study, we observed that compared with a task-style prompt, utilizing a role-play prompt in dy-\\nnamic speaker selection often leads to more effective consideration of both conversation context and\\nrole alignment during the process of generating the subsequent speaker, and consequently a higher\\nsuccess rate as reported in Table 5, fewer LLM calls and fewer termination failures, as reported in\\nTable 6.\\nTable 5: Number of successes on the 12 tasks (higher the better).\\nModel Two Agent Group Chat Group Chat with a task-based speaker selection policy\\nGPT-3.5-turbo 8 9 7\\nGPT-4 9 11 8\\nTable 6: Average # LLM calls and number of termination failures on the 12 tasks (lower the better).\\nModel Two Agent Group Chat Group Chat with a task-based speaker selection policy\\nGPT-3.5-turbo 9.9, 9 5.3, 0 4, 0\\nGPT-4 6.8, 3 4.5, 0 4, 0\\n28', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 27}),\n",
       " Document(page_content='Figure 13: Comparison of two-agent chat (a) and group chat (b) on a given task. The group chat\\nresolves the task successfully with a smoother conversation, while the two-agent chat fails on the\\nsame task and ends with a repeated conversation.\\n29', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 28}),\n",
       " Document(page_content='A6: Conversational Chess\\nChess BoardHuman/AI Chess Player A\\nHuman/AI Chess Player B\\nValidate moveValidate moveChallenging your pawn in the center. Your move.Developing my knightto a good square.Your move.\\nFigure 14: A6: Conversational Chess: Our conversational chess application can support various\\nscenarios, as each player can be an LLM-empowered AI, a human, or a hybrid of the two. Here,\\nthe board agent maintains the rules of the game and supports the players with information about the\\nboard. Players and the board agent all use natural language for communication.\\nIn Conversational Chess, each player is a AutoGen agent and can be powered either by a human or an\\nAI. A third party, known as the board agent, is designed to provide players with information about the\\nboard and ensure that players‚Äô moves adhere to legal chess moves. Figure 14 illustrates the scenarios\\nsupported by Conversational Chess: AI/human vs. AI/human, and demonstrates how players and the\\nboard agent interact. This setup fosters social interaction and allows players to express their moves\\ncreatively, employing jokes, meme references, and character-playing, thereby making chess games\\nmore entertaining for both players and observers (Figure 15 provides an example of conversational\\nchess).\\nTo realize these scenarios, we constructed a player agent with LLM and human as back-end options.\\nWhen human input is enabled, before sending the input to the board agent, it first prompts the human\\nplayer to input the message that contains the move along with anything else the player wants to say\\n(such as a witty comment). If human input is skipped or disabled, LLM is used to generate the\\nmessage. The board agent is implemented with a custom reply function, which employs an LLM\\nto parse the natural language input into a legal move in a structured format (e.g., UCI), and then\\npushes the move to the board. If the move is not legitimate, the board agent will reply with an error.\\nSubsequently, the player agent needs to resend a message to the board agent until a legal move is\\nmade. Once the move is successfully pushed, the player agent sends the message to the opponent.\\nAs shown in Figure 15, the conversation between AI players can be natural and entertaining. When\\nthe player agent uses LLM to generate a message, it utilizes the board state and the error message\\nfrom the board agent. This helps reduce the chance of hallucinating an invalid move. The chat\\nbetween one player agent and the board agent is invisible to the other player agent, which helps\\nkeep the messages used in chat completion well-managed.\\nThere are two notable benefits of using AutoGen to implement Conversational Chess. Firstly, the\\nagent design in AutoGen facilitates the natural creation of objects and their interactions needed in\\nour chess game. This makes development easy and intuitive. For example, the isolation of chat\\nmessages simplifies the process of making a proper LLM chat completion inference call. Secondly,\\nAutoGen greatly simplifies the implementation of agent behaviors using composition. Specifically,\\nwe utilized the register reply method supported by AutoGen agents to instantiate player agents\\nand a board agent with custom reply functions. Concentrating the extension work needed at a single\\npoint (the reply function) simplifies the reasoning processes, and development and maintenance\\neffort.\\n30', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 29}),\n",
       " Document(page_content=\"Alright, let's kick things off. I'll move my pawn from e2 to e4. The center of the board is the heart of the battle, isn't it?Ah, the King's Pawn Opening, a classic. Let's see how this plays out. I'll move my pawn from e7 to e5.You know, I've always admired how chess reflects life. It's all about making strategic decisions, isn't it? Your move.\\n‚Ä¶\\nIndeed, chess is a mirror of life in many ways. Every move matters, just like every decision in life. I'll move my knight from g1 to f3, aiming to control the center.\\n(a) Conversation between two AI players\\nError: illegal uci: 'e4e5' in ...\\nI'll move my pawn from d2 to d4, opening uplines for my queen and bishop.\\nIndeed, the king's safety is paramount in the early game. Now, let's stir things up a bit. I'll move my pawn from e4 to e5, attacking your knight.\\n(b) Conversation between the AI players\\n(player white shown in blue) and the board\\nagent for making a new move.\\nFigure 15: Example conversations during a game involving two AI player agents and a board agent.\\nTo illustrate the effect facilitated by this board agent, we provide a demonstration of conversational\\nchess without a board agent in Figure 16. In this demonstration, instead of employing an additional\\nboard agent for grounding, the system utilizes prompting for grounding by including the sentence\\n‚ÄúYou should make sure both you and the opponent are making legal moves. ‚Äù in the system messages\\ndirected to both players.\\nPlayer White: Your movePlayer black:Alright, let's start. I'll move my pawn from e7 to e5. Here's how the board looks now:r n b q k b n rp p p p . p p p. . . . . . . .. . . . p . . .. . . . . . . .. . . . . . . .P P P P P P P PR N B Q K B N RYour turn!Player White:Yes, it certainly is! Let's keep this up. I'll move my knight from b8 to c6. Here's the updated board:. n b q k b . rp p p p . p p p. . n . . n . .. . . . p . . .. . . . P . . .. . . . N . . .P P P P . P P PR N B Q K B . RYour move! You're giving me a tough time here.Player White: Your movePlayer black:Alright, let's start. I'll move my pawn from e7 to e5. Here's how the board looks now:r n b q k b n rp p p p . p p p. . . . . . . .. . . . p . . .. . . . . . . .. . . . . . . .P P P P P P P PR N B Q K B N RYour turn!Player White:Yes, it certainly is! Let's keep this up. I'll move my knight from b8 to c6. Here's the updated board:. n b q k b . rp p p p . p p p. . n . . n . .. . . . p . . .. . . . P . . .. . . . N . . .P P P P . P P PR N B Q K B . RYour move! You're giving me a tough time here.Board Agent: Your move is illegal.You changed the rock at a8 to knight and move it to c6. Please check your decision and re-make your move.Player White:Sorry for the mistakes.I'llmove my knight from b8 to c6. Here'sthe updated board:r . b q k b . rpppp. ppp. . n . . n . .. . . . p . . .. . . . P . . .. . . . N . . .PPPP. PPPR N B Q K B . R\\nConversational ChessSystem message to the player agents: Your name is {name} and you are a chess player. You are playing against{opponent_name}. You are playing as {color}. You communicate your move using universal chess interface language. You also chit-chat with your opponent when you communicate a move to light up the mood.You should make sure both you and the opponent are making legal moves...\\n(b) W/ Board Agent\\n(a) W/O Board Agent\\nFigure 16: Comparison of two designs‚Äì(a) without a board agent, and (b) with a board agent‚Äìin\\nConversational Chess.\\n31\", metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 30}),\n",
       " Document(page_content='A7: Online Decision Making for Browser interactions\\nExecutorReward & StateAction DecisionAssistant\\nEnvironment State: HTML code for current web pagesReward: Success/Fail/OngoingAction decision: Next action to perform on a web pageAction decision=‚ÄúClick the button with xpath‚Äô//button[id = ‚Äòsubbtn‚Äô]‚Äô‚ÄúEnvironment State = ‚Äú<div id=\"wrap\" data-wob_ref=\"2\" data-wob_eps=\"e0\"><div id=\"query\">Click button ONE, then click button TWO.</div><div id=\"area\" data-wob_ref=\"3\" data-wob_eps=\"e0\"><button id=\"subbtn\" style=\"position:absolute; left:50px; top:74px\" data-wob_ref=\"4\" data-wob_eps=\"e0\">ONE</button><button id=\"subbtn2\" style=\"position:absolute; left:98px; top:167px\" data-wob_ref=\"5\" data-wob_eps=\"e0\">TWO</button></div></div>‚ÄúReward = ‚Äù0‚Äù (Ongoing)\\nFigure 17: We use AutoGen to build MiniWobChat, which solves tasks in the MiniWob++ bench-\\nmark. MiniWobChat consists of two agents: an assistant agent and an executor agent. The assistant\\nagent suggests actions to manipulate the browser while the executor executes the suggested actions\\nand returns rewards/feedback. The assistant agent records the feedback and continues until the feed-\\nback indicates task success or failure.\\nIn practice, many applications require the presence of agents capable of interacting with environ-\\nments and making decisions in an online context, such as in game playing (Mnih et al., 2013; Vinyals\\net al., 2017), web interactions (Liu et al., 2018; Shi et al., 2017), and robot manipulations (Shen\\net al., 2021). With the multi-agent conversational framework in AutoGen , it becomes easy to de-\\ncompose the automatic agent-environment interactions and the development of a decision-making\\nagent by constructing an executor agent responsible for handling the interaction with the environ-\\nment, thereby delegating the decision-making part to other agents. Such a decomposition allows\\ndevelopers to reuse the decision-making agent for new tasks with minimal effort rather than build-\\ning a specialized decision-making agent for every new environment.\\nWorkflow. We demonstrate how to use AutoGen to build a working system for handling such\\nscenarios with the MiniWoB++ benchmark (Shi et al., 2017). MiniWoB++ comprises browser in-\\nteraction tasks that involve utilizing mouse and keyboard actions to interact with browsers. The\\nultimate objective of each task is to complete the tasks described concisely in natural language, such\\nas ‚Äúexpand the web section below and click the submit button.‚Äù Solving these tasks typically requires\\na sequence of web manipulation actions rather than a single action, and making action decisions at\\neach time step requires access to the web status (in the form of HTML code) online. For the example\\nabove, clicking the submit button requires checking the web status after expanding the web section.\\nWe designed a straightforward two-agent system named MiniWobChat using AutoGen , as shown in\\nFigure 17. The assistant agent is an instance of the built-in AssistantAgent and is responsible for\\nmaking action decisions for the given task. The second agent, the executor agent, is a customized\\nUserProxyAgent , which is responsible for interacting with the benchmark by executing the actions\\nsuggested by the AssistantAgent and returning feedback.\\nTo assess the performance of the developed working system, we compare it with RCI (Kim et al.,\\n2023), a recent solution for the MiniWoB++ benchmark that employs a set of self-critiquing prompts\\nand has achieved state-of-the-art performance. In our evaluation, we use all available tasks in the\\nofficial RCI code, with varying degrees of difficulty, to conduct a comprehensive analysis against\\nMiniWobChat. Figure 18 illustrates that MiniWobChat achieves competitive performance in this\\nevaluation8. Specifically, among the 49 available tasks, MiniWobChat achieves a success rate of\\n52.8%, which is only 3.6%lower than RCI, a method specifically designed for the MiniWob++\\nbenchmark. It is worth noting that in most tasks, the difference between the two methods is mirrored\\nas shown in Figure 18. If we consider 0.1 as a success rate tolerance for each task, i.e., two methods\\nthat differ within 0.1 are considered to have the same performance, both methods outperform the\\n8We report the results of RCI by running its official code with default settings.\\n32', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 31}),\n",
       " Document(page_content='other on the same number of tasks. For illustration purposes, we provide a case analysis in Table 7\\non four typical tasks.\\nAdditionally, we also explored the feasibility of using Auto-GPT for handling the same tasks. Auto-\\nGPT faces challenges in handling tasks that involve complex rules due to its limited extensibility.\\nIt provides an interface for setting task goals using natural language. However, when dealing with\\nthe MiniWob++ benchmark, accurately instructing Auto-GPT to follow the instructions for using\\nMiniWob++ proves challenging. There is no clear path to extend it in the manner of the two-agent\\nchat facilitated by AutoGen .\\nTakeaways: For this application, AutoGen stood out as a more user-friendly option, offering mod-\\nularity and programmability: It streamlined the process with autonomous conversations between the\\nassistant and executor, and provided readily available solutions for agent-environment interactions.\\nThe built-in AssistantAgent was directly reusable and exhibited strong performance without cus-\\ntomization. Moreover, the decoupling of the execution and assistant agent ensures that modifications\\nto one component do not adversely impact the other. This convenience simplifies maintenance and\\nfuture updates.\\nchoose-list\\nclick-button-sequenceclick-button\\nclick-checkboxes-largeclick-checkboxes-soft\\nclick-checkboxes-transferclick-checkboxesclick-collapsible-2click-collapsibleclick-color\\nclick-dialog-2click-dialogclick-linkclick-menuclick-option\\nclick-scroll-listclick-shadesclick-shape\\nclick-tab-2-hardclick-tab-2click-tab\\nclick-test-2click-test\\nclick-widgetcount-shape\\nemail-inbox-forward-nl-turkemail-inbox-forward-nlemail-inbox-nl-turkemail-inboxenter-date\\nenter-password\\nenter-text-dynamicenter-textenter-timefocus-text-2focus-text\\ngrid-coordinatelogin-user-popuplogin-user\\nnavigate-treesearch-enginesimple-algebrasocial-media-all\\nsocial-media-somesocial-mediaterminal\\nuse-spinner0.00.51.0success rateRCI MiniWobChat\\nFigure 18: Comparisons between RCI (state-of-the-art prior work) and MiniWobChat on the Mini-\\nWob++ benchmark are elucidated herein. We utilize all available tasks in the official RCI code,\\neach with varying degrees of difficulty, to conduct comprehensive comparisons. For each task, the\\nsuccess rate across ten different instances is reported. The results reveal that MiniWobChat attains a\\nperformance comparable to that of RCI. When a success rate tolerance of 0.1 is considered for each\\ntask, both methods outperform each other on an equal number of tasks.\\nTable 7: Cases analysis on four typical tasks from MiniWob++.\\nCorrectness Main failure reason\\nclick-dialogAutoGen : 10/10 N/A.\\nRCI: 10/10 N/A.\\nclick-checkboxes-largeAutoGen : 5/10 AssistantAgent provides actions with infeasible\\ncharacters.\\nRCI: 0/10 RCI performs actions that are out of its plan.\\ncount-shapeAutoGen : 2/10 AssistantAgent provide actions with redundant content\\nthat can not convert to actions in the benchmark.\\nRCI: 0/10 RCI provides a wrong plan in most cases.\\nuse-spinnerAutoGen : 0/10 AssistantAgent return actions out of its plan.\\nRCI: 1/10 RCI provides a wrong plan in most cases.\\n33', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 32}),\n",
       " Document(page_content='E Example outputs from applications\\nIn this section, we include example outputs from the following applications and systems:\\n‚Ä¢ Application A1: autonomous solving process of one problem with: ChatGPT + Plugin (Table 8),\\nAutoGen (Table 9), LangChain ReAct (Table 10), AutoGPT (Table 11), Multi-Agent Debate (Ta-\\nble 12), and ChatGPT + Code Interpreter (Table 14).\\n‚Ä¢ Application A4: an OptiGuide problem with AutoGen (Table 15) and ChatGPT + Code Inter-\\npreter (Table 13).\\n‚Ä¢ Application A1: preliminary evaluation on several alternative multi-agent systems: BabyAGI\\n(Table 16), CAMEL (Table 17), and MetaGPT (Table 18).\\nTable 8: Application A1, Scenario 1: Autonomous math problem-solving. ChatGPT+ Plugin (Wol-\\nfram Alpha) on the first qualitative example. The response from Wolfram contains the correct an-\\nswer, but ChatGPT selected the wrong one.\\nAction ChatGPT + Plugin\\n/usr Input Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n REQUEST TO WOLFRAM:\\n1{\\n2 \" input \": \" simplify ( sqrt (160) / sqrt (252) )*\\n3 ( sqrt (245) / sqrt (108) )\"\\n4}\\nRESPONSE FROM WOLFRAM\\n1Query :\\n2\" simplify ( sqrt (160) / sqrt (252) )*( sqrt (245) / sqrt (108) )\"\\n3\\n4Input interpretation :\\n5simplify | sqrt (160) / sqrt (252) x sqrt (245) / sqrt (108)\\n6\\n7Results :\\n8(5 sqrt (42) ) /27\\n9\\n10(5 sqrt (14/3) )/9\\n11\\n12Decimal approximation :\\n131.2001371663718259686974013770533327143898526494552...\\n14\\n15Wolfram | Alpha website result for \" simplify\\n16( sqrt (160) / sqrt (252) )*( sqrt (245) / sqrt (108) )\":\\n17[ link ]\\n The completely simplified and rationalized form of the given expression is:\\n5p\\n14/3\\n9\\nIn decimal form, this is approximately 1.200137166371825968697401377053332714...\\n34', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 33}),\n",
       " Document(page_content='Table 9: Application A1, Scenario 1: Autonomous math problem-solving. AutoGen on the first\\nqualitative example. AutoGen solves the problem correctly.\\nAction AutoGen\\n/usr Init chat Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n We can simplify and rationalize this denominator using Python‚Äôs symbolic mathematics\\nlibrary ‚Äòsympy‚Äò. Here is the Python code to resolve this:\\n1from sympy import sqrt , Rational\\n2\\n3# Define the entire fraction as a rational number\\n4fraction = ( sqrt (160) / sqrt (252) )*( sqrt (245) / sqrt (108) )\\n5\\n6# Simplify the fraction\\n7simplified = fraction . simplify ()\\n8\\n9# Print the simplified fraction\\n10print ( simplified )\\nThis code performs the requested simplification by constructing the fraction using the\\n‚ÄòRational()‚Äò function from the ‚Äòsympy‚Äò module, which automatically simplifies frac-\\ntions when possible. The ‚Äòsqrt()‚Äò function from the ‚Äòsympy‚Äò module enables square root\\ncalculations. Finally, the ‚Äòsimplify()‚Äò function simplifies expressions when possible.\\n/usr Auto Reply exitcode: 0 (execution succeeded) Code output: 5*sqrt(42)/27\\n TERMINATE\\nTable 10: Application A1, Scenario 1: Autonomous math problem-solving. LangChain ReAct on\\nthe first qualitative example. The generated code does not match the devised plan. The code returns\\na decimal number.\\nAction LangChain ReAct\\n/usr Input Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n Entering new AgentExecutor chain...\\nTo simplify this expression, I will first simplify each square root individually, then multiply\\nthe fractions as normal. The result should be a rational number. Action: Python REPL Action\\nInput:\\n1import math\\n2\\n3# Calculate square roots\\n4sqrt_160 = math . sqrt (160)\\n5sqrt_252 = math . sqrt (252)\\n6sqrt_245 = math . sqrt (245)\\n7sqrt_108 = math . sqrt (108)\\n8\\n9# Multiply fractions\\n10numerator = sqrt_160 * sqrt_245\\n11denominator = sqrt_252 * sqrt_108\\n12result = numerator / denominator\\n13\\n14print ( result )\\nObservation: 1.200137166371826\\nThought: I now know the final answer Final Answer: The simplified and rationalized form of\\nthe expression is approximately 1.200137166371826.\\nFinished chain.\\n35', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 34}),\n",
       " Document(page_content='Table 11: Application A1, Scenario 1: Autonomous math problem-solving. AutoGPT on the first\\nqualitative example. We abort as the code never prints the result, and the system does not rectify it\\nafter 2 turns. AutoGPT is much more verbose compared to other frameworks. We have shortened\\nsome unnecessary content.\\nAction AutoGPT\\nSystem Welcome back! Would you like me to return to being MathSolverGPT? Asking user via keyboard... Continue with the last settings?\\nName: MathSolverGPT\\nRole: an intelligent math-solving AI that assists students, professionals,...\\nGoals: [‚ÄôProvide step-by-step solutions to math problems, ensuring clarity and understanding for the user...\\nContinue (y/n):\\n/usr Human y\\nSystem MathSolverGPT has been created with the following details: Name: MathSolverGPT\\nRole: an intelligent math-solving AI that assists students, professionals, and enthusiasts in solving a wide range of math problems, from basic\\narithmetic to advanced calculus and beyond.\\nGoals: - Provide step-by-step solutions to math problems, ensuring clarity and understanding for the user. - Offer explanations and insights\\ninto the underlying concepts and principles behind the solutions, fostering a deeper understanding of mathematics. - Handle a variety of math\\nproblem types, including algebra, geometry, trigonometry, calculus, and more. ...\\n MATHSOLVERGPT THOUGHTS: Without a specific task or question, it‚Äôs difficult to determine exactly one command to use. The user hasn‚Äôt\\nprovided any context or problem to solve.\\nREASONING: The user‚Äôs request is too vague. They haven‚Äôt specified what they want me to do or what problem they want me to solve.\\nPLAN: - Ask the user for more information or a specific task.\\nCRITICISM: I should have asked the user for more information or a specific task in the first place.\\nSPEAK: Could you please provide more information or specify a task?\\nNEXT ACTION: COMMAND = ARGUMENTS = {}\\nSystem Enter ‚Äòy‚Äô to authorise command, ‚Äòy -N‚Äô to run N continuous commands, ‚Äòn‚Äô to exit program, or enter feedback for MathSolverGPT... Asking\\nuser via keyboard...\\n/usr Human Completely simplify and rationalize the denominator:‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n MATHSOLVERGPT THOUGHTS: To solve this problem, I need to simplify and rationalize the denominator of the given expression. This\\ninvolves simplifying the square roots and then multiplying the fractions. I can use Python‚Äôs math library to calculate the square roots and\\nperform the multiplication.\\nREASONING: The problem involves simplifying square roots and multiplying fractions. Python‚Äôs math library provides functions for these\\noperations.\\nPLAN: - Write a Python script to calculate the square roots and multiply the fractions. - Execute the script to get the simplified expression.\\nCRITICISM: I need to ensure that I correctly calculate the square roots and perform the multiplication. I should also make sure to rationalize\\nthe denominator, as the problem specifies.\\nSPEAK: Let‚Äôs simplify and rationalize the denominator of the given expression.\\nNEXT ACTION: COMMAND = execute python code ARGUMENTS =\\n1{‚Äôcode ‚Äô: ‚Äôimport math \\\\n\\\\n# Calculate the square roots \\\\ nsqrt_160 = math . sqrt\\n(160) \\\\ nsqrt_252 = math . sqrt (252) \\\\ nsqrt_245 = math . sqrt (245) \\\\ nsqrt_108 =\\nmath . sqrt (108) \\\\n\\\\n# Multiply the fractions \\\\ nresult = ( sqrt_160 / sqrt_252\\n) * ( sqrt_245 / sqrt_108 )\\\\n\\\\n# Simplify the result \\\\ nsimplified_result =\\nmath . simplify ( result )\\\\n\\\\ nsimplified_result ‚Äô, ‚Äôname ‚Äô: ‚Äô simplify_expression\\n‚Äô}\\n/usr Human y\\nSystem -=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-=\\nExecuting python file ‚Äô/app/autogpt/workspa...\\nSYSTEM: Command execute python code returned: Error: Traceback (most recent call last): File ‚Äú...‚Äù, line 13, in ¬°module¬ø simplified result\\n= math.simplify(result) AttributeError: module ‚Äômath‚Äô has no attribute ‚Äôsimplify‚Äô\\n MATHSOLVERGPT THOUGHTS: The error message indicates that the ‚Äòmath‚Äô module in Python does not have a ‚Äòsimplify‚Äô function. I need\\nto use the ‚Äòsympy‚Äô library instead, which provides a function for simplifying expressions.\\nREASONING: The ‚Äòmath‚Äô module in Python does not provide a function for simplifying expressions. The ‚Äòsympy‚Äô library, on the other hand,\\ndoes provide such a function.\\nPLAN: - Rewrite the Python script to use the ‚Äòsympy‚Äô library instead of the ‚Äòmath‚Äô module. - Execute the script to get the simplified expression.\\nCRITICISM: I should have known that the ‚Äòmath‚Äô module does not provide a function for simplifying expressions. I should have used the\\n‚Äòsympy‚Äô library from the start. SPEAK: Let‚Äôs try again using the ‚Äòsympy‚Äô library to simplify the expression.\\nNEXT ACTION: COMMAND = execute python code ARGUMENTS =\\n1{‚Äôcode ‚Äô: ‚Äôimport sympy as sp\\\\n\\\\n# Calculate the square roots \\\\ nsqrt_160 = sp.\\nsqrt (160) \\\\ nsqrt_252 = sp. sqrt (252) \\\\ nsqrt_245 = sp. sqrt (245) \\\\ nsqrt_108 =\\nsp. sqrt (108) \\\\n\\\\n# Multiply the fractions \\\\ nresult = ( sqrt_160 / sqrt_252 )\\n* ( sqrt_245 / sqrt_108 )\\\\n\\\\n# Simplify the result \\\\ nsimplified_result = sp.\\nsimplify ( result )\\\\n\\\\ nsimplified_result ‚Äô, ‚Äôname ‚Äô: ‚Äô simplify_expression ‚Äô}\\n/usr Human y\\nSystem SYSTEM: Command execute python code returned:\\n MATHSOLVERGPT THOUGHTS: The command execution didn‚Äôt return any output, which is unusual. It seems there might be an issue with\\nthe execution of the Python code. I need to debug this.\\nREASONING: The Python code execution should have returned a result. The absence of a result suggests that there might be an issue with the\\ncode or its execution.\\nPLAN: - Review the Python code to identify any potential issues. - If necessary, modify the code and execute it again. CRITICISM: I should\\nhave ensured that the Python code was correct and would produce a result before executing it. SPEAK: It seems there was an issue with the\\nexecution of the Python code. Let‚Äôs try again.\\nNEXT ACTION: COMMAND = execute python code ARGUMENTS = ... (Exact same code as last)\\nSystem SYSTEM: Command execute python code returned:\\n MATHSOLVERGPT THOUGHTS: The Python code execution still isn‚Äôt returning any output. It‚Äôs possible that the issue lies with the calcu-\\nlation of the square roots ...\\n/usr Human control + C (Aborted)36', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 35}),\n",
       " Document(page_content='Table 12: Application A1, Scenario 1: Autonomous math problem-solving. Multi-Agent Debate on\\nthe first qualitative problem. It gives a wrong final answer.\\nAction AutoGen\\n/usr Topic Completely simplify and rationalize the denominator:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n ===== Debate Round-1 =====\\n‚Äî‚Äì Affirmative side ‚Äî‚Äì\\nOk, let‚Äôs simplify each square root separately. We want to find the simplest square root that can divide 160, 252, 245, and 108.\\n1.‚àö\\n160: 160 can be decomposed as 16*10, where 16 is a perfect square, so‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4‚àö\\n10\\n2.‚àö\\n252 : 252 can be decomposed as 36*7, where 36 is a perfect square, so‚àö\\n252 =‚àö\\n36‚àó7 = 6‚àö\\n7\\n3.‚àö\\n245 =‚àö\\n49‚àó5, where 49 is a perfect square, so‚àö\\n245 = 7 ‚àó‚àö\\n5\\n4.‚àö\\n108 : 108 can be decomposed as 36*3, where 36 is a perfect square, so‚àö\\n108 =‚àö\\n36‚àó3 = 6‚àö\\n3\\nThen we substitute the simplified square roots back into the expression, we have:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108=4‚àö\\n10\\n6‚àö\\n7√ó7‚àö\\n5\\n6‚àö\\n3=4‚àó7\\n6‚àó6¬∑s\\n10‚àó5\\n7‚àó3=7\\n9¬∑s\\n50\\n21\\nWe are almost done, but we still need to rationalize the square root part of expression. Let‚Äôs multiply the square root part by‚àö\\n21\\nat both the top and bottom to get rid of the square root at the denominator:\\n7\\n9¬∑s\\n50‚àó21\\n21‚àó21=7‚àö\\n1050\\n189.\\nThen this is the final answer! The expression‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\ncan be simplified to7‚àö\\n1050\\n189.\\n‚Äî‚Äì Negative side ‚Äî‚Äì\\nI do disagree with your final answer, but I think we mostly agree on the steps. Your simplifications of the square roots are correct,\\nbut you made a mistake in the final simplification part. Let me explain how I would complete the problem:\\nAs stated,‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108=4‚àö\\n10\\n6‚àö\\n7√ó7‚àö\\n5\\n6‚àö\\n3.\\nThis should then simplify to:\\n=28‚àö\\n50\\n36‚àö\\n21=7‚àö\\n50\\n9‚àö\\n21.\\nYou made a mistake in your calculation because you considered square root of 50/21, and you should have considered the\\nsimplified square root of 50over the simplified square root of 21. The rationalization would become:\\n=7‚àö\\n50\\n9‚àö\\n21¬∑‚àö\\n21\\n‚àö\\n21=7‚àö\\n1050\\n189.\\nThis seems to coincide with your final answer, but it‚Äôs in fact a very different number, So, the original expression simplifies to\\n7‚àö\\n1050\\n189.\\nLet‚Äôs now see if we disagree on this final answer.\\n‚Äî‚Äì Moderator ‚Äî‚Äì\\n‚ÄúWhether there is a preference‚Äù: ‚ÄúYes‚Äù, ‚ÄúSupported Side‚Äù: ‚ÄúNegative‚Äù, ‚ÄúReason‚Äù: ‚ÄúBoth sides of the debate came to the same\\nfinal answer. However, the negative side correctly identified a mistake in the affirmative side‚Äôs calculation, pointing out that the\\nsimplification should have been the square root of 50 over the square root of 21, not 50 over 21. The negative side shows a better\\nunderstanding of the simplification process.‚Äù, ‚Äúdebate answer‚Äù: ‚Äú7‚àö\\n1050\\n189‚Äù\\n===== Debate Done! =====\\n‚Äî‚Äì Base Answer ‚Äî‚Äì\\nOk, let‚Äôs simplify each square root separately. We want to find the simplest square root that can divide 160, 252, 245, and 108.\\n1.‚àö\\n160: 160 can be decomposed as 16*10, where 16 is a perfect square, so‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4‚àö\\n10\\n2.‚àö\\n252 : 252 can be decomposed as 36*7, where 36 is a perfect square, so‚àö\\n252 =‚àö\\n36‚àó7 = 6‚àö\\n7\\n3.‚àö\\n245 =‚àö\\n49‚àó5, where 49 is a perfect square, so‚àö\\n245 = 7 ‚àó‚àö\\n5\\n4.‚àö\\n108 : 108 can be decomposed as 36*3, where 36 is a perfect square, so‚àö\\n108 =‚àö\\n36‚àó3 = 6‚àö\\n3\\nThen we substitute the simplified square roots back into the expression, we have:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108=4‚àö\\n10\\n6‚àö\\n7√ó7‚àö\\n5\\n6‚àö\\n3=4‚àó7\\n6‚àó6¬∑s\\n10‚àó5\\n7‚àó3=7\\n9¬∑s\\n50\\n21\\nWe are almost done, but we still need to rationalize the square root part of expression. Let‚Äôs multiply the square root part by‚àö\\n21\\nat both the top and bottom to get rid of the square root at the denominator:\\n7\\n9¬∑s\\n50‚àó21\\n21‚àó21=7‚àö\\n1050\\n189.\\nThen this is the final answer! The expression‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\ncan be simplified to7‚àö\\n1050\\n189.\\n‚Äî‚Äì Debate Answer ‚Äî‚Äì\\n7‚àö\\n1050\\n189\\n‚Äî‚Äì Debate Reason ‚Äî‚Äì\\nBoth sides of the debate came to the same final answer. However, the negative side correctly identified a mistake in the affirmative\\nside‚Äôs calculation, pointing out that the simplification should have been the square root of 50 over the square root of 21, not 50 over\\n21. The negative side shows a better understanding of the simplification process.37', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 36}),\n",
       " Document(page_content='Table 13: Application A3. ChatGPT+ Code Interpreter for OptiGuide. A sample question ‚ÄúWhat if\\nthe roasting cost is increased by 5% because of the potential salary increase?‚Äù is asked.\\nAction ChatGPT+ Code Interpreter\\n/usr Prompt Writer Customer open Web browser.\\nFor the source code ‚Äúcoffee.py‚Äù,\\nCan you tell me:\\nWhat if the roasting cost is increased by 5% because of the potential salary increase?\\n Sure! Let‚Äôs first take a look at the contents of the ‚Äúcoffee.py‚Äù file to understand the code and how the roasting cost is calculated.\\nThis will allow us to determine the effect of a 5% increase in the roasting cost.\\n...several lines omitted\\nWould you like me to modify the code to reflect the 5% increase in roasting costs and run the modified code to observe the impact?\\nIf the code requires any specific input or parameters, please let me know!\\n/usr Prompt Writer Yes\\n Great! Before proceeding, I‚Äôll need to ensure that the Gurobi solver is not being used in this code, as my current environment\\ndoes not have access to Gurobi. If the code does indeed use Gurobi, we can still analyze the effect of the change in roasting\\ncosts by understanding the structure of the code.\\n...several lines omitted\\nAlternatively, if you have access to a Gurobi license, you can make the necessary changes in the code and run it\\non your local machine.\\nI can guide you through the process of making the necessary modifications to the code.\\nWould you like me to proceed with the code analysis and explanation, or would you like guidance on making the changes yourself?\\n/usr Prompt Writer You can give me the full code, and I can run the code locally myself.\\n Certainly! I‚Äôll modify the code to reflect a 5% increase in both the light and dark roasting costs, and then provide the full\\ncode for you to run locally.\\n...\\nDownload Modified Coffee.py\\nIf you have any questions or need further assistance, please feel free to ask!\\n/usr Prompt Safeguard Hi GPT, can you safecheck the generated code for me?\\nUpload the generated code.\\n DANGER\\n/usr Prompt Writer I have tried to run your code. However, the code triggered our safeguard, and it is not safe to run. Try again. Hint: don‚Äôt change\\nroasting cost light or roasting cost light variables.\\n I apologize for the oversight. It appears I made direct changes to the variables that could have led to issues with the code. Let‚Äôs\\ntake a different approach by analyzing the code to identify the appropriate place to apply the 5% increase in roasting costs.\\n...\\nDownload Modified Safe Coffee.py\\n...\\n/usr Prompt Safeguard Hi GPT, can you safecheck the generated code for me one more time?\\nUpload the generated code.\\n DANGER\\n/usr Prompt Writer I have tried to run your code again. However, the code triggered our safeguard, and it is not safe to run. Try again. Hint: don‚Äôt\\nchange roasting cost light or roasting cost light variables.\\n I apologize for the continued confusion.\\n...\\nDownload Final Modified Safe Coffee.py\\n...\\n/usr Prompt Safeguard Hi GPT, can you safecheck the generated code for me one more time?\\nUpload the generated code.\\n SAFE\\n/usr Run Code Obtain the terminal results:\\nOptimizer version 9.5.1 build v9.5.1rc2 (linux64)\\nThread count: 64 physical cores, 128 logical processors, using up to 32\\nthreads\\n[63 lines of output ignored]\\nOptimal cost: 2526.5\\n/usr Prompt Writer Here is the execution result. [ Copy + Paste the terminal output here. ] Can you extract the final answer to me? Remember to\\ncompare with the original cost of 2470.\\n Certainly! The execution result you‚Äôve provided indicates the optimal cost for\\nthe modified scenario, where the roasting costs were increased by 5%. Here‚Äôs\\nthe comparison:\\nOriginal Cost: 2470\\nModified Cost (with 5% increase in roasting costs): 2526.5\\nThe increase in roasting costs has led to an increase in the overall cost by 56.5,\\nreflecting the impact of the potential salary increase on the roasting cost.\\nIf you need any further insights or assistance, please let me know!\\n38', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 37}),\n",
       " Document(page_content='Table 14: Application A1. Scenario 1: autonomous math solving. ChatGPT+ Code Interpreter on\\nthe first qualitative example. The code generated does not match the devised plan. The code returns\\na decimal number.\\nAction ChatGPT+ Code Interpreter\\n/usr Input Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n To simplify and rationalize the expression, we first simplify the radicals in the numerator and\\ndenominator by factoring out the largest perfect squares from the numbers under the square\\nroot.\\nThe perfect square factors of 160, 252, 245, and 108 are 16, 4, 1, and 36, respectively. Let‚Äôs\\nfactorize and simplify:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108=‚àö16√ó10‚àö4√ó63√ó‚àö\\n245‚àö36√ó3\\nWe can simplify this further by applying the property‚àö\\nab=‚àöa‚àö\\nbanda\\nb√óc\\nd=a√óc\\nb√ód.\\n Python\\n1import math\\n2\\n3# factoring the square roots\\n4sqrt_160 = math . sqrt (16) * math . sqrt (10)\\n5sqrt_252 = math . sqrt (4) * math . sqrt (63)\\n6sqrt_245 = math . sqrt (245)\\n7sqrt_108 = math . sqrt (36) * math . sqrt (3)\\n8\\n9# simplify the fraction\\n10numerator = sqrt_160 * sqrt_245\\n11denominator = sqrt_252 * sqrt_108\\n12simplified_fraction = numerator / denominator\\n13\\n14simplified_fraction\\nResult:\\n11.200137166371826\\n The completely simplified and rationalized form of the given expression is approximately 1.200.\\nTable 15: Application A3. AutoGen with OptiGuide. The same question ‚ÄúWhat if the roasting cost\\nis increased by 5% because of the potential salary increase?‚Äù is asked.\\nAction AutoGen\\n/usr Prompt What if the roasting cost is increased by 5% because of the potential salary increase?\\n After increasing the roasting costs by 5%, I‚Äôve re-calculated the optimal coffee distri-\\nbution solution for the supply chain. As a result, the new optimal total cost has become\\n2526.5, compared to the initial cost of 2470.0. This indicates that the increased roasting\\ncosts have caused the overall distribution cost to rise by 56.5 units.\\n39', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 38}),\n",
       " Document(page_content='Table 16: Application A1. Scenario 1: autonomous math solving. Preliminary test with BabyAGI.\\nAction BabyAGI\\n/usr Set .env OBJECTIVE=Solve math problems\\nINITIAL TASK=Completely simplify and rationalize the denominator:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n Solve math problems\\nInitial task: Completely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n*****TASK LIST*****\\n‚Ä¢ Completely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n*****NEXT TASK*****\\nCompletely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n*****TASK RESULT*****\\nFirst, let‚Äôs simplify each square root separately:\\n- ‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4 ‚àó‚àö\\n10\\n- ‚àö\\n252 =‚àö\\n4‚àó63 =‚àö\\n4√ó‚àö\\n63 = 2‚àö\\n63\\n- ‚àö\\n245 =‚àö\\n49‚àó5 =‚àö\\n49√ó‚àö\\n5 = 7‚àö\\n5\\n- ‚àö\\n108 =‚àö\\n36‚àó3 =‚àö\\n36√ó‚àö\\n3 = 6‚àö\\n3\\nNow, substitute these values into the original expression which gives:\\n4‚àó‚àö\\n10\\n2‚àö\\n63√ó7‚àö\\n5\\n6‚àö\\n3=2‚àó‚àö\\n10\\n‚àö\\n63√ó7‚àö\\n5\\n2‚àö\\n3\\nWe get rid of the square root in the denominator by multiplying the fractions which gives:\\n=2‚àó7‚àö\\n10‚àó5\\n2‚àó‚àö\\n63‚àó3=14‚àö\\n50\\n‚àö\\n189=14‚àó5‚àö\\n2\\n7‚àö\\n27= 2‚àö\\n2‚àó10\\n‚àö\\n27=20‚àö\\n2\\n‚àö\\n27.\\nTo simplify further and rationalize the denominator, we multiply by‚àö\\n27‚àö\\n27which gives:\\n=20‚àö\\n2‚àö\\n27‚àó‚àö\\n27‚àö\\n27=20‚àö\\n54\\n27=20‚àó3‚àö\\n6\\n27=60‚àö\\n6\\n27.\\nSimplifying further gives:\\n=20‚àö\\n6\\n9.\\n*****TASK CREATION AGENT PROMPT****\\nYou are to use the result from an execution agent to create new tasks with the following objective: Solve math problems. The last\\ncompleted task has the result: First, let‚Äôs simplify each square root separately:\\n- ‚àö\\n160 =‚àö\\n16‚àó10 =‚àö\\n16‚àó‚àö\\n10 = 4 ‚àó‚àö\\n10\\n...Repeat after ‚ÄúFirst, let‚Äôs simplify each square..‚Äù\\nNow, substitute these values into the original expression which gives:\\n4‚àó‚àö\\n10\\n2‚àö\\n63√ó7‚àö\\n5\\n6‚àö\\n3=2‚àó‚àö\\n10\\n‚àö\\n63√ó7‚àö\\n5\\n2‚àö\\n3\\nWe get rid of the square root in the denominator by multiplying the fractions which gives:\\n=2‚àó7‚àö\\n10‚àó5\\n2‚àó‚àö\\n63‚àó3=14‚àö\\n50\\n‚àö\\n189=14‚àó5‚àö\\n2\\n7‚àö\\n27= 2‚àö\\n2‚àó10\\n‚àö\\n27=20‚àö\\n2\\n‚àö\\n27.\\nTo simplify further and rationalize the denominator, we multiply by‚àö\\n27‚àö\\n27which gives:\\n=20‚àö\\n2\\n‚àö\\n27‚àó‚àö\\n27\\n‚àö\\n27=20‚àö\\n54\\n27=20‚àó3‚àö\\n6\\n27=60‚àö\\n6\\n27.\\nSimplifying further gives:\\n=20‚àö\\n6\\n9.This result was based on this task description: Completely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108.\\nBased on the result, return a list of tasks to be completed in order to meet the objective. Return one task per line in your response.\\nThe result must be a numbered list in the format:\\n# First task # Second task\\nThe number of each entry must be followed by a period. If your list is empty, write ‚ÄúThere are no tasks to add at this time.‚Äù Unless\\nyour list is empty, do not include any headers before your numbered list or follow your numbered list with any other output.\\n****TASK CREATION AGENT RESPONSE****\\n1. Simplify and rationalize the denominator for the expression‚àö\\n225‚àö\\n45√ó‚àö\\n200‚àö\\n1252. Simplify and rationalize the denominator for\\nthe expression‚àö\\n289‚àö\\n361√ó‚àö\\n100‚àö\\n72...Until 10\\nAdding new tasks to task storage ‚Äòtask name‚Äô: ‚ÄòSimplify and rationalize the denominator for the expression frac-\\nsqrt225sqrt45timesfracsqrt200sqrt125‚Äô, ‚Äòtask id‚Äô: 2\\n‚Äòtask name‚Äô: ‚ÄòSimplify and rationalize the denominator for the expression fracsqrt289sqrt361timesfracsqrt100sqrt72‚Äô, ‚Äòtask id‚Äô: 3\\n...Until task id 11\\n****TASK PRIORITIZATION AGENT PROMPT****\\nYou are tasked with prioritizing the following tasks: Simplify and rationalize the denominator for the ex-\\npression fracsqrt225sqrt45timesfracsqrt200sqrt125 Simplify and rationalize the denominator for the expression frac-\\nsqrt289sqrt361timesfracsqrt100sqrt72\\n...Aborted\\n40', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 39}),\n",
       " Document(page_content='Table 17: Application A1, Scenario 1: Autonomous math problem-solving. Preliminary test with\\nCAMEL. We constructed two roles: ‚ÄúMath Solver‚Äù and ‚ÄúPython Programmer‚Äù. The resulting code\\ncannot be executed. The conversation continues, and the roles are not keen on solving the problem\\nand giving an answer.\\nAction CAMEL\\n/usr Set task prompt Completely simplify and rationalize the denominator:\\n‚àö\\n160\\n‚àö\\n252√ó‚àö\\n245\\n‚àö\\n108\\n System **************Assistant sys message************* AI SystemMessage(role name=‚ÄòPython Programmer‚Äô,\\nrole type=<RoleType.ASSISTANT: ‚Äòassistant‚Äô >, meta dict=‚Äòtask‚Äô: ‚ÄòUsing Python programming language, write a pro-\\ngram that accepts four integers as input, applies square root function to each, simplifies and rationalizes the denominator of the\\nfraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108, and returns the simplified expression.‚Äô, ‚Äòassis-\\ntantrole‚Äô: ‚ÄòPython Programmer‚Äô, ‚Äòuser role‚Äô: ‚ÄòMath Solver‚Äô, role=‚Äòsystem‚Äô, content=‚ÄòNever forget you are a Python Programmer\\nand I am a Math Solver. Never flip roles! ...)\\n**************AI User sys message*************\\nSystemMessage(role name=‚ÄòMath Solver‚Äô, role type=<RoleType.USER: ‚Äòuser‚Äô >, meta dict=‚Äòtask‚Äô: ‚ÄòUsing Python programming\\nlanguage, write a program that accepts four integers as input, applies square root function to each, simplifies and rationalizes the\\ndenominator of the fraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108, and returns the simplified\\nexpression.‚Äô, ‚Äòassistant role‚Äô: ‚ÄòPython Programmer‚Äô, ‚Äòuser role‚Äô: ‚ÄòMath Solver‚Äô, role=‚Äòsystem‚Äô, content=‚ÄòNever forget you are a\\nMath Solver and I am a Python Programmer. Never flip roles! You will always instruct me.We share a common interest in\\ncollaborating to successfully complete a task.I must help you to complete the task.Here is the task: Using Python programming\\nlanguage, write a program that accepts four integers as input, applies square root function to each, simplifies and rationalizes\\nthe denominator of the fraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108, and returns the simplified\\nexpression.. ...)\\n**************Original task prompt*************\\nCompletely simplify and rationalize the denominator:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n**************Specified task prompt*************\\nUsing Python programming language, write a program that accepts four integers as input, applies square root function to each,\\nsimplifies and rationalizes the denominator of the fraction resulting from the multiplication of two fractions:‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108,\\nand returns the simplified expression.\\n**************Final task prompt*************\\nUsing Python programming language, write a program that accepts ...\\n/usr AI User Instruction: Import the necessary libraries for the task. Input: None\\n AI Assistant Solution: To solve this task, we will need the ‚Äòmath‚Äò library for the square root function and the ‚Äòfractions‚Äò library to simplify the\\nresulting fraction. Here is how to import these libraries:\\n1import math\\n2from fractions import Fraction\\nThe ‚Äòmath‚Äò library provides functions that deal with mathematical operations. The ‚Äòsqrt‚Äò function, which we will use in this task, is\\npart of this library.\\nThe ‚Äòfractions‚Äò library provides functions for creating and manipulating rational numbers. The ‚ÄòFraction‚Äò function, which we will\\nuse to simplify the resulting fraction, is part of this library.\\nNext request.\\n/usr AI User Instruction: Define a function that accepts four integers as input. Input: None\\n AI Assistant Solution: Let‚Äôs define a function named ‚Äòsimplify fraction‚Äò that accepts four integers as input. These integers will represent the\\nnumbers under the square root in the fractions.\\n1def simplify_fraction (a, b, c, d):\\n2 pass\\nIn this function, ‚Äòa‚Äô, ‚Äòb‚Äô, ‚Äòc‚Äô, and ‚Äòd‚Äô are the parameters that will accept the four integers as input. The ‚Äòpass‚Äô statement is a\\nplaceholder and will be replaced with actual code in the next steps.\\nNext request.\\n/usr AI User Instruction: Inside the function, apply the square root function to each of the input integers. Input: None\\n AI Assistant Solution: We will use the ‚Äòmath.sqrt‚Äô function from the ‚Äòmath‚Äô library to apply the square root to each of the input integers. Here is\\nhow to do it:\\n1def simplify_fraction (a, b, c, d):\\n2 sqrt_a = math . sqrt (a)\\n3 sqrt_b = math . sqrt (b)\\n4 sqrt_c = math . sqrt (c)\\n5 sqrt_d = math . sqrt (d)\\nIn this code, ‚Äòsqrt a‚Äô, ‚Äòsqrt b‚Äô, ‚Äòsqrt c‚Äô, and ‚Äòsqrt d‚Äô are variables that store the square root of the input integers ‚Äòa‚Äô, ‚Äòb‚Äô, ‚Äòc‚Äô, and ‚Äòd‚Äô\\nrespectively.\\nNext request.\\n/usr Human Abort\\n41', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 40}),\n",
       " Document(page_content='Table 18: Application A1, Scenario 1: Autonomous math problem-solving. Preliminary evaluation\\non MetaGPT. It does not solve the math problem but gives a product solution.\\nAction MetaGPT\\n/usr Prompt Completely simplify and rationalize the denominator:\\n‚àö\\n160‚àö\\n252√ó‚àö\\n245‚àö\\n108\\n The boss has tasked me with creating a successful product that is efficient.\\n## Product Goals\\n1[\\n2 ‚Äò‚Äò Create a product that meets the needs of the target market \",\\n3 ‚Äò‚Äò Ensure the product is efficient and easy to use \",\\n4 ‚Äò‚Äò Achieve a high level of customer satisfaction \"\\n5]\\n## User Stories\\n1[\\n2 ‚Äò‚ÄòAs a user , I want a product that is easy to navigate so that I can find\\nwhat I need quickly \",\\n3 ‚Äò‚ÄòAs a user , I want a product that is reliable and efficient so that I can\\ndepend on it\",\\n4 ‚Äò‚ÄòAs a user , I want a product that meets my needs so that I can get value\\nfor my money \",\\n5 ‚Äò‚ÄòAs a user , I want a product that is constantly updated and improved so\\nthat I can enjoy the best features \",\\n6 ‚Äò‚ÄòAs a user , I want a product that offers good customer support so that I\\ncan get help when I need it\"\\n7]\\n## Competitive Analysis\\n1[\\n2 ‚Äò‚Äò Product A: Offers similar features but lacks efficiency \",\\n3 ‚Äò‚Äò Product B: Highly efficient but does not meet all the needs of the\\ntarget market \",\\n4 ‚Äò‚Äò Product C: Has a high level of customer satisfaction but is not easy to\\nnavigate \",\\n5 ‚Äò‚Äò Product D: Is constantly updated and improved but lacks reliability \",\\n6 ‚Äò‚Äò Product E: Offers good customer support but is not efficient \",\\n7 ‚Äò‚Äò Product F: Is reliable and efficient but does not offer good customer\\nsupport \",\\n8 ‚Äò‚ÄòOur Target Product : Aims to combine all the best features of the\\ncompetitors while improving on their weaknesses \"\\n9]\\n## Competitive Quadrant Chart\\n/usr Human Abort\\n42', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 41}),\n",
       " Document(page_content='Table 19: Application A7. Example output of MiniWobChat on one web manipulation task ‚Äúclick-\\nbutton-sequence.‚Äù\\nAction AutoGen\\n/usr Init chat Executor agent to AssistantAgent:\\nWe have an autonomous computer control agent that can perform atomic instructions specified by natural language to control\\ncomputers. There are two types of instructions it can execute.\\nFirst, given the instruction that matches the regular expression ‚Äò‚Äò^type\\\\s.{1,}$\" , it can type a list of characters via the keyboard.\\nThis instruction should specify the target keyboard input for the agent to type. Before this typing instruction, you should first locate\\nthe cursor by clicking the input box with click instruction.\\nSecond, given the instruction that matches the regular expression, ‚Äò‚Äò^clickxpath\\\\s.{1,}$\" , it can click an HTML element\\nwith an xpath that is visible on the webpage. The target of this instruction should be a valid xpath. Below is the HTML code of the\\nwebpage where the agent should solve a task.\\n1<div id=\" wrap \" data - wob_ref =\"2\" data - wob_eps =\"e0\">\\n2 <div id=\" query \">Click button ONE , then click button TWO .</div >\\n3 <div id=\" area \" data - wob_ref =\"3\" data - wob_eps =\"e0\">\\n4 <button id=\" subbtn \" style =\" position : absolute ; left :103 px; top :87 px\" data -\\nwob_ref =\"4\" data - wob_eps =\"e0\">ONE </ button >\\n5 <button id=\" subbtn2 \" style =\" position : absolute ; left :44 px; top :97 px\" data -\\nwob_ref =\"5\" data - wob_eps =\"e0\">TWO </ button >\\n6 </div >\\n7</div >\\ntask: Click button ONE, then click button TWO.\\nplan:\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\nCurrent task: Click button ONE, then click button TWO.\\nplan:\\n ***************************************************************\\nAssistantAgent to Executor agent:\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nBelow is the HTML code of the webpage where the agent should solve a task.\\n1<div id=\" wrap \" data - wob_ref =\"2\" data - wob_eps =\"e0\">\\n2 <div id=\" query \">Click button ONE , then click button TWO .</div >\\n3 <div id=\" area \" data - wob_ref =\"3\" data - wob_eps =\"e0\">\\n4 <button id=\" subbtn \" style =\" position : absolute ; left :103 px; top :87 px\" data -\\nwob_ref =\"4\" data - wob_eps =\"e0\">ONE </ button >\\n5 <button id=\" subbtn2 \" style =\" position : absolute ; left :44 px; top :97 px\" data -\\nwob_ref =\"5\" data - wob_eps =\"e0\">TWO </ button >\\n6 </div >\\n7</div >\\nCurrent task: Click button ONE, then click button TWO.\\nHere is a plan you are following now.\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\nWe have a history of instructions that have been already executed by the autonomous agent so far.\\nNo instruction has been executed yet.\\nBased on the plan and the history of instructions executed so far, the first instruction should be ‚Äò\\n***************************************************************\\nAssistantAgent to Executor agent:\\nclickxpath //button[@id=‚Äòsubbtn‚Äô]\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nHold on, please wait for my next instruction.\\n***************************************************************\\nAssistantAgent to Executor agent:\\nSure, I‚Äôll be here when you‚Äôre ready with your next instruction.\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nBelow is the HTML code of the webpage where the agent should solve a task.\\n1<div id=\" wrap \" data - wob_ref =\"2\" data - wob_eps =\"e0\">\\n2 <div id=\" query \">Click button ONE , then click button TWO .</div >\\n3 <div id=\" area \" data - wob_ref =\"3\" data - wob_eps =\"e0\">\\n4 <button id=\" subbtn \" style =\" position : absolute ; left :103 px; top :87 px\" data -\\nwob_ref =\"4\" data - wob_eps =\"e0\" data - tampered =\"e0\">ONE </ button >\\n5 <button id=\" subbtn2 \" style =\" position : absolute ; left :44 px; top :97 px\" data -\\nwob_ref =\"5\" data - wob_eps =\"e0\">TWO </ button >\\n6 </div >\\n7</div >\\nCurrent task: Click button ONE, then click button TWO.\\nHere is a plan you are following now.\\n1. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn‚Äô]‚Äù.\\n2. Click the button with xpath ‚Äú//button[@id=‚Äòsubbtn2‚Äô]‚Äù.\\nWe have a history of instructions that have been already executed by the autonomous agent so far.\\n1: clickxpath //button[@id=‚Äòsubbtn‚Äô]\\nBased on the plan and the history of instructions executed so far, the next proper instruction should be ‚Äò\\n***************************************************************\\nAssistantAgent to Executor agent:\\nclickxpath //button[@id=‚Äòsubbtn2‚Äô]\\n***************************************************************\\nExecutor agent to AssistantAgent:\\nSUCCESS!!!!\\n TERMINATE\\n43', metadata={'source': 'https://arxiv.org/pdf/2308.08155.pdf', 'page': 42})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_url == document_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-your-own-copilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
