{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have configured Azure AI services, set the appropriate configuration parameters, and set up a Conda environment to ensure reproducibility. You can find the setup instructions and how to create a Conda environment in the [REQUIREMENTS.md](REQUIREMENTS.md) file.\n",
    "\n",
    "## ðŸ“‹ Table of Contents\n",
    "\n",
    "This notebook guides you through the following sections:\n",
    "\n",
    "> **ðŸ’¡ Note:** Please refer to the notebook `01-creation-indexes.ipynb` for detailed information and steps on how to create Azure AI Search Indexes.\n",
    "\n",
    "1. [**Indexing Vectorized Content from multiple formats and sources**](#index-documents)\n",
    "    - Chunk, vectorize, and index local PDF files and website addresses.\n",
    "    - Download, chunk, vectorize, and index all `.docx` files from a SharePoint site.\n",
    "    \n",
    "2. [**Indexing Vectorized Content from complex layout documents laveraging OCR Capabilities**](#index-images)\n",
    "    - Leverage complex OCR, image recognition using Azure Document Intelligence. Chunk, vectorize, and index extracted metadata from Dcouments\n",
    "\n",
    "3. [**Indexing Vectorized Content from Audio**](#index-audio) (TODO)\n",
    "    - Process WAV audio data using Azure AI Speech transalations capabilities, chunk, vectorize, and index audio files stored in Blob Storage and indexed in Azure AI Search.\n",
    "\n",
    "Before you start, please take a look at the README. It contains detailed instructions, diagrams, and information about the class structure and automation used in this project, specifically in the `AzureAIIndexer` backend. [README.md](README.md) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = r\"C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\"  # change your directory here\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Azure AI Search Indexes \n",
    "\n",
    "Please refer to the notebook [01-creation-indexes.ipynb](01-creation-indexes.ipynb) for detailed information and steps on how to create Azure AI Search Indexes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“š Indexing Vectorized Content from Multiple Sources and Various Formats\n",
    "\n",
    "In this section, we will explore how to index vectorized content from various sources and in different formats. This includes local PDF files, website addresses, `.docx` files from a SharePoint site, and more. We will chunk, vectorize, and index these different types of content, leveraging the power of Azure AI Search Indexes. This process allows us to create a comprehensive, searchable index that can handle a wide range of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 14:27:11,943 - micro - MainProcess - INFO     Loading OpenAIEmbeddings object with model, deployment foundational-ada, and chunk size 1000 (ai_search_indexing.py:load_embedding_model:161)\n",
      "c:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\vector-indexing-azureaisearch\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189: LangChainDeprecationWarning: The class `AzureOpenAIEmbeddings` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.AzureOpenAIEmbeddings instead.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\vector-indexing-azureaisearch\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.OpenAIEmbeddings instead.\n",
      "  warn_deprecated(\n",
      "2024-02-11 14:27:13,478 - micro - MainProcess - INFO     AzureOpenAIEmbeddings object has been created successfully. You can now access the embeddings\n",
      "                using the '.embeddings' attribute. (ai_search_indexing.py:load_embedding_model:174)\n",
      "vector_search_configuration is not a known attribute of class <class 'azure.search.documents.indexes.models._index.SearchField'> and will be ignored\n",
      "2024-02-11 14:27:15,353 - micro - MainProcess - INFO     The Azure AI search index 'test-index-002' has been loaded correctly. (ai_search_indexing.py:load_azureai_index:225)\n",
      "2024-02-11 14:27:15,364 - micro - MainProcess - INFO     Successfully loaded environment variables: TENANT_ID, CLIENT_ID, CLIENT_SECRET (sharepoint_data_extractor.py:load_environment_variables_from_env_file:87)\n",
      "2024-02-11 14:27:15,970 - micro - MainProcess - INFO     New access token retrieved. (sharepoint_data_extractor.py:msgraph_auth:121)\n"
     ]
    }
   ],
   "source": [
    "# Import the AzureAIndexer class from the ai_search_indexing module\n",
    "from src.indexers.ai_search_indexing import AzureAIndexer\n",
    "\n",
    "DEPLOYMENT_NAME = \"foundational-ada\"\n",
    "INDEX_NAME = \"test-index-002\"\n",
    "\n",
    "# Create an instance of the AzureAIndexer class\n",
    "azure_search_indexer_client = AzureAIndexer(\n",
    "    index_name=INDEX_NAME, embedding_azure_deployment_name=DEPLOYMENT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing PDFs, DOCX, and Images from Blob Storage \n",
    "\n",
    "The `load_files_and_split_into_chunks` function is a powerful tool for indexing and processing documents. It is designed to streamline the initial steps of loading files, splitting them into manageable chunks, and preparing your documents for further processing and conversion. \n",
    "\n",
    "Here are its key features:\n",
    "\n",
    "- **Multi-Format Support**: The function can process documents in different formats (PDFs, Word documents, images, etc.) from various sources (blob storage, URLs, local paths). You can pass a list of file paths, each possibly in a different format.\n",
    "\n",
    "- **Automated File Loading**: The function efficiently loads files into memory, eliminating the need for manual file handling. It manages the reading and processing of each file.\n",
    "\n",
    "- **Advanced Text Splitting**: After loading, the function splits the text into manageable chunks, crucial for processing large documents. You can customize the chunk size and overlap according to your needs.\n",
    "\n",
    "- **Versatile Splitting Options**: You can choose from various splitters - 'by_title', 'by_character_recursive', 'by_character_brute_force' - to fit your specific text processing requirements.\n",
    "\n",
    "- **Encoding Capabilities**: The function can optionally use an encoder during splitting. This feature is particularly useful for certain text analysis tasks. You can specify the model used for encoding (default is \"gpt-4\").\n",
    "\n",
    "- **OCR Capabilities**: If the 'ocr' parameter is set to True, the function will use Optical Character Recognition (OCR) via Azure Document Intelligence to extract text from images or scanned documents.\n",
    "\n",
    "- **Verbose Logging**: You can enable detailed logging for in-depth progress tracking and easier debugging.\n",
    "\n",
    "- **High Customizability**: The function's behavior can be tailored to your needs with additional keyword arguments. This includes options like retaining separators in chunks, using separators as regex patterns, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 14:27:25,945 - micro - MainProcess - INFO     Reading .pdf file from local path C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\\utils\\data\\autogen.pdf. (from_blob.py:load_document:67)\n",
      "2024-02-11 14:27:25,946 - micro - MainProcess - INFO     Loading file with Loader PyPDFLoader (from_blob.py:load_document:79)\n",
      "2024-02-11 14:27:28,396 - micro - MainProcess - INFO     Reading .pdf file from https://arxiv.org/pdf/2308.08155.pdf. (from_blob.py:load_document:75)\n",
      "2024-02-11 14:27:28,397 - micro - MainProcess - INFO     Loading file with Loader PyPDFLoader (from_blob.py:load_document:79)\n",
      "2024-02-11 14:27:32,116 - micro - MainProcess - INFO     Successfully downloaded blob file autogen.pdf (blob_data_extractors.py:extract_content:91)\n",
      "2024-02-11 14:27:32,160 - micro - MainProcess - INFO     Reading .pdf file from temporary location C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmp5kk5eaws originally sourced from https://testeastusdev001.blob.core.windows.net/testretrieval/autogen.pdf. (from_blob.py:load_document:63)\n",
      "2024-02-11 14:27:32,161 - micro - MainProcess - INFO     Loading file with Loader PyPDFLoader (from_blob.py:load_document:79)\n",
      "2024-02-11 14:27:34,253 - micro - MainProcess - INFO     Deleted temporary file: C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmp5kk5eaws (from_blob.py:load_document_from_bytes:132)\n",
      "2024-02-11 14:27:34,254 - micro - MainProcess - INFO     Reading .docx file from local path C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\\utils\\data\\test.docx. (from_blob.py:load_document:67)\n",
      "2024-02-11 14:27:34,255 - micro - MainProcess - INFO     Loading file with Loader Docx2txtLoader (from_blob.py:load_document:79)\n",
      "2024-02-11 14:27:34,508 - micro - MainProcess - INFO     Successfully downloaded blob file test.docx (blob_data_extractors.py:extract_content:91)\n",
      "2024-02-11 14:27:34,543 - micro - MainProcess - INFO     Reading .docx file from temporary location C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpu2ucf3o1 originally sourced from https://testeastusdev001.blob.core.windows.net/testretrieval/test.docx. (from_blob.py:load_document:63)\n",
      "2024-02-11 14:27:34,545 - micro - MainProcess - INFO     Loading file with Loader Docx2txtLoader (from_blob.py:load_document:79)\n",
      "2024-02-11 14:27:34,646 - micro - MainProcess - INFO     Deleted temporary file: C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpu2ucf3o1 (from_blob.py:load_document_from_bytes:132)\n",
      "2024-02-11 14:27:34,648 - micro - MainProcess - INFO     Creating a splitter of type: by_character_recursive (by_character.py:get_splitter:62)\n",
      "2024-02-11 14:27:34,649 - micro - MainProcess - INFO     Obtained splitter of type: RecursiveCharacterTextSplitter (by_character.py:split_documents_in_chunks_from_documents:165)\n",
      "2024-02-11 14:27:34,713 - micro - MainProcess - INFO     Number of chunks obtained: 1408 (by_character.py:split_documents_in_chunks_from_documents:168)\n"
     ]
    }
   ],
   "source": [
    "# Define file paths and URLs\n",
    "local_pdf_path = \"utils/data/autogen.pdf\"\n",
    "remote_pdf_url = \"https://arxiv.org/pdf/2308.08155.pdf\"\n",
    "blob_pdf_url = (\n",
    "    \"https://testeastusdev001.blob.core.windows.net/testretrieval/autogen.pdf\"\n",
    ")\n",
    "local_word_path = \"utils/data/test.docx\"\n",
    "remote_word_url = (\n",
    "    \"https://testeastusdev001.blob.core.windows.net/testretrieval/test.docx\"\n",
    ")\n",
    "\n",
    "# Combine all paths and URLs into a list. This is optional if you want to process multiple files at once.\n",
    "# It will also work by passing a string for simple file processing.\n",
    "file_sources = [\n",
    "    local_pdf_path,\n",
    "    remote_pdf_url,\n",
    "    blob_pdf_url,\n",
    "    local_word_path,\n",
    "    remote_word_url,\n",
    "]\n",
    "\n",
    "# Define parameters for the load_files_and_split_into_chunks function\n",
    "splitter_params = {\n",
    "    \"splitter_type\": \"by_character_recursive\",\n",
    "    \"use_encoder\": False,\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 128,\n",
    "    \"verbose\": False,\n",
    "    \"keep_separator\": True,\n",
    "    \"is_separator_regex\": False,\n",
    "    \"model_name\": \"gpt-4\",\n",
    "}\n",
    "\n",
    "# Load files and split them into chunks\n",
    "document_chunks_to_index = azure_search_indexer_client.load_files_and_split_into_chunks(\n",
    "    file_paths=file_sources, **splitter_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 14:27:38,725 - micro - MainProcess - INFO     Embedding and indexing initiated for 1408 text chunks. (ai_search_indexing.py:index_text_embeddings:490)\n",
      "2024-02-11 14:30:05,022 - micro - MainProcess - INFO     Embedding and indexing completed for 1408 text chunks. (ai_search_indexing.py:index_text_embeddings:494)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index the document chunks using the Azure Search Indexer client\n",
    "azure_search_indexer_client.index_text_embeddings(document_chunks_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Pdfs and Docs from Sharepoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\"testdocx.docx\", \"autogen.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 11:04:23,101 - micro - MainProcess - INFO     Getting the Site ID... (sharepoint_data_extractor.py:get_site_id:191)\n",
      "2024-02-06 11:04:23,728 - micro - MainProcess - INFO     Site ID retrieved: mngenvmcap747548.sharepoint.com,877fe60f-a62d-4ed1-8eda-af543c437d2c,ac47d8a7-cd54-4344-bd9d-26ada5a075c0 (sharepoint_data_extractor.py:get_site_id:195)\n",
      "2024-02-06 11:04:24,556 - micro - MainProcess - INFO     Successfully retrieved drive ID: b!D-Z_hy2m0U6O2q9UPEN9LKfYR6xUzURDvZ0mraWgdcAot0GWx37EQLiVD3sO7-vm (sharepoint_data_extractor.py:get_drive_id:212)\n",
      "2024-02-06 11:04:24,557 - micro - MainProcess - INFO     Making request to Microsoft Graph API (sharepoint_data_extractor.py:get_files_in_site:251)\n",
      "2024-02-06 11:04:25,148 - micro - MainProcess - INFO     Received response from Microsoft Graph API (sharepoint_data_extractor.py:get_files_in_site:254)\n",
      "2024-02-06 11:04:26,324 - micro - MainProcess - INFO     Reading .docx file from temporary location C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmp0orls4bf originally sourced from https://mngenvmcap747548.sharepoint.com/sites/Contoso/_layouts/15/Doc.aspx?sourcedoc=%7BE89A926E-0D92-42A8-800D-B8E8DB806BFB%7D&file=testdocx.docx&action=default&mobileredirect=true. (from_sharepoint.py:load_file:111)\n",
      "2024-02-06 11:04:26,325 - micro - MainProcess - INFO     Loading file with Loader Docx2txtLoader (from_sharepoint.py:load_file:127)\n",
      "2024-02-06 11:04:26,425 - micro - MainProcess - INFO     Deleted temporary file: C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmp0orls4bf (from_sharepoint.py:load_file_from_bytes:75)\n",
      "2024-02-06 11:04:26,426 - micro - MainProcess - INFO     Getting the Site ID... (sharepoint_data_extractor.py:get_site_id:191)\n",
      "2024-02-06 11:04:27,074 - micro - MainProcess - INFO     Site ID retrieved: mngenvmcap747548.sharepoint.com,877fe60f-a62d-4ed1-8eda-af543c437d2c,ac47d8a7-cd54-4344-bd9d-26ada5a075c0 (sharepoint_data_extractor.py:get_site_id:195)\n",
      "2024-02-06 11:04:27,707 - micro - MainProcess - INFO     Successfully retrieved drive ID: b!D-Z_hy2m0U6O2q9UPEN9LKfYR6xUzURDvZ0mraWgdcAot0GWx37EQLiVD3sO7-vm (sharepoint_data_extractor.py:get_drive_id:212)\n",
      "2024-02-06 11:04:27,710 - micro - MainProcess - INFO     Making request to Microsoft Graph API (sharepoint_data_extractor.py:get_files_in_site:251)\n",
      "2024-02-06 11:04:28,280 - micro - MainProcess - INFO     Received response from Microsoft Graph API (sharepoint_data_extractor.py:get_files_in_site:254)\n",
      "2024-02-06 11:04:29,722 - micro - MainProcess - INFO     Reading .pdf file from temporary location C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpk6y91p9b originally sourced from https://mngenvmcap747548.sharepoint.com/sites/Contoso/Shared%20Documents/autogen.pdf. (from_sharepoint.py:load_file:111)\n",
      "2024-02-06 11:04:29,723 - micro - MainProcess - INFO     Loading file with Loader PyPDFLoader (from_sharepoint.py:load_file:127)\n",
      "2024-02-06 11:04:31,663 - micro - MainProcess - INFO     Deleted temporary file: C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpk6y91p9b (from_sharepoint.py:load_file_from_bytes:75)\n",
      "2024-02-06 11:04:31,664 - micro - MainProcess - INFO     Creating a splitter of type: by_character_recursive (by_character.py:get_splitter:62)\n",
      "2024-02-06 11:04:31,666 - micro - MainProcess - INFO     Obtained splitter of type: RecursiveCharacterTextSplitter (by_character.py:split_documents_in_chunks_from_documents:165)\n",
      "2024-02-06 11:04:31,683 - micro - MainProcess - INFO     Number of chunks obtained: 495 (by_character.py:split_documents_in_chunks_from_documents:168)\n"
     ]
    }
   ],
   "source": [
    "# Define parameters for the load_files_and_split_into_chunks function\n",
    "splitter_params = {\n",
    "    \"splitter_type\": \"by_character_recursive\",\n",
    "    \"use_encoder\": False,\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 128,\n",
    "    \"verbose\": False,\n",
    "    \"keep_separator\": True,\n",
    "    \"is_separator_regex\": False,\n",
    "    \"model_name\": \"gpt-4\",\n",
    "}\n",
    "\n",
    "document_chunks_to_index = (\n",
    "    azure_search_indexer_client.load_files_and_split_into_chunks_from_sharepoint(\n",
    "        site_domain=os.environ[\"SITE_DOMAIN\"],\n",
    "        site_name=os.environ[\"SITE_NAME\"],\n",
    "        file_names=file_names,\n",
    "        **splitter_params,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 11:04:31,712 - micro - MainProcess - INFO     Embedding and indexing initiated for 495 text chunks. (ai_search_indexing.py:index_text_embeddings:485)\n",
      "2024-02-06 11:05:24,688 - micro - MainProcess - INFO     Embedding and indexing completed for 495 text chunks. (ai_search_indexing.py:index_text_embeddings:489)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index the document chunks using the Azure Search Indexer client\n",
    "azure_search_indexer_client.index_text_embeddings(document_chunks_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Indexing Vectorized Content from Complex Layout Documents Leveraging OCR Capabilities\n",
    "\n",
    "In this section, we will be using Azure's Document Intelligence in the backend to extract elements from complex layout documents. This process involves extracting the title and other metadata from the documents, which allows us to chunk the document by sections. \n",
    "\n",
    "We use a simple algorithm that chunks the document based on sections. This approach ensures that each chunk is semantically coherent and can be indexed separately. However, there's a risk that some sections might be longer than the context window of our model. To mitigate this, we can add an additional layer to the algorithm that further chunks sections based on a cutoff count number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_blob = \"https://testeastusdev001.blob.core.windows.net/customskillspdf/instruction-manual-fieldvue-dvc6200-hw2-digital-valve-controller-en-123052.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 11:05:24,725 - micro - MainProcess - INFO     Blob URL detected. Extracting content. (ocr_document_intelligence.py:analyze_document:147)\n",
      "2024-02-06 11:05:25,574 - micro - MainProcess - INFO     Successfully downloaded blob file instruction-manual-fieldvue-dvc6200-hw2-digital-valve-controller-en-123052.pdf (blob_data_extractors.py:extract_content:89)\n",
      "2024-02-06 11:05:57,701 - micro - MainProcess - INFO     Successfully extracted content from https://testeastusdev001.blob.core.windows.net/customskillspdf/instruction-manual-fieldvue-dvc6200-hw2-digital-valve-controller-en-123052.pdf (ocr_data_extractors.py:extract_content:82)\n",
      "2024-02-06 11:05:57,704 - micro - MainProcess - INFO     Section headings: ['# Scope of Manual', '# Conventions Used in this Manual', '# Description', '# Specifications', '# WARNING A', '# Related Documents', '# Table 1-2. Specifications', '# Table 1-2. Specifications (continued)'] (by_title.py:split_text_by_headings:34)\n",
      "2024-02-06 11:05:57,705 - micro - MainProcess - INFO     Number of chunks: 9 (by_title.py:split_text_by_headings:35)\n",
      "2024-02-06 11:05:57,707 - micro - MainProcess - INFO     Processed chunk 1 of 9 (by_title.py:combine_chunks:58)\n",
      "2024-02-06 11:05:57,708 - micro - MainProcess - INFO     Processed chunk 2 of 9 (by_title.py:combine_chunks:58)\n",
      "2024-02-06 11:05:57,710 - micro - MainProcess - INFO     Processed chunk 3 of 9 (by_title.py:combine_chunks:58)\n",
      "2024-02-06 11:05:57,713 - micro - MainProcess - INFO     Processed chunk 4 of 9 (by_title.py:combine_chunks:58)\n",
      "2024-02-06 11:05:57,716 - micro - MainProcess - INFO     Processed chunk 5 of 9 (by_title.py:combine_chunks:58)\n",
      "2024-02-06 11:05:57,716 - micro - MainProcess - INFO     Processed chunk 6 of 9 (by_title.py:combine_chunks:58)\n",
      "2024-02-06 11:05:57,718 - micro - MainProcess - INFO     Processed chunk 7 of 9 (by_title.py:combine_chunks:58)\n",
      "2024-02-06 11:05:57,721 - micro - MainProcess - INFO     Processed chunk 8 of 9 (by_title.py:combine_chunks:58)\n",
      "2024-02-06 11:05:57,725 - micro - MainProcess - INFO     Processed chunk 9 of 9 (by_title.py:combine_chunks:58)\n",
      "2024-02-06 11:05:57,727 - micro - MainProcess - INFO     Processed document 1 of 1 (by_title.py:split_documents_in_chunks_from_documents:104)\n"
     ]
    }
   ],
   "source": [
    "# Define parameters for the load_files_and_split_into_chunks function\n",
    "splitter_params = {\n",
    "    \"splitter_type\": \"by_title\",\n",
    "    \"ocr\": True,\n",
    "    \"ocr_output_format\": \"markdown\",\n",
    "    \"pages\": \"3-7\",\n",
    "}\n",
    "\n",
    "document_chunks_to_index = azure_search_indexer_client.load_files_and_split_into_chunks(\n",
    "    file_paths=document_blob,\n",
    "    **splitter_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 11:05:57,751 - micro - MainProcess - INFO     Embedding and indexing initiated for 5 text chunks. (ai_search_indexing.py:index_text_embeddings:485)\n",
      "2024-02-06 11:05:58,714 - micro - MainProcess - INFO     Embedding and indexing completed for 5 text chunks. (ai_search_indexing.py:index_text_embeddings:489)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index the document chunks using the Azure Search Indexer client\n",
    "azure_search_indexer_client.index_text_embeddings(document_chunks_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here, we are extracting the text from complex documents using OCR and passing it to a batched recursive iterator, sectioned by title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 11:05:58,743 - micro - MainProcess - INFO     Blob URL detected. Extracting content. (ocr_document_intelligence.py:analyze_document:147)\n",
      "2024-02-06 11:05:58,916 - micro - MainProcess - INFO     Successfully downloaded blob file instruction-manual-fieldvue-dvc6200-hw2-digital-valve-controller-en-123052.pdf (blob_data_extractors.py:extract_content:89)\n",
      "2024-02-06 11:06:30,396 - micro - MainProcess - INFO     Successfully extracted content from https://testeastusdev001.blob.core.windows.net/customskillspdf/instruction-manual-fieldvue-dvc6200-hw2-digital-valve-controller-en-123052.pdf (ocr_data_extractors.py:extract_content:82)\n",
      "2024-02-06 11:06:30,398 - micro - MainProcess - INFO     Creating a splitter of type: by_character_recursive (by_character.py:get_splitter:62)\n",
      "2024-02-06 11:06:30,399 - micro - MainProcess - INFO     Obtained splitter of type: RecursiveCharacterTextSplitter (by_character.py:split_documents_in_chunks_from_documents:165)\n",
      "2024-02-06 11:06:30,402 - micro - MainProcess - INFO     Number of chunks obtained: 38 (by_character.py:split_documents_in_chunks_from_documents:168)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Number: 1, Character Count: 504, Token Count: 107\n",
      "Chunk Number: 2, Character Count: 339, Token Count: 63\n",
      "Chunk Number: 3, Character Count: 362, Token Count: 65\n",
      "Chunk Number: 4, Character Count: 511, Token Count: 96\n",
      "Chunk Number: 5, Character Count: 472, Token Count: 95\n",
      "Chunk Number: 6, Character Count: 510, Token Count: 99\n",
      "Chunk Number: 7, Character Count: 183, Token Count: 36\n",
      "Chunk Number: 8, Character Count: 253, Token Count: 74\n",
      "Chunk Number: 9, Character Count: 408, Token Count: 90\n",
      "Chunk Number: 10, Character Count: 304, Token Count: 55\n",
      "Chunk Number: 11, Character Count: 380, Token Count: 70\n",
      "Chunk Number: 12, Character Count: 494, Token Count: 159\n",
      "Chunk Number: 13, Character Count: 487, Token Count: 142\n",
      "Chunk Number: 14, Character Count: 244, Token Count: 55\n",
      "Chunk Number: 15, Character Count: 502, Token Count: 138\n",
      "Chunk Number: 16, Character Count: 210, Token Count: 62\n",
      "Chunk Number: 17, Character Count: 454, Token Count: 88\n",
      "Chunk Number: 18, Character Count: 456, Token Count: 113\n",
      "Chunk Number: 19, Character Count: 479, Token Count: 123\n",
      "Chunk Number: 20, Character Count: 503, Token Count: 129\n",
      "Chunk Number: 21, Character Count: 376, Token Count: 88\n",
      "Chunk Number: 22, Character Count: 445, Token Count: 105\n",
      "Chunk Number: 23, Character Count: 340, Token Count: 100\n",
      "Chunk Number: 24, Character Count: 505, Token Count: 125\n",
      "Chunk Number: 25, Character Count: 346, Token Count: 89\n",
      "Chunk Number: 26, Character Count: 386, Token Count: 97\n",
      "Chunk Number: 27, Character Count: 401, Token Count: 111\n",
      "Chunk Number: 28, Character Count: 484, Token Count: 164\n",
      "Chunk Number: 29, Character Count: 494, Token Count: 162\n",
      "Chunk Number: 30, Character Count: 391, Token Count: 110\n",
      "Chunk Number: 31, Character Count: 343, Token Count: 69\n",
      "Chunk Number: 32, Character Count: 378, Token Count: 99\n",
      "Chunk Number: 33, Character Count: 478, Token Count: 128\n",
      "Chunk Number: 34, Character Count: 452, Token Count: 112\n",
      "Chunk Number: 35, Character Count: 467, Token Count: 104\n",
      "Chunk Number: 36, Character Count: 460, Token Count: 108\n",
      "Chunk Number: 37, Character Count: 373, Token Count: 121\n",
      "Chunk Number: 38, Character Count: 295, Token Count: 87\n"
     ]
    }
   ],
   "source": [
    "# Define parameters for the load_files_and_split_into_chunks function\n",
    "splitter_params = {\n",
    "    \"splitter_type\": \"by_character_recursive\",\n",
    "    \"ocr\": True,\n",
    "    \"ocr_output_format\": \"text\",\n",
    "    \"pages\": \"3-7\",\n",
    "    \"use_encoder\": False,\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 128,\n",
    "    \"verbose\": False,\n",
    "    \"keep_separator\": True,\n",
    "    \"is_separator_regex\": False,\n",
    "    \"verbose\": True,\n",
    "}\n",
    "\n",
    "document_chunks_to_index = azure_search_indexer_client.load_files_and_split_into_chunks(\n",
    "    file_paths=document_blob,\n",
    "    **splitter_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 11:06:30,422 - micro - MainProcess - INFO     Embedding and indexing initiated for 38 text chunks. (ai_search_indexing.py:index_text_embeddings:485)\n",
      "2024-02-06 11:06:37,675 - micro - MainProcess - INFO     Embedding and indexing completed for 38 text chunks. (ai_search_indexing.py:index_text_embeddings:489)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index the document chunks using the Azure Search Indexer client\n",
    "azure_search_indexer_client.index_text_embeddings(document_chunks_to_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-your-own-copilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
